<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Macaron十色]]></title>
    <url>%2F%E5%BF%AB%E6%8D%B7%E6%B8%85%E5%8D%95%2F20190410-3bf3.html</url>
    <content type="text"><![CDATA[传说中让人很舒服的马克龙十色。 英文名 改释 代码 视觉改释 bewitched tree 迷惑的树 #19CAAD mystical green 神秘的绿 #8CC7B5 light heart blue #A0EEE1 glass gall #BEE7E9 silly fizz #BEEDC7 brain sand #D6D5B7 mustard addicted #D1BA74 magic powder #E6CEAC true blush #ECAD9E merry cranesbill #F4606C]]></content>
      <categories>
        <category>快捷清单</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[降维04 - TSNE引领时尚]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20190328-acd8.html</url>
    <content type="text"><![CDATA[t-SNE (t-distributed Stochastic Neighbor Embedding) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。相比于SNE，t-SNE的主要优化有：联合概率替代条件概率、低维空间下使用t分布代替高斯分布。[1] 背景可视化早期的可视化 (Visualization) 工具不负责解释数据，这就限制了这些工具在真实世界数据上的应用，因为我们要想解释数据，我们还是只能靠人眼看。相比于能解释数据的监督学习而言，可视化只需要展示训练数据，而不需要训练模型使它能够拟合到测试数据集。可视化的任务简单许多。[2] 线性降维 将数据从高位空间映射到低维空间的过程我们称之为 map，相应的，低维空间中的映射点被称之为 map points。降维算法已经注意到，要将高维空间中的数据结构问题尽可能的保留到低维空间。 但是传统的线性降维 (Linear dimentionality reduction) 算法，例如PCA、MDS，更加侧重在低维空间中保持高维空间中的差异性，即尽可能地分开数据。同时它们更加关注数据地全局特征，这点与非线性降维算法显著不同。 非线性降维大部分非线性降维 (non-linear dimentionality reduction) 算法关注的是 在低维空间中保持高维空间地局部特征。这就意味着，它们不能同时关注数据的全局特征和局部特征。全局特征就是基于所有数据进行的解释，比如聚类结果就是基于所有数据进行的，理想情况下每个数据点都能找到它自己所属的类；局部特征只是基于部分数据点进行的推导，比如在SNE算法中，总是计算离中心点欧式距离小的部分点进行下降，它关注的是以中心点为圆心，以有限长度为半径的（超）球体内的点。 下面7个常见的非线性降维算法，它们在局部特征提取上都是很优秀的：Sammon mapping [3], CCA [4], SNE [5], Isomap [6], MVU [7], LLE [8], Laplacian Eigenmaps [9]。 t-SNEt-SNE继承自SNE算法，同样是非线性降维，它的优势在于：能够保持大部分局部特征到低维空间，同时不丢失全局特征（例如聚类）。 与SNE一样，t-SNE的思想还是计算两个点间的相似度 (similarity)。 方法论SNE尽管能得到比较好的可视化结果，但是它的损失函数难以优化，并且还存在 crowding problem (拥挤问题) 。相比之下，t-SNE能缓和上面提到的所有问题（优化问题和拥挤问题），与SNE相比，t-SNE主要在两个方面进行改进：1.使用对称的损失函数，新的损失函数求导会更加容易。[10]2.计算低维空间中两点的相似度使用t分布而不是高斯分布，t分布是一种重尾分布 (heavy-tailed distribution)，它能够有效缓解拥挤问题和优化问题，后面将会详细介绍。 优化SNE成对称结构联合概率替换条件概率 在SNE中我们通过条件概率分别计算高维空间和低维空间中点对间的相似度：$$\begin{cases}&amp; p_{ij}=p(x_j|x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{k{\ne}i}{\exp(-||x_i-x_k||^2)}} \\&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}}\end{cases}$$ 然后在t-SNE中我们将条件概率换成联合概率：$$\begin{cases}&amp; p_{ij}=p(x_j,x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{m{\ne}n}{\exp(-||x_m-x_n||^2)}} \\&amp; q_{ij}=q(y_j,y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}\end{cases}$$ 注意上面两种表述方式的分母的差异： 条件概率的分母是中心点 $x_i$ 与其它所有点的相似度之和; 联合概率的分母没有中心点一说，计算的是所有点对（n个数据点有 $C_n^2$ 个点对）的相似度之和。 条件概率中 $p_{ij}{\ne}p_{ji}$，而联合概率中 $p_{ij}=p_{ji}$（q同理），这正好也与分母的这种差异吻合。 注意到联合概率算法会产生一个条件概率算法不会遇到的问题：离群点。 观察上面的联合概率公式，对于离群点 $x_i$，所有与它配对计算出来的 $p_{ij}$ 或者 $p_{ji}$ 的 $||x_i-x_j||^2$ 将会特别大，这导致 $p_{ij}$ 或者 $p_{ji}$ 总是特别的小，即与 $x_i$ 相关的 $p_{ij}$ 或者 $p_{ji}$ 在对损失函数的贡献总是特别小。这相当于自动减小了那些低密度区域的点在损失函数中的权重，使得通过相似性确定离群点在低维空间中的位置更加困难。 所以呢，必须想办法消除这种效应，增大离群点在损失函数中的比重，文中用用条件概率公式代替上述 $p_{ij}$ 的定义，即：$$\begin{cases}&amp; p_{ij}=\frac{p(x_j|x_i)+p(x_i|x_j)}{2n} \\&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}\end{cases}$$ 这样每个点 $x_i$ 对损失函数的贡献度 $p(x_i)=\sum_jp_{ij}&gt;\frac1{2n}$，这就保证了离群点的贡献不会太少。 KL散度作为损失函数 t-SNE仍然使用KL散度作为损失函数，所不同的是，这里求的是两个联合概率分布之间的散度：$$C=KL(P||Q)=\sum_i\sum_jp_{ij}\log{\frac{p_{ij}}{q_{ij}}}$$ 此时KL损失函数求导的结果更加简洁：$$\frac{\partial{C}}{\partial{y_i}}=4\sum_k(p_{ik}-q_{ik})(y_i-y_k)$$ SNE求导结果为：$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$ 解决SNE的拥挤问题什么是拥挤问题流形的直观理解manifold的Wiki解释： In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. 而中文概念“流形”是由北大已故数学教授江泽涵先生提出来。江老的堂姐夫是胡适？… … 不过“流形：这个词真的很艺术，我初次见到时就感叹其形象而不能自已。流形的Wiki中文解释： 是局部具有欧几里得空间性质的空间，是欧几里得空间中的曲线、曲面等概念的推广。欧几里得空间就是最简单的流形的实例。地球表面这样的球面则是一个稍微复杂的例子。一般的流形可以通过把许多平直的片折弯并粘连而成。 为什么说二维流形面上的点距容易建模 (model)？ 这个问题直观上理解最是简单。首先对于欧几里得空间，我们普通人类最多能直观感受到三维。换算成黎曼空间，就意味着我们只能在三维空间中直观感不超过二维流形曲面的存在，二维流形曲面上的距离就是曲面内连接它们的最短曲线长度。经典的二维流形曲面如下（Swiss Roll 流形, Swiss Roll dataset）： 为什么存在拥挤为了便于可视化，我们会将高维流形上的点映射到二维空间，同时最大程度的保留它们的相对位置（这种每个点相对于整体数据点的定位就是一种全局特征）。然而这种映射是很难完美实现的，举个例子，十维空间（欧几里得空间或者黎曼空间）中可以很容易找到11个相互等距的点（就好比二维空间中能轻易找到三个相互等距的点一样），然而映射到二维空间是不可能找到11个相互等距的点的，势必会有一些点会相互靠近挤在一起，如下图所示：以二维相互等距的三个点映射到一维空间为例，无论怎么努力，三点都不可能再等距。归根揭底，不同维度空间内的距离分布是不同的，降维映射难免尽如人意。 再以球内区域为例解释crowding现象：以数据点 $x_i$ 为中心的球的体积与 $r^m$ 直接相关（ $r$ 是半径，$m$ 是球所在空间的维度）。如果在十维流形曲面上数据点均匀分布在这个球中，我们试图在二维流形曲面上以 $y_i$ 为中心对与 $x_i$ 相关的两两距离进行建模。此时我们就会遇到传说中的拥挤问题：与容纳中心点附近数据点的区域相比，容纳适中距离数据点的区域显得不够用。 在均匀分布的条件下，等距点的数量与半径相关，距离越大数量越多，这意味着映射到低维空间就会越“挤”。因此如果我们想要较为准确的在二维流形曲面中对以 $x_i$ 为中心的两两距离建模，我们就必须把距离 $x_i$ 适中位置的那些点往更远的地方推置（因为太挤了）。 不只是SNE，其它局部特征提取算法例如Sammon mapping等也都面临着拥挤难题。 怎么解决拥挤问题一种叫做UNI-SNE的改良算法[10]提出了一种解决办法：给每一个两两相似度添加一个背景值，背景值采样自均匀分布并以一定的比例 $\rho$ 进行混合。由于每个点对之间都引进了背景值，因此不管低维空间中两个映射点离的多么远，$q_{ij}$ 永远不会小于 $\frac{\rho}{n(n-1)/2}$（n个数据点可组合成 $C_n^2$ 个点对）。 引进背景值导致对于高维空间中相距很远的两个数据点总有$q_{ij}&gt;p_{ij}$（$p_{ij}{\rightarrow}0$ 时 $q_{ij}{\rightarrow}\frac{\rho}{n(n-1)/2}$），这表示低维空间中点对并没有完全拟合高维空间的点对相似度，映射后相似度变小。 尽管UNI-SNE的效果比SNE好，但是其损失函数却很难优化。目前较好的优化UNI-SNE的方法是：开始的时候将背景值混合比例设为0，这实际上等效于运行SNE；当SNE开始使用模拟退火策略时增大背景值的混合比例，促进自然分类间的gaps形成。 直接优化UNI-SNE并不可行，因为低维空间中两个相距很远的映射点的 $q_{ij}$ 几乎全部来自于背景值，即高维空间中相应两点间（即使他们的 $p_{ij}$ 很大）的距离对 $q_{ij}$ 的影响很小，这使得映射后的两点间的 $q_{ij}$ 没有什么实际意义。这表示，如果一个自然类的两部分在优化早期就分开了，就再也不会再聚合在一起了。 低维空间采用柯西分布表达联合概率UNI-SNE通过添加背景值使低维空间中相距甚远的 $q_{ij}$ 不至于趋近于0。 本文提出了一种新的解决办法，采用与高斯分布性质极其相似的重尾分布计算联合概率。右重尾分布使得当随机变量取值很大时其对应的概率值高斯分布要大，典型的重尾分布是t分布，如下图所示：t分布实际上是不同方差的高斯分布的混合分布，它的性质与高斯分布十分接近，而且更加容易计算：因为高斯分布涉及到指数运算，而t分布只需要求倒数。 这里采用的是自由度 $\nu=1$ 的t、分布，又叫做柯西分布，其概率密度函数如下：$$f(x;x_0,\gamma)=\frac1{\pi\gamma[1+(\frac{x-x_o}{\gamma})]^2}$$ 取 $x_0=0, \gamma=1$ 得标准柯西分布：$$f(x;0,1)=\frac1{\pi[1+(x-x_0)]^2}$$ 用标准柯西分布表示联合概率：$$q_{ij}=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum_{m{\ne}n}{(1+||y_m-y_n||^2)^{-1}}}=\frac{\sum_{m{\ne}n}(1+||y_m-y_n||^2)}{1+||y_i-y_j||^2}$$ 求导结果如下：$$\frac{\partial{C}}{\partial{y_i}}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}}$$ Pseudo code下面是精简版t-SNE算法伪代码，非常简洁： 优化方法添加微小的动量项可以减少到达最优解的迭代次数 精简版的t-SNE算法采用适应性学习率加速训练：在梯度较稳定的方向上增大学习率。[11] 尽管精简版算法已经可以吊打其它非参数降维技术了，还是可以继续优化，文中提出了两个技巧： 1. early compression：优化起始的时候将所有的映射点初始化在原点附近，有利于映射点移动、形成自然类。early compression通过给损失函数加上一个L2惩罚项实现。2. early exaggeration：在优化的初始阶段将所有的 $p_{ij}$ 扩大指定倍数加快收敛速度。 总结一下模型优化的参数配置： 起始的50次迭代中将所有的 $p_{ij}$ 乘以4（这个步骤在精简版算法的伪代码中没有写出来）； 梯度下降的迭代轮数T设为1000； 动量项 $\alpha^{(t)}$ 当 $t&lt;250$ 时设为0.5，当 $t{\ge}250$ 时设为0.8； 学习率初始值设为100，每次迭代都将进行适应性更新 [11]。 算法的Matlab实现 不足作者分析了三个不足之处。 1. 不能用于低维空间超过三维的情况 因为t-SNE算法在映射空间利用了柯西分布的重尾特性解决拥挤问题，柯西分布是自由度为1的t分布，这种特性在二维空间中表现十分优异。但如果需要降到三维以上的映射空间，1自由度的t分布不能很好的保留局部特征，我们可能需要使用更多自由度的t分布。 2. 本征维度诅咒 t-SNE虽然能够保留全局特征，但是总体上还是基于局部特征进行的降维，这表示t-SNE对原始数据的 本征维度 (intrinsic dimentionality) 十分敏感，因为本征维度过高，我们就不能再把流形曲面的局部区域当欧几里得空间处理了，数据点间的局部特征更加复杂 [12]。不仅t-SNE，其它主流的基于局部特征提取的降维算法（如Isomap，LLe）都面临着这个诅咒。 作者提出了一种可行的办法：先用 自编码器 (autoencoder)[13] 对数据进行压缩，这类模型可以大大降低原始数据的维度，同时最大保留高维数据的特征。经过编码的数据再进行t-SNE降维。 3. 损失函数不凸~ 不幸的是当前主流降维算法使用的损失函数都是凸函数，而t-SNE优化的超参更多，这使得其损失函数是非凸的。这意味着，不同的超参取值、不同的初始化都可能收敛到不同的（局部最优）解。但是作者表示，如果固定这些超参，t-SNE就可以应用于不同的可视化任务，优化结果不会随着不同批次而发生变化。 t-SNE降维结果中点间的距离是没有实际意义的。原始的t-SNE训练很慢，后面有许多改进，比如 multiple maps of t-SNE parametric t-SNE … … 引用[1] Maaten, L. V. D., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.[2] M.C. Ferreira de Oliveira and H. Levkowitz. From visual data exploration to visual data mining: A survey. IEEE Transactions on Visualization and Computer Graphics, 9(3):378–394, 2003.[3] J.W. Sammon. A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, 18(5):401–409, 1969.[4] P. Demartines and J. Herault. ´ Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets. IEEE Transactions on Neural Networks, 8(1):148–154, 1997[5] G.E. Hinton and S.T. Roweis. Stochastic Neighbor Embedding. In Advances in Neural Information Processing Systems, volume 15, pages 833–840, Cambridge, MA, USA, 2002. The MIT Press.[6] J.B. Tenenbaum, V. de Silva, and J.C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.[7] K.Q. Weinberger, F. Sha, and L.K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the 21st International Confernence on Machine Learning, 2004.[8] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by Locally Linear Embedding. Science, 290(5500):2323–2326, 2000.[9] M. Belkin and P. Niyogi. Laplacian Eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, volume 14, pages 585–591, Cambridge, MA, USA, 2002. The MIT Press.[10] J.A. Cook, I. Sutskever, A. Mnih, and G.E. Hinton. Visualizing similarity data with a mixture of maps. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, volume 2, pages 67–74, 2007.[11] R.A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1: 295–307, 1988.[12] Y. Bengio. Learning deep architectures for AI. Technical Report 1312, Universite´ de Montreal, ´ 2007.[13] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>t-SNE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维03 - SNE原理]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20190325-80ae.html</url>
    <content type="text"><![CDATA[Hinton, G. E., &amp; Roweis, S. T. (2003). Stochastic neighbor embedding. In Advances in neural information processing systems (pp. 857-864). 随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE) 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 定义条件概率部分情况下，高维空间中两个点间的相似性可以用基于欧式距离的不相似度 $d_{ij}$ 来衡量：$$d_{ij}^2=\frac{||x_i-x_j||^2}{2\sigma_i^2}$$ SNE用条件概率来代替欧式距离度量两个高维数据点间的相似性。即：以点 $x_i$ 为中心，点 $x_i$ 选择 $x_j$ 作为自己邻居的概率记为 $p(x_j|x_i)$，定义$$p_{ij}=p(x_j|x_i)=\frac{\exp(-d_{ij}^2)}{\sum_{k{\ne}i}{\exp(-d_{ik}^2)}} $$注意这里对于 $p(x_i|x_i)$ 的理解有点怪异：它表示点 $x_i$ 选择自己作为邻居的概率，显然自己永远不可能是自己的邻居，所以 $p(x_i|x_i)=0$，而不是1。 确定方差的确定上式中的 $\sigma_i^2$ 是以 $x_i$ 为中心的高斯分布的方差：不同点周围的点的密度是不一样的，所以每个点的高斯分布对应的方差 $\sigma_i^2$ 也不相同，周围点密度大的中心点对应的方差应该较小。作者定义了困惑度 k (perplexity) ：手动指定的超参，代表某个点的有效邻居数，这个值对所有点都是常数。$\sigma_i^2$ 的取值将使得以点 $x_i$ 为中心选择其它所有点作为邻居的分布对应的熵等于 $\log{k}$，即$$H(P_i)=-\sum_{j{\ne}i}{p(x_j|x_i)\log_2{p(x_j|x_i)}}=\log_2k$$理论上可以通过上面的式子可以针对每个点 $x_i$ 解出对应的 $\sigma_i^2$。 熵 $H(P_i)$ 的理解 不确定性：熵本身就意味着不确定性，当区域点密集时，中心点位置的不确定性就小； 能量：不确定性大意味着能量大，拉不住中心点，它要到处跑； 有效邻居数：点密集时中心点的有效邻居就多。 映射到低维空间在低维空间（二维或者三维）确定一点 $y_i$，它与高维空间的点 $x_i$ 对应，我们手动设置点 $y_i$ 的条件概率分布，即固定以 $y_i$ 为中心点的高斯分布对应的方差为 $\frac12$，当 $j{\ne}i$ 时：$$ q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}} $$当j=i时 $q(y_j|y_i)=0$ 。 此时，如果低维点 $y_i$ 能够正确表示高维点 $x_i$，意味着 $q(y_j|y_i)=p(x_j|x_i)$。为了使两个概率（近似）相等，我们可以最小化KL散度。损失函数如下：$$C=\sum_iKL(P_i|Q_i)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$ $P_i$ 表示：给定点 $x_i$，其它所有点的条件概率分布； $Q_i$ 表示：给定低维空间映射点 $y_i$，其它所有低维映射点的条件概率分布。 KL散度又叫相对熵，用来度量两个分布间的距离。假设P是真实分布，Q是模型分布，$KL(P|Q)$ 表示用Q表示P分布的数据所需的额外信息。 KL散度是不对称的 KL散度中包含 $\log\frac{p_{ij}}{q_{ij}}$ 意味着这个映射不是对称的，即： 使用距离较小的低维点表示距离较大的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于小于0，则损失C较小； 使用距离较大的低维点表示距离较小的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于大于0，则损失C较大。 这里就存在一个问题：当两个高维点距离很远，而我构造两个距离很近的低维点能够使损失函数更小，却与实际的目的不相符！所以，SNE算法使得高维空间中距离近的点在低维空间中距离仍然很近，但是远的点就嘿嘿嘿了。 最小化损失函数从 $q_{ij}$ 的定义式的分母部分可知，低维空间中点 $y_i$ 选择点 $y_j$ 的概率 $q_{ij}$ 与低维空间中的每一个映射点都有关系（分母起到了normalization的作用），但是求导结果却十分简洁：$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$ 想沿着所有点以最陡梯度下降是不现实的，不仅低效，还可能陷入糟糕费解的局部最优。上面的梯度公式右边理论上是针对所有低维映射点进行迭代，但是实际上，相距较远的一堆点间的影响十分小（抽象），在计算时往往可以忽略不计，也就是说：仅仅计算与中心点相距较近的一部分点，即邻居 ，这也是为什么算法中含有单词neighbour的原因了吧。 选择中心点的部分较近的邻居参与计算表示，我们只保留了中心点附近区域的特性，而忽略了整体局势，所以说SNE是保留局部特征而非全局特征的算法。这个局部特性主要反应为 $\sigma_i^2$：局部点密集方差小，局部点稀疏方法大。方差确定的方法前面已经陈述了~。 带动量项的梯度更新为了加速优化过程、避免很一般的局部解，可以在每次下降时添加动量项。动量 (momentum) 的作用就是在下降到局部最优时，小球仍然具有沿切线方向的分量，这使得小球将继续朝前运动，这会有两种结果： 小球越过障碍，继续前行； 小球回退，返回局部最优解；具体的，更新公式如下：$$\gamma^{(t)}=\gamma^{(t-1)}-{\eta}\frac{\partial{C}}{\partial{\gamma}}+\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$$ 式中 $\alpha(t)$ 即动量，动量项 ($\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$) 还与上一次运动幅度有关，直观的看，上一次运动越剧烈，下一次就越刹不住车。 随机抖动随机抖动 (random jitter) 是一种初始化技巧，即将低维空间中的所有数据点初始化在离坐标原点极近的地方。在迭代的过程中，它们将抖抖抖抖抖动直至收敛。尽管还是很慢，不过在节约时间和选择更优局部解时还是有明显提升的。 模拟退火在优化早期给每一步迭代加上高斯噪音，这可以帮助避免不好的局部最优解。随着迭代次数变多，噪音方差将逐渐减小。当方差变化非常慢时，这表明开始形成全局结构。（玄学） 然而，高斯噪音的数量和衰减速率是十分难以确定的，它们不仅与动量相关，也受学习率的影响。怎么办？多算几次！666]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>非线性降维</tag>
        <tag>SNE</tag>
        <tag>t-SNE</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维02 - 主成分分析 (PCA)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20190325-c024.html</url>
    <content type="text"><![CDATA[PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了线性代数之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。 PCA是最易于理解的特征提取过程：通过对原始特征的线性组合构造新的“特征”，这些特征不同于原始特征，但是又能与原始特征一样表达原始数据的信息。 PCA (Primary Component Analysis, 主成分分析) 为什么叫做主成分分析呢？因为PCA构造出的新特征地位并不是等同的，即这些新特征的重要程度存在差异： 第一主成分 (the first component) 是新特征中最重要的特征，它在所有新特征中方差最大，这意味着它对数据变异的贡献是最大的； 第二主成分 (the second component) 在保证不影响第一主成分的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异）； 第三主成分 (the third component) 在不保证第一和第二主成分呢的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异 - 第二主成分引起的变异）; 依次类推…… 线代原理预备知识：线性代数（矩阵运算、特征值&amp;特征向量、特征值分解） 可对角化如果一个n阶方阵A相似于对角矩阵，即存在可逆矩阵$P$使得$P^{-1}AP$是对角矩阵，则称方阵A是可对角化的。 n阶方阵A可对角化的充要条件是A每个特征值的几何重数与代数重数相等：代数重数指特征多项式中该特征值的幂次，几何重数指特征值对应的线性无关的特征向量的个数。 n阶方阵A可对角化的充要条件是A有n个线性无关的特征向量：几何重数与代数重数相等意味着n个线性无关的特征向量。 即使方阵A可逆也不能保证每个特征值的代数重数与几何重数相等，因此A可逆不是A可对角化的充要条件！ 特征值分解如果矩阵A是一个可对角化的方阵，它就可以进行特征值分解，即A可表示为：$$A=Q{\Lambda}Q^{-1}$$其中 $Q$ 是n阶方阵，它的n个列向量是方阵A的n个特征向量 $\Lambda$ 是对角方阵，对角线元素是方阵A的特征值，其位置与 $Q$ 中的特征向量位置相对应 特征值分解的应用？求逆。如果方阵A是非奇异矩阵（即可以进行特征值分解且特征值不含0），则 $A^{-1}=Q{\Lambda}^{-1}Q^{-1}$，其中 $[{\Lambda}^{-1}]_{ii}=\frac1{\lambda_i}$。 奇异值分解特征值分解对A的要求格外严格：可逆、特征值不含0、方阵……放松特征值分解的限制，将A扩展到任意 $m{\times}n$ 的矩阵即得到 奇异值分解 (Singular Value Decomposition, SVD) 。 假设M是定义在实数域或者复数域上的 $m{\times}n$ 阶的矩阵：$$M=U{\Sigma}V^\ast$$其中 U是 $m{\times}m$ 阶酉矩阵：U的m个列向量实际上是 $M^{\ast}M$ 的特征向量，称为M的左奇异向量。 $\Sigma$ 是 $m{\times}n$ 阶非负实数对角矩阵：对角线元素称为M的奇异值，一般情况下奇异值按从大到小的顺序排列！ $V^\ast$ 是 $V$ 的共轭转置，是 $n{\times}n$ 阶酉矩阵：V的n个列向量实际上是 $MM^\ast$ 的特征向量，称为M的右奇异向量。 共轭转置：共轭转置与转置是两个概念，当矩阵定义在实数域上时二者结果相同，矩阵A的共轭转置记为 $A^\ast$，定义如下：$$A^\ast=(\overline{A})^T=\overline{A^T}$$其中，$\overline{A}$ 表示对A的元素复共轭，当A定义在实数域时 $\overline{A}=A$。 当矩阵M定义在实数域时有：$$M=U{\Sigma}V^T$$我们在应用SVD时一般都是定义在实数域上的哟~ 主成分分析上面提到了对于任意 $m{\times}n$ 阶矩阵M的SVD分解：$$M_{m{\times}n}=U_{m{\times}m}{\Sigma_{m{\times}n}}V_{n{\times}n}^T$$直观图如下（这里假设样本数量m多于特征数量n，这意味着M有n个奇异值）：其中 $\Sigma$ 矩阵很有意思，当m&gt;n时，矩阵 $\Sigma_{m{\times}n}$ 中只有子矩阵 $\Sigma_{n{\times}n}$ 的对角线上的值不为0，如下图所示： 以scRNA测序为例：假设在表达谱矩阵中，一行表示一个细胞中不同基因的表达量，一列表示一个基因在不同细胞中的表达量。这与我们的习惯（一列表示一个细胞，一行表示一个基因）有所不同！ 对应到上述SVD分解式我们发现，n表示细胞数量，m表示基因数量。我们降维的结果肯定是要保证细胞总数m不变，而将基因数目从n减小到k。 具体的，取 $\Sigma$ 中最大的k个奇异值，即取 $\Sigma_{k{\times}k}$ 子矩阵，相应的取U的前k列和V的前k列（即$V^\ast$的前k行），即：此时$$M_{m{\times}n}=U_{m{\times}k}{\Sigma_{k{\times}k}}V_{k{\times}n}^T$$上式中的 $U_{m{\times}k}$ 就是 $M_{m{\times}n}$ 从n维特征空间降到k维特征空间的结果。注意矩阵 $U_{m{\times}k}$ 的k个列向量并不在矩阵 $M_{m{\times}n}$ 中，而是M中的n个列向量线性组合的结果。 工具12# in pythonfrom sklearn.decomposition import PCA]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维01 - 特征选择和特征提取]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20190325-d2ce.html</url>
    <content type="text"><![CDATA[大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的降维浓缩技术应运而生。 降维的好处有哪些？ 减少数据维度，存储数据需要的空间也会减少（盘霸可忽略~）； 低维数据可以减少计算量，缩短模型训练时间； 很多算法在高维数据上的表现远远没有在低维数据上好； 去掉冗余特征（强相关特征），提高数据的质量； 有助于可视化，我们只能形象观察三维及以下的数据！ 分类降维总是围绕着减少特征数进行的，根据对特征的操作可分为： 特征选择：保留原始特征集的子集，即选取部分原始特征； 特征提取：构造不同于原始特征的新特征，新特征往往是原始特征的组合，替代原始特征表达原始数据想表达的信息。特征提取是降维算法研究的核心内容。 特征选择特征选择只是对每个特征进行评估，去掉不重要的或者选出重要的： 缺失值比率：按缺失值比率删除特征； 低方差滤波：删除方差小的特征； 高相关滤波：只保留高相关特征中的一个； 随机森林：计算每个特征的重要性； 前向特征选择：依次增加特征数检验模型性能； 反向特征消除：依次减少特征数检验模型性能。 特征提取特征提取才是降维思想的核心内容，降维算法家族枝繁叶茂，先做一个总体分类：]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper3 - RUV算法移除批次效应 (Davide Risso, 20140824)]]></title>
    <url>%2F%E7%94%9F%E4%BF%A1%E5%B7%A5%E5%85%B7%E5%92%8C%E7%AE%97%E6%B3%95%2F20190322-afa7.html</url>
    <content type="text"><![CDATA[方法 Gagnon-Bartsch et al.提出了RUV-2用来标准化连续的微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化离散的RNA测序数据。 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建泛化线性模型 (Generalized Linear Model, GLM):$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$参数意义如下： $Y$ 是 $n{\times}J$ 的表达矩阵； $W$ 是 $n{\times}k$ 与不需要的变异相关的多余变异相关矩阵（k是不需要的变异相关的变量的个数），$\alpha$ 是 $k{\times}J$ 的多余变异相关矩阵的系数（参数）； $X$ 是 $n{\times}p$ 与感兴趣的变异相关的期望变异相关矩阵（p是感兴趣的变异相关的变量的个数），$\beta$ 是 $p{\times}J$ 的期望变异相关矩阵的系数（参数）； $O$ 是一个 $n{\times}J$ 的矩阵，它可以置零，也可以包含其它的标准化过程（如UQ标准化）。 矩阵 $X$ 是一个随机变量，是我们实验的测量值，是已知的（先验）。 矩阵 $W$ 是未观测的随机变量；$\alpha$、$\beta$、$k$ 都是未知参数。 不同于先前的标准化方法，RUV可以使用GLM标准化技术同时标准化reads计数（$W\alpha$）和推断差异表达（$X\beta$）。标准化的计数也可以通过由原始计数对不需要的因子进行回归分析后求残差得到，但是直接从原始计数中移除不需要的因子（$W\alpha$）可能会损失掉 $X$ 的一部分。[reference] 同时估计 $W$, $\alpha$, $\beta$ 和 $k$ 是很难的。对于一个给定的 $k$ 值，我们尝试着用下面三种方法对$W$ 进行估计： 1. 基于阴性对照基因的RUVg 假设我们鉴定出了一个阴性对照基因 (negative control genes) 的集合（大小为 $J_c$），例如不差异表达的基因，对这个基因集合来说 $\beta_c=0$ 即 $\log{E[Y_c|W,X,O]}=W\alpha_c+O_c$，公式中的下标c将矩阵限制在了大小为 $J_c$ 的基因集合里。 定义 $Z=\log{Y}-O$，$Z^\ast$ 是 $Z$ 列向量中心化（$Z$ 的各个列向量均值都为0）的结果。 对 $Z_c^\ast$ 进行奇异值分解 (singular value decomposition, SVD) 即 $Z_c^\ast=U{\Lambda}V^T$。矩阵 $U$ 是 $n{\times}n$ 列正交矩阵，它的列向量是 $Z^\ast$ 的左奇异向量集；矩阵 $V$ 是 $J_c{\times}J_c$ 的列正交矩阵，它的列向量是 $Z^\ast$ 的右奇异向量集；$\Lambda$ 矩阵是由 $Z^\ast$ 的奇异值组成的非方形对角矩阵，大小为 $n{\times}J_c$。$Z^\ast$ 最少有 $\min{(n,J_c)}$ 个奇异值。对于一个给定的 $k$，通过 $\widehat{W\alpha_c}=U\Lambda_kV^T$ 估计 $W\alpha_c$，通过 $\hat{W}=U\Lambda_k$ 估计 $W$。$|lambda_k$ 是由 $\Lambda$ 导出的大小为 $n{\times}J_c$ 的非方形对角矩阵，保留 $\Lambda$ 中最大的 $k$ 个奇异值，将其它的奇异值置为0。 将 $\hat{W}$ 带入上面基于 $J$ 个基因构建的公式中，通过GLM回归估计 $\alpha$ 和 $\beta$。 （可选）将标准化的读段计数定义为 $Z$ 对 $\hat{W}$ 的普通最小二乘回归 (ordinary least squares, OLS) 的残差。 这是最基础的RUV-2的离散版本。其中的关键假设是我们能够找到这个阴性对照基因集合。然而，RUV-2已被证实对对照基因的选择十分敏感。我们因此考虑下面的两种方法：RUVr不需要阴性对照基因，RUVs对阴性对照基因选择的鲁棒性更强。 2. 基于残差的RUVr 计算残差矩阵 $E(n{\times}J)$: 计数矩阵 $Y(n{\times}J)$ 关于感兴趣的协变量矩阵 $X(n{\times}J)$ 的初步GLM回归，例如异常值残差。这里用于回归计算的计数矩阵可以是未标准化的原始数据，也可以是经过其它标准化工具（例如UQ）处理过的数据。 对残差进行奇异值分解，即 $E=U{\Lambda}V^T$，通过 $\hat{W}=U\Lambda_k$ 估计 $w$。接下来的步骤与 RUVg 的第4、5步相同。 3. 基于重复/阴性对照样本的RUVs 假设在多个复制样本中具有生物学特征的（我们感兴趣的）某些协变量的表达量可看作恒定的，它们的计数差异与RUVg中的阴性对照基因一样，对我们后续的研究没有影响。现在假设有 $R$ 个复制组，$r(i){\in}{1,…,R}$ 表示样本 $i$ 所属的复制组；如果样本 $i$ 不属于任何一个复制组，则 $r(i)=0$。例如，对于SEQC数据集，样本A和样本B各自的64个复制本（$=4[\text{libraries}]{\times}2[\text{flow-cell}]{\times}8[\text{lanes}]$）分别组成了一个复制组。 对每一个复制本对应的计数矩阵进行列中心化处理，即矩阵各个列向量的均值都为0。去掉不属于预期复制组的样本，即筛选出 $n_d=\sum_i{I(r(i)\ne0)}$ 个样本对应的列中心化后的计数子矩阵 $Y_d(n_d{\times}J)$。 此时 $\log{E[Y_d|W,X,O]}=W_d\alpha+O_d$，对应的矩阵大小是 $(n_d{\times}J){\leftarrow}({n_d\times}k)({k\times}J)+(n_d{\times}J)$。 定义 $Z_d=\log{Y_d}-O_d$，$Z_d^\ast$ 是 $Z_d$ 列中心化的结果，$Z_d^\ast=U{\Lambda}V^T$。通过 $\hat{\alpha}=\Lambda_kV^T$（保留最大的 $k$ 个奇异值，$k{\le}\min{(n_d,J)}$）来估计 $\alpha$。 在所有 $n$ 个原始数据和 $J_c$ 个阴性对照基因上对 $Z_c$ 进行最小二乘回归（OLS）。估计讨厌因子 $W$：$\hat{W}=Z_c\hat\alpha_c^T(\hat\alpha_c\hat\alpha_c^T)^{-1}$。接下来的步骤与 RUVg的第4、5步相同。 要点两个数据集 SEQC data set: The third phase of the MicroArray Quality Control (MAQC) project, also known as the Sequencing Quality Control17 (SEQC) project, aims to assess the technical performance of high-throughput sequencing platforms by generating benchmarking data sets. Zebrafish (斑马鱼) data set: All procedures were conducted in compliance with US federal guidelines in an AAALAC-accredited facility and were approved by the UC Berkeley Office of Animal Care and Use. 两种讨厌因子本文分析了两种讨厌因子：library preparation &amp; flow-cell effects。 flowcell：流动室，别称鞘流池、流动池，是流式细胞技术的基础关键部件。大概长这个样子： 作者用正交的主成分图展示了这两种讨厌因子：Scatterplot matrix of first three principal components (PC) for unnormalized counts (log scale, centered). The principal components are orthogonal linear combinations of the original 21,559-dimensional gene expression profiles, with successively maximal variance across the 128 samples, that is, the first principal component is the weighted average of the 21,559 gene expression measures that provides the most separation between the 128 samples. Each point corresponds to one of the 128 samples. The four sample A and the four sample B libraries are represented by different shades of blue and red, respectively (16 replicates per library). Circles and triangles represent samples sequenced in the first and second flow-cells, respectively. As expected for the SEQC data set, the first principal component is driven by the extreme biological difference between sample A and sample B. The second and third principal components clearly show library preparation effects (the samples cluster by shade) and, to a lesser extent, flow-cell effects reflecting differences in sequencing depths (within each shade, the samples cluster by shape). 算法横向对比上分位数标准化 (Upper-quartile normalization, UQ)，UQ只能消除流细胞效应而对文库效应束手无策，RUV算法解决的就是如何消除不同文库的影响。 局部加权回归散点平滑法 (Locally Weighted Scatterplot Smoothing, LOWESS/LOESS)不能消除文库效应。 补充ERCC spike-in controlsERCC 即 External RNA Controls Consortium，是斯坦福大学为了定制一套spike-in RNA而成立的专门性组织，主要的工作是设计了好用的spike-in RNA，方便microarray以及RNA-Seq进行内参定量。[官方首页] RNA spike-in是一种数量和序列都已知的RNA转录本，用于校准RNA杂交实验（例如DNA微阵列实验、RT-qPCR、RNA测序等）的测量值。RNA spike-in作为对照组（控制组）探针，被设计成能与具有相应匹配序列的DNA分子结合，这个特异性结合的过程我们称之为杂交。在制备的过程中，已知数量的spike-in将与实验样本进行混合。spike-ins的杂交程度可以用来标准化样本RNA的测量值。[wiki] [reference]]]></content>
      <categories>
        <category>生信工具和算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[批次效应（batch effect）]]></title>
    <url>%2F%E7%94%9F%E4%BF%A1%E5%B7%A5%E5%85%B7%E5%92%8C%E7%AE%97%E6%B3%95%2F20190319-cca5.html</url>
    <content type="text"><![CDATA[一、定义下面是大佬给出来的关于批次效应(batch effect)的定义： Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. Leek et. al (2010) 批次效应是测量结果中的一部分，它们因为实验条件的不同而具有不同的表现形式，并且与我们研究的变量没有半毛钱关系。一般批次效应可能在下述情形中出现： 一个实验的不同部分在不同时间完成； 一个实验的不同部分由不同的人完成； 试剂用量不同、芯片不同、实验仪器不同； 将自己测的数据与从网上下载的数据混合使用； …… 二、检测批次效应相关协变量已知时，直接聚类观察结果是否和相应协变量相关。混合数据因为实验条件迥异，一般批次效应都很大。 以R为例，通过聚类检验是否存在批次效应。请先查看下面的示例数据集。12345678# t() 转置函数# dist() 距离函数：按照指定规则求行向量间的距离，因此要转置&gt; dist_mat &lt;- dist(t(edata))&gt; clustering &lt;- hclust(dist_mat) # hclust 的输入结构与 dist 相同！# 按照批次信息聚类&gt; plot(clustering, labels = pheno$batch)# 按照是否是正常细胞聚类&gt; plot(clustering, labels = pheno$cancer) 聚类结果如下：左边的红色框框是正常细胞中混入的癌细胞，右边蓝色框框中是癌细胞中混入的正常细胞。 还有许多检验批次效应的的方法，这篇文章给出了多种检验方式： 图分析：双柱状图、QQ图、箱线图、块图、… 定量分析：F检验、双样本t检验、… 三、处理实验条件允许的条件下，应该优化实验设计，将引起批次效应的协变量采样分散开来。例如，对于时间批次效应，实验的不同部分应该在各个时间内均匀采样。这叫“治病就治本”。 但是大多数情况下实验条件不允许，如果够幸运的话批次效应相关的协变量已经被记录下来了，此时对批次效应进行验证，然后使用统计模型过滤；如果十分不幸，批次效应相关的协变量没有被记录或者不明显，我们就需要借助相关工具猜一下哪个变量可能造成了批次效应，然后使用统计模型过滤。前者叫参数化方法，后者叫非参数化方法。 1.导入示例数据集bladderbatch包bladderbatch包包含了一项膀胱癌研究中相关的57个样本的基因表达数据，这些数据已经使用RMA标准化，并且已经按照相关协议进行了预处理。 另外阅读R文档我们发现： eSet是一个包含高通量实验元数据的一个类，它不能直接被实例化。 pData方法在类eSet中被定义，它的作用是访问数据的元数据（注释信息）。 ExpressionSet继承自eSet，同样是一个高通量测序数据的容器，由&gt; * biobase包引入，封装了表达矩阵和样本分组信息。表达矩阵存储在exprs中。 bladderbatch数据集是（类似）ExpressionSet类型，我们可以使用pData()加载元数据，使用exprs()加载表达谱数据。bladderbatch数据集用来演示如何校正批次效应。 下载和加载数据集123456789101112## 1.安装并加载数据集&gt; BiocInstaller::biocLite("bladderbatch")&gt; library(bladderbatch) # 或者 library("bladderbatch", character.only=TRUE)## 2.查看当前可用数据集&gt; data()## 3.检查是否有如下信息Data sets in package ‘bladderbatch’:bladderEset (bladderdata) Bladder Gene Expression Data Illustrating Batch Effects## 加载数据集&gt; data(bladderdata) # 实际加载进来的数据集名字叫做 bladderEset !&gt; pheno &lt;- pData(bladderEset) # 使用 pData 加载元数据/注释信息&gt; edata &lt;- exprs(bladderEset) # 使用 exprs 加载数据 pheno如下所示：样本的批次信息存储作为元数据存储在pheno$batch中（R中使用$访问对象的属性）。 edata如下所示：一列表示一个样本（细胞），后面求距离需要转置。 2.R中的sva包sva用于移除高通量测序数据中的批次效应以及其它无关变量的影响。 sva包含用于标识和构建高维数据集（例如基因表达、RNA测序/甲基化/脑成像数据等可以直接进行后续分析的数据）代理变量的函数。代理变量是直接从高维数据构建的协变量，可以在后续分析中用于调整未知的、未建模的或潜在的噪音源。 代理变量（surrogate/proxy variable）: A variable that can be measured (or is easy to measure) that is used in place of one that cannot be measured (or is difficult to measure). For example, whereas it may be difficult to assess the wealth of a household, it is relatively easy to assess the value of a house. See also proxy variable. (from Oxford Reference)代理变量分析（Surrogate Variable Analysis）：Click here sva从三个方面消除人为设计造成的影响： 为未知变异源构造代理变量；(Leek and Storey 2007 PLoS Genetics, 2011 Pharm Stat.) 使用ComBat直接移除已知的批次效应；(Johnson et al. 2007 Biostatistics) 使用已知的控制探针(known control probes)移除批次效应；(Leek 2014 biorXiv)移除批次效应和使用代理变量可以减少依赖性，稳定错误率估计值，提高重现性。 查看sva在线文档。 &gt; 已记录批次信息当批次协变量已知时（即每个样本分属于哪一个批次记录在数据集的元数据中），可以使用sva的ComBat校正批次效应。ComBat使用参数（parametric）或者非参数（non-parametric）的经验贝叶斯框架（Empirical Bayes Frameworks）进行批次效应的校正。 先看ComBat的用法：摘自官方文档12345&gt; ComBat(dat, batch, mod=NULL, par.prior = TRUE, prior.plots = FALSE)# dat: 基因组测量矩阵（探针维度 X 样本数），探针维度例如marker数、基因数.....，例如表达谱矩阵# batch: 批次协变量，只能传入一个批次协变量！# mod: 这是一个模式矩阵，里面包含了我们感兴趣的变量！# par.prior: 基于参数/非参数，默认为基于参数 有了背景知识我们就可以进行膀胱癌数据的批次校正：123456&gt; pheno$hasCancer &lt;- pheno$cancer == "Cancer"# 或者 &gt; pheno$hasCancer &lt;- as.numeric(pheno$cancer == "Cancer")&gt; model &lt;- model.matrix(~hasCancer, data=pheno)&gt; combat_edata &lt;- ComBat(dat = edata, batch = pheno$batch, mod = model)# 这里的 mod 参数就比较有意思了，它记录的是我们感兴趣的变量。因为初次接触R只能肤浅理解一下。# 它应该是一个我们期望样本能被正确聚类所依据的协变量，它总是数值型变量 model.matrix(...)的详细解释见这里。 画图：1234&gt; dist_mat_combat &lt;- dist(t(combat_edata))&gt; clustering_combat &lt;- hclust(dist_mat_combat, method = "complete")&gt; plot(clustering, labels = pheno$batch)&gt; plot(clustering, labels = pheno$cancer)) 我们发现批次效应被移除了： &gt; 没有记录批次信息看这里 3.R中的ber包ber的全称就是batch effects removal，使用&gt; install.packages(&quot;ber&quot;)安装ber包，查看用户手册。 这个包里有6个函数，它们的作用就是校正微阵列标准数据中的批次效应。标准数据指的是：输入矩阵每一行代表独立的样本，每一列代表基因；批次信息作为已知的分类变量；期望变量可以大大提高批次效应校正的效率。 berr(Y, b, covariates = NULL) using a two-stage regression approach (M. Giordan. February 2013) ber_bg(Y, b, covariates = NULL,partial=TRUE,nSim=150) using a two-stage regression approach and bagging (M. Giordan. February 2013) combat_p(Y, b, covariates = NULL, prior.plots=T) using a parametric empirical Bayes approach (n Johnson et al. 2007) combat_np(Y, b, covariates = NULL) using a non-parametric empirical Bayes approach (n Johnson et al. 2007) mean_centering(Y, b) using the means of the batches standardization(Y, b) using the means and the standard deviations of the batches 上表中的： Y是输入矩阵（样本数 $n{\times}g$ 探针数） b是 $n$ 维分类1向量，每个分量对应着每个样本的批次信息 covariates是一个 $n$ 行的data.frame实例 上面的6个函数都需要指定b，所以它们都是用来处理批次信息被记录的情形的，对于启发性的校正貌似没提出解决方案。 4.R中的RUVSeq包RUVSeq means Remove Unwanted Variation from RNA-Seq Data, which shows us how to conduct a differential expression (DE) analysis that controls for “unwanted variation”, e.g., batch, library preparation, and other nuisance effects, using the between-sample normalization methods proposed in Risso et al. (2014). RUV算法基本原理参考这里，原文在这里。 5. R中的BatchQC包BatchQC工具 四、FAQ 标准化（normalization）可以消除批次效应吗？ 只能缓解，不能消除。 五、其它资料Stanford大学MOOC公开课讲义：PH525x series - Biomedical Data Science TCGA Batch Effects Viewer From BioMedSearch: Removing batch effects in analysis of expression microarray data: an evaluation of six batch adjustment methods.]]></content>
      <categories>
        <category>生信工具和算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[model.matrix(...)]]></title>
    <url>%2FR%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%2F20190319-1548.html</url>
    <content type="text"><![CDATA[R中的模型矩阵函数。 分类变量分类变量（Factors）：R中用来存储分类数据的类别信息。12345678910&gt; f = factor(c('a','b','a','c'))# 检查变量是否是分类变量（因子）&gt; class(f)[1] "factor"# 查看分类变量中有哪些类别&gt; levels(f)[1] "a" "b" "c"# 查看分类变量中有几类&gt; nlevels(f)[1] 3 哑变量虚拟变量/哑变量（dummy variable）：量化非数值类型的变量，通常取0/1。例如，衡量一个人的性别：男 -&gt; 1，女 -&gt; 0。 解释变量解释变量（explanatory variable）：单纯从数理角度来看，解释变量等同于控制变量/自变量，与之相对的是被解释变量（反应变量/因变量）。REF 设计矩阵设计矩阵（design matrix）：又叫模型矩阵（model matrix）或者回归矩阵（regressor matrix）。由解释变量值组成的矩阵：一行代表一个独立的观测对象（样本），一列代表对应的变量（特征值、元数据），通常记为$X$。简单理解，就是我们所说的输入矩阵，可以是元数据的，也可以是数据的。 model.matrix(…)定义：1234# S3 method for default model.matrix(object, data = environment(object), contrasts.arg = NULL, xlev = NULL, …)# 函数依据 object 创建设计矩阵，矩阵的创建必须借助于数据集 data# data 必须能提供与 object 相同名字的变量！ 以膀胱癌去批次效应为例，元数据形式如下 下面是部分列处理后的结果：123456789101112131415161718192021222324252627&gt; model &lt;- model.matrix(~batch, data = pheno) (Intercept)batchGSM71019.CEL 1 3GSM71020.CEL 1 2# pheno$batch 是数值型变量，相当于提取列# 此时新的变量名仍然是 batch#---------------------------------------------------------------------&gt;model &lt;- model.matrix(~cancer, data = pheno) (Intercept) cancerCancer cancerNormalGSM71019.CEL 1 0 1GSM71020.CEL 1 0 1# pheno$cancer 被处理成分类变量，每一类将单独作为列（哑变量），取值为0/1# 此时新的变量名为 cancerCancer 和 cancerNormal#---------------------------------------------------------------------&gt; model &lt;- model.matrix(~cancer=="Cancer", data = pheno) (Intercept) cancer == "Cancer"TRUEGSM71019.CEL 1 0GSM71020.CEL 1 0# cancer=="Cancer" 是一个 logical 类型# 这种写法极不优雅！我们应该先定好名字&gt; pheno$hasCancer &lt;- pheno$cancer == "Cancer"&gt; model &lt;- model.matrix(~hasCancer, data=pheno) (Intercept) hasCancerGSM71019.CEL 1 0GSM71020.CEL 1 0]]></content>
      <categories>
        <category>R统计语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-SCENIC]]></title>
    <url>%2F%E7%94%9F%E4%BF%A1%E5%B7%A5%E5%85%B7%E5%92%8C%E7%AE%97%E6%B3%95%2F20190315-94a8.html</url>
    <content type="text"><![CDATA[SCENIC: Single-cell regulatory network inference and clustering 基于regulon(TF-&gt;targets)构建GRNs，基于GRNs可以进行细胞聚类。 与其说是算法，不如说SCENIC是组合算法的一个流程。如下图所示： 1. 基因数据过滤共表达分析之前需要对基因数据进行过滤，本文使用如下两个方法： 1.根据每个基因的reads数目移除可信度低或者将产生较大噪声的基因。reads数目的阈值与数据集的大小有关，以下是原文表述：3 UMI counts (slightly over the median of the nonzero values) multiplied by 1% of the number of cells in the dataset (e.g. in mouse brain: 3 UMI counts x 30 (1% of cells) = minimum 90 counts per gene). 2.根据可检测到某个基因的细胞数目过滤掉只在一个细胞或者极少量细胞中表达的基因，细胞数目的阈值设定参考原文：set a percentage lower than the smallest population of cells to be detected. For example, since microglia cells represent approximately 3% of the total cells in the dataset, we used a detection threshold of at least 1% of the cells. 2. 共表达分析仅仅借助共表达分析(co-expression analysis)(GENIE3或者GRNBoost)得到regulons的表达情况（GRNs）。一个regulon由一个TF和它调控的靶基因组成。当数据集非常大时GENIE3的运行速度将会变得非常慢，此时使用GRNBoost替换GENIE3能大大加快计算速度。 但是，共表达分析存在许多假阳性结果，我们需要找到这些实际上不存在的TF-靶基因配对，因此需要下一步的基序富集分析。 GENIE3GENIE3的核心算法是随机森林回归模型。随机森林能够处理非线性共表达关系。针对不同的TF训练不同的模型，这些模型用于计算相应TF的权重，这些权重可以用来衡量它与靶基因共表达的强度。 输入：一个基因表达矩阵，矩阵的每一列代表一个细胞的不同基因，每一行代表一个基因在不同细胞里的表达量。矩阵的元素可以是UMI计数，也可以是其它指标，例如TPM (Transcripts Per Million)、FPKM/RPKM等等。输入矩阵应避免进行标准化或归一化处理，这样会人为的引入多余的协方差。 输出：一张三列表，分别代表TF、靶基因、权重（TF靶向目标基因的可信度） GRNBoost Spark RDD：Risilient Distributed Datasets 弹性分布式数据库广播变量(Broadcast Variable)：将只读变量广播到各个节点以供读取，避免变量在任务间进行传递。变量被广播之后应避免被修改。 GRNBoost作为GENIE3在大数据集下的替代方案，它仍然接受GENIE3的基本思想：仅从基因表达数据中推断GRNs。 算法方面，GRNBoost使用了XGBoost库中的GBM (Gradient Boosting Machines)。GBM是一种结合多种弱学习器、以提升学习作为基本策略的集成学习方法。相对于随机森林，GBM使用了bagging自助聚合进行模型的平均以提高回归准确度。 然而，GRNBoost的主要贡献是基于Spark实现了多回归并行计算。软件输入是基因表达向量（一系列基因和一个转录因子表达量组成的向量？）。GRNBoost首先将基因表达向量分发给集群的各个节点，然后构建一个基于表达数据全集的预测矩阵。然后使用广播变量（Broadcast Variable）将这个预测矩阵广播到各个节点，接下来进行Map/Reduce分布式计算。 Map阶段：基于基因表达向量使用预测器训练XGBoost回归模型。基于训练的模型，TF和靶基因的靶向强度将以网络的边的形式呈现出来。 Reduce阶段：整合所有的边形成最终的GRNs。 3. 基序富集分析基序富集分析(motif enrichment analysis)使用的工具是RcisTarget，它能找到共表达分析的假阳性结果。删除这些假阳性结果就能得到正确的GRNs (Gene Regulatory Networks)。 RcisTarget 是 i-cisTarget 和 iRegulon 基序富集框架的 R/Bioconductor 实现。 主要分为两个步骤： 选择在基因的转录起始位点（TSS）附近明显高表达的DNA基序This is achieved by applying a recovery-based method on a database that contains genome-wide cross-species rankings for each motif. 实现方法：在基序全基因组跨物种排名数据库上使用recovery方法保留那些可以注释到TF并且标准富集分数(Normalized Enrichment Score, NES)大于3.0的基序2.对于每个基序和基因集，RcisTarget预测候选靶基因（如在基因集中排列在前缘以上的基因）方法详情见引用[32]，此方法在i-cisTarget&amp;iRegulon中均有实现，因此使用RcisTarget得到的结果与i-cisTarget&amp;iRegulon的结果一致 为了构建最终的调控子，我们将每个有基序富集的TF模块预测的靶基因进行归并。上面针对的是正调控，对于抑制，仍然可以对负相关的TF模块做相同的处理；但是我们的分析中，这类模块较少。基于上述事实，本实验之研究正相关，不研究负相关本文使用的数据集：the “18k motif collection” from iRegulon (gene-based motif rankings) for human and mouseTSS搜索空间：10kb around the TSS or 500bp upstream the TSS 4. AUCell打分The relative scores of each regulon across the cells allow identifying which cells have a significantly high sub-network activity细胞的调节子打分容许我们识别哪些细胞具有明显的高子网络活性？？？结果是一个二进制的活性矩阵，可用于下游分析——对此矩阵的聚类可用于细胞类型或者细胞状态的识别，基于调控子网络的活性共享。对抗dropouts增强鲁棒性：对调节子整体进行打分，而不是针对特定的转录因子或者单个基因。 基于单细胞测序数据，从活化的GRNs中鉴定细胞输入是一个基因集，输出为每个细胞中基因集的活性（AUC指标）在SCENIC中，这些基因集表现为regulons，每个调控子由一个TF和它对应的靶基因组成AUCell calculates the enrichment of the regulon as an area under the recovery curve (AUC) across the ranking of all genes in a particular cell, whereby genes are ranked by their expression value.将AUC区域面积作为regulon的富集量，该区域包含了特定细胞中所有基因的排序。This method is therefore independent of the gene expression units and the normalization procedure.因为是在单个细胞上进行检验，因此很容易可以应用到大数据集 ——AUCell用来估计每个细胞中每个regulon的活性，通过计算恢复曲线下的面积，整合了每个regulon里所有排列的基因的信息——AUC打分（上面计算出来的）通过设定阈值构建Regulon Activity Matrix，用来判定哪些细胞里的regulon处于on状态——左图横坐标是一个regulon的靶基因的排列，纵坐标是从输入数据集中数出来的基因数目然后，AUCell使用“曲线下面积”(AUC)计算输入基因集中的一个关键子集是否在每个细胞的排名顶部富集。AUC表示表达基因在特征中的比例以及相对于细胞内其他基因的相对表达值这一部的输出是一个矩阵：每一个细胞的每一个基因集的AUC分数使用细胞中的一系列regulon的AUC值进行细胞的聚类，或者使用处理过的二值矩阵——二值矩阵：自动 or 手动——下图是AUC分布的几个例子 5. 基于GRNs的细胞聚类AUC activity matrix：每个细胞中每个regulon的AUC值，连续值regulon activity matrix：上面矩阵二值化的结果，01矩阵可视化：主要用t-SNE、层次聚类的热图探究结果的其他可选项—— t-SNE的高密度区域 =&gt; 最可能的稳定状态—— 鉴定key regulators—— 基于数据库注释了解细胞属性—— GO terms (regulon内的基因富集分析) 操纵子（operon）：包含了操纵基因的核苷酸序列，被某个启动子控制，对应一组受操纵基因调控的基因调节子（regulon）：对应受某个起调节作用的蛋白质调节的一组基因刺激子（stimulon）：对应某类起调节作用的细胞调节的一组基因 引用Nat Methods. 2017 Nov;14(11):1083-1086. doi: 10.1038/nmeth.4463. Epub 2017 Oct 9.SCENIC: Single-cell regulatory network inference and clusteringAibar S1,2, González-Blas CB1,2, Moerman T3,4, Huynh-Thu VA5, Imrichova H1,2, Hulselmans G1,2, Rambow F6,7, Marine JC6,7, Geurts P5, Aerts J3,4, van den Oord J8, Atak ZK1,2, Wouters J1,2,8, Aerts S1,2. &gt;&gt;&gt; SCENIC]]></content>
      <categories>
        <category>生信工具和算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-RaceID2+StemID]]></title>
    <url>%2F%E7%94%9F%E4%BF%A1%E5%B7%A5%E5%85%B7%E5%92%8C%E7%AE%97%E6%B3%95%2F20190315-98e6.html</url>
    <content type="text"><![CDATA[背景数据分析时要处理的噪声不仅来自实验误差和测量误差，基因表达和少量mRNA的扩增都会产生一定的噪声。其中来自生物过程的噪声可以通过大量测序来缓和。 如果根据某一时刻的转录表达情况推断细胞系谱结构一直是一个严峻的挑战。作者肯定了Wanderlust算法的设计思想，但是同时对其设计的拓扑结构的合理性表示怀疑，并提出了自己的解决方案。 RaceID2算法旨在：基于单细胞转录组数据，通过聚类鉴定出干细胞 StemID算法旨在：利用RaceID2的聚类结果，构建系谱树 算法算法在小肠的Lgr5+细胞、骨髓造血干细胞上学习，在人类胰腺多能干细胞上测试。Lgr5+小鼠肠道干细胞群的分化过程研究已经发表，作为算法学习的基础数据集。 RaceID此处嘀向作者2015年发表的论文 RaceID2作者对自己已经发表的RaceID算法进行优化：RaceID使用K-means进行聚类，因此求所有样本平均值的做法使得RaceID对异常值十分敏感，同时RaceID使用间隔统计量（Gap Statistics, GS）确定分类个数；但是作者认为这种方法并不理想，因此在RaceID2中改用K-medoids方法聚类，并且依据类内散布饱和临界值为依据确定分类个数。K-medoids聚类方法使用类似于中位数的方法确定聚类中心，与K-means不同，它的聚类中心始终产生在样本点上。 RaceID2是一种改进的聚类算法，能够将大量细胞进行聚类，从而确定不同细胞群/亚群的分界线。 StemIDStemID是一种系谱图推导方法，StemID的系谱图推导基于RaceID2的聚类结果。 下面是从原文摘录的算法流程图： 图A是RaceID2的聚类结果，其聚类中心都在样本点上。 这里先介绍作者给出的一个感性假设：每个节点 $k$ (一个细胞)除了属于自身的第 $i$ 类外，它还将连接到一个其它的某个类 $j$ ，这个类 $j$ 实际意义等同于另一个的细胞群/亚群，细胞 $k$ 将倾向于朝细胞群 $j$ 分化。 如图A所示，Cluster 1的聚类中心是 $m_i$，这里 $i=1$，将 $m_i$ 与其它所有聚类中进行连接（如图A黑色矢量）。第 $i$ 类中的节点 $k$（蓝色矢量箭头处）与类中心 $m_i$ 组成了一个向量（如图A蓝色矢量）。蓝色矢量将在所有黑色矢量上产生一个 投影。我们取与 最大投影长度 相对应的那个外类作为 感性假设 中陈述的潜在分化方向。 如图B2所示，将所有节点转换成到之相对应的投影位置，此时所有节点都将落到由聚类中心组成的网络上。 此网络就是StenID算法所构建的系谱树框架。 给网络的边打分：映射后不同边上点的分布是不同的，对于某条边 $L$ 上的任意两点 $r_i$ 和 $r_j$，定义打分公式： $$score=1-\max_{i,j{\in}L}{||r_i-r_j||}$$ 当 $score{\to}0$ 时说明该边上所有点非常紧密的靠近聚类中心。 p值计算：重复采样，略 细胞的熵得计算：略 不足作者自己分析了一下，在下面两种情况出现时算法可能不太灵光： 出现中间过渡态细胞的样本缺失； 出现不直接关联的细胞。 引用Cell Stem Cell. 2016 Aug 4;19(2):266-277. doi: 10.1016/j.stem.2016.05.010. Epub 2016 Jun 23.De Novo Prediction of Stem Cell Identity using Single-Cell Transcriptome DataGrün D1, Muraro MJ2, Boisset JC2, Wiebrands K2, Lyubimova A2, Dharmadhikari G3, van den Born M2, van Es J2, Jansen E2, Clevers H4, de Koning EJP3, van Oudenaarden A5. github：StemIDomicX：stemid-tool]]></content>
      <categories>
        <category>生信工具和算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-Wanderlust]]></title>
    <url>%2F%E7%94%9F%E4%BF%A1%E5%B7%A5%E5%85%B7%E5%92%8C%E7%AE%97%E6%B3%95%2F20190315-643b.html</url>
    <content type="text"><![CDATA[背景以人类B细胞为例，确定每个细胞在相应细胞过程（例如细胞分化）中的先后顺序。 算法输入：算法的输入是一个 $M{\times}N$ 的矩阵，其中 $M$ 是细胞数量，$N$ 是选取的marker数量。每个marker的丰度由质谱流式细胞技术(Mass Cytometry)测定。 输出：每个细胞的路径打分。此打分值介于0~1之间：0表示路径起点细胞，1代表路径终点细胞。 Wanderlust实际上就是最近邻图(Nearest Neighbour Graph)与EM算法的组合。 下图是摘自原文的算法流程： 图A是输入数据的形象表示。 1.凭先验知识确定一个起始节点（图B红色节点），随机确定 $nl$ 个路标节点（图B紫色节点） 为什么设置路标节点？路标节点起到缓冲噪声干扰的作用。相对于起始点，具有更小的最短路径距离（Shortest Path Distance）。而随机选取可以排除了先验知识的影响。 2.构建k-NNG，该图以邻接矩阵的方式进行存储，计算相连节点间的距离，可选的距离定义有欧式距离、余弦距离、…… 3.NNG下采样：从这个k-NNG构造出 $l$ 个l-k-NNG。算法只在子图上迭代运行，最后取均值作为最终结果。 为什么采样成子图进行计算？前面构造的k-NNG实际上包含了许多与实际情况不符的连接，即“假边”。进一步的随机下采样使得这些“假边”在子图中出现一定程度的缺失，这将增强模型的适应能力（鲁棒性）。 对于每个子图进行迭代优化： 4.初始化每个节点（细胞）的路径打分；起始节点为0，终止节点为1，中间节点的初始打分为该节点到起始节点的最短路径距离。最短路径距离通过Dijkstra算法计算。 初始化两节点连接的方向：距离起始节点路径打分小的节点作为上游节点。 5.对每个目标节点 $t$ 和每个路标节点 $l$ 间的距离进行打分： $$w_{l,t}=\frac{d(l,t)^2}{\sum_m{d(l,m)^2}}$$ 这个打分有什么意义？尚未知晓 6.计算每个目标节点 $t$ 的路径打分，即该目标节点到所有路标节点距离的加权平均： $$traj_t=\frac1{nl}\sum_l{w_{l,t}d(l,t)}$$ 7.根据计算出来的路径打分计算新的方向。 8.重复步骤567直到路径打分收敛。 引用Cell. 2014 Apr 24;157(3):714-25. doi: 10.1016/j.cell.2014.04.005.Single-cell trajectory detection uncovers progression and regulatory coordination in human B cell development.Bendall SC1, Davis KL2, Amir el-AD3, Tadmor MD3, Simonds EF4, Chen TJ5, Shenfeld DK3, Nolan GP6, Pe’er D7.]]></content>
      <categories>
        <category>生信工具和算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10系统激活]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20190301-83a5.html</url>
    <content type="text"><![CDATA[晚上打开公司发的个人电脑，桌面右下角的“激活windows”提示十分难受，遂上网了一下激活方法。 Win + X 选择命令提示符(管理源)打开cmd 三行命令激活：1234567891011121314//Win10专业版(2019年3月26日测试可用)slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GXslmgr /skms kms.03k.orgslmgr /ato//Win10企业版slmgr /ipk NPPR9-FWDCX-D2C8J-H872K-2YT43slmgr /skms kms.03k.orgslmgr /ato //Win10家庭版slmgr /ipk TX9XD-98N7V-6WMQ6-BX7FG-H8Q99slmgr /skms kms.03k.orgslmgr /ato Win10激活密钥key激活次数有限制，不能保证100%激活成功。 2019年3月最新可用KMS激活服务器地址 KMS一键激活服务 KMS一句命令激活windows/office]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
      <tags>
        <tag>win10</tag>
        <tag>激活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[监督学习-广义线性模型01-普通最小二乘法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20190225-724a.html</url>
    <content type="text"><![CDATA[线性模型：输出是输入的线性组合，即： $$y(w,x)=w_0+w_1x_1+\cdots+w_px_p$$ 在sklearn中，变量 coef_ 存储向量 $w=(w_1,\cdots,w_p)$，变量 intercept_ 存储 $w_0$。 1from sklearn import linear_model 线性回归普通最小二乘法(Ordinary Least Squares)就是简单的计算残差和： $$min_w{||Xw-y||_2}^2$$ 这个算法叫做 线性回归(Linear Regression): 123from sklearn import linear_modelestimator = linear_model.LineaRegression(...) 简单的线性回归只在输入X数据集的各特征之间线性不相关时表现良好。 当X的特征线性相关时，估计结果受随机误差影响大，此时就需要进行模型的矫正。 脊回归/岭回归当特征间 共线性（Collinearity）关系较强时，脊回归（Ridge Regression）可以使模型具有收缩能力。 这通过给线性回归添加L2正则项实现： $$min_w{||Xw-y||_2}^2+\alpha{||w||_2}^2，其中\alpha\le0$$ 1234from sklearn import linear_model# 指定关键超参数α的值estimator = linear_model.Ridge(alpha=0.5) 对于α，可以使用交叉验证进行最优解搜索： 12345from sklearn import linear_model# 给定α的取值范围，默认值如下# cv指定交叉验证的折数，默认如下，默认使用留一交叉验证（Leave-One-Out CV）estimator = linear_model.RidgeCV(alpha=[0.1, 1.0, 10.0], cv=None) LassoLasso是用来估计稀疏系数的线性模型，和其变异体广泛用于语义压缩领域？ $$min_w\frac1{2n_{samples}}{||Xw-y||_2}^2+\alpha||w||_1，其中\alpha\le0$$ 1234from sklearn import linear_model# α默认值如下estimator = linear_model.Lasso(alpha=1.0)]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[获得一组数的全排列]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20190221-9924.html</url>
    <content type="text"><![CDATA[自己实现的python函数。123456789101112131415161718192021222324def permutation(xs): if isinstance(xs,str): def str2charArray(str): charArray = [] for i in str: charArray.append(i) return charArray xs = str2charArray(xs) if len(xs) == 0 or len(xs) == 1: return [xs] result = [] for i in range(0,len(xs)): temp_list = xs[:] temp_list.pop(i) temp = permutation(temp_list) for j in temp: j.insert(0,xs[i]) result.append(j) return result]]></content>
      <categories>
        <category>python编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[glob内建模块]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20190221-3133.html</url>
    <content type="text"><![CDATA[glob模块是python的一个很基础、很简单的模块，用于匹配文件路径。 glob这个单词本身有“通配符”的意思，通配的一个很关键的应用就是筛选出符合条件的文件。 与python的另一个专门用于正则匹配的 re模块 不同，glob只需要三个通配符：*、？、[]。 以下是常见的匹配情形： glob.glob(&#39;/a/b/*.txt&#39;): 匹配目录 /a/b/ 下的所有.txt文件 glob.glob(&#39;/a/b/^[xyz]*.txt&#39;): 匹配目录 /a/b/ 下所有文件名以字母xyz中任意一个开始的文件 glob.glob(&#39;/a/*/*.txt&#39;): 匹配目录 /a/ 下所有的.txt文件 此外，glob.glob() 是一次查询完所有结果。在查询结果较多时，可以使用 glob.iglob() 迭代查询，glob.iglob() 返回一个生成器。]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os内建模块]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20190104-af30.html</url>
    <content type="text"><![CDATA[os模块用于处理文件系统中的文件和目录。 工作目录 os.getcwd() 当前脚本文件的工作目录 os.chdir(DIR) 切换工作目录至DIR，默认工作目录为脚本所在目录 目录的增删查改 新建目录 os.mkdir(“A/B/“) 创建一个目录A/B/，不能递归创建，即要求目录A存在 os.makedirs(DIR) 创建一个目录DIR，与上面不同的是，可以递归创建目录 删除目录 os.rmdir(DIR) 当目录DIR为空时删除目录，不为空时报错 os.removedirs(DIR) 待查 列举目录 os.listdir(DIR) 列出直接属于目录DIR的文件和子目录 os.walk(DIR) 遍历目录DIR下所有的文件和目录，返回生成器，返回结果较复杂，待查 文件和目录的重命名 os.rename(OLD_NAME, NEW_NAME) os.system(“COMMAMD_STRING”) 调用shell命令进行重命名 路径操作 路径类型判断 os.path.isfile(PATH) 判断是否为文件，是文件返回True os.path.isdir(PATH) 判断是否为目录，是目录返回 True os.path.exists(PATH) 判断是否存在，存在返回 True os.path.getsize(PATH) 是文件返回文件大小，是目录返回0 路径的切割、合并 os.path.split(‘1/2/3’) 分割成目录和文件，得到 (&#39;1/2&#39;,&#39;3&#39;) os.path.split(‘1/2/3/‘) 分割成目录和文件，得到 (&#39;1/23&#39;,&#39;&#39;) os.splitext(‘path/name.txt’) 分割出文件后缀，得到 (&#39;path/name&#39;, &#39;.txt&#39;) 路径的连接 os.path.join(A, B) 使用默认路径分割符连接两个字符串，得到 &quot;A/B&quot; 常量 os.sep 或 os.path.sep 当前系统下是使用的 路径分割符 os.linesep 当前系统下使用的 行终止符 os.environ 字典：环境变量]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取当前文件所在的目录]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20190103-5c31.html</url>
    <content type="text"><![CDATA[记录下常用的方法。 利用os模块12# 当前脚本文件所在的目录（工作目录）os.getcwd() # Get current work directory 这个方法不一定得到正确结果！ 利用内建数组sys.argvsys.argv 数组的第一个值（sys.argv[0]）存储的永远是当前脚本文件的绝对路径。 从这个路径中去掉文件名就是当前脚本文件所在目录的绝对路径。 12345import sysimport os# current file directorycur_fdir = os.path.split(sys.argv[0])[0]]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客改造计划]]></title>
    <url>%2F%E5%BF%AB%E6%8D%B7%E6%B8%85%E5%8D%95%2F20190102-19a0.html</url>
    <content type="text"><![CDATA[比较有意思的优化过程。 博文置顶修改hexo-generator-index插件：备份文件node_modules/hexo-generator-index/lib/generator.js并将其中代码替换为：123456789101112131415161718192021222324252627282930313233'use strict';var pagination = require('hexo-pagination');module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; // 两篇文章top都有定义 if(a.top &amp;&amp; b.top) &#123; // 若top值一样则按照文章日期降序排 if(a.top == b.top) return b.date - a.date; // 否则按照top值降序排 else return b.top - a.top; &#125; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） else if(a.top &amp;&amp; !b.top) &#123; return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; // 都没定义按照文章日期降序排 else return b.date - a.date; &#125;); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, &#123; perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: &#123; __index: true &#125; &#125;);&#125;; 设置hexo new生成博文时自动添加top元数据：将top:添加到文件scaffolds\post.md:1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories: top: tags:--- 设置top值即可，top值越大，文章越靠前 fork me on github效果如下： 在themes\next\layout\_layout.swig中搜索&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;找到如下位置：123&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;&lt;!-- 这个地方 --&gt;&lt;header id=&quot;header&quot; class=&quot;header&quot; itemscope itemtype=&quot;http://schema.org/WPHeader&quot;&gt; 添加代码：12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!-- fork me on github begin --&gt;&lt;!-- 默认启用右上角图标，需要换成左上角请更换注释部分 --&gt; &lt;!-- 右上角 --&gt; &lt;a href=&quot;https://github.com/barwe&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt; &lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; class=&quot;fork-me-on-github&quot; style=&quot;position: absolute; border: 0;&quot; aria-hidden=&quot;true&quot;&gt; &lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;/a&gt; &lt;style&gt; .github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; @keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125; @media (max-width:500px)&#123; .github-corner:hover .octo-arm&#123;animation:none&#125; .github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; &#125; &lt;/style&gt; &lt;!-- 左上角 &lt;a href=&quot;https://github.com/barwe&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt; &lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; class=&quot;fork-me-on-github&quot; style=&quot;position: absolute; border: 0; transform: scale(-1, 1);&quot; aria-hidden=&quot;true&quot;&gt; &lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;style&gt; .github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; @keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125; @media (max-width:500px)&#123; .github-corner:hover .octo-arm&#123;animation:none&#125; .github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; &#125; &lt;/style&gt; &lt;/a&gt; --&gt; &lt;!-- fork me on github end --&gt; 在themes\next\source\css\_custom\custom.styl文件中添加：12345678//在右上角或者左上角添加fork me on github图块.fork-me-on-github &#123; fill: red // 背景色 color: white // 猫的颜色 top: 0 right: 0 // 在右上角 //left: 0 // 在左上角&#125; 这个功能刷新可能需要重启服务器。]]></content>
      <categories>
        <category>快捷清单</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统信息查询]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20190101-1ff5.html</url>
    <content type="text"><![CDATA[emmmmmm,先记下来…… 系统uname -a：内核、操作系统、CPU信息 head -n 1 /etc/issue：操作系统版本 cat /proc/cpuinfo：CPU信息 hostname：计算机名 lspci -tv：列出所有PCI设备 lsusb -tv：列出所有USB设备 lsmod：列出加载的内核模块 env：查看环境变量 内存与资源free -m：查看内存使用量和交换区使用量 df -h：查看各分区使用情况 du -sh DIR：查看目录DIR的大小，非本人的目录可能要使用 sudo 提权 grep MemTotal /proc/meminfo：查看内存总量 grep MemFree /proc/meninfo：查看空闲内存量 uptime：查看系统运行时间、用户数、负载 cat /proc/loadavg：查看系统负载 磁盘和分区信息mount | column -t：查看挂接的分区状态 fdisk -l：查看所有分区 swapon -s：查看所有交换分区 hdparm -i /dev/hda：查看磁盘参数 dmesg | grep IDE：查看启动时IDE设备检测状况 网络ifconfig：查看所有网络接口的属性 iptables -L：查看防火墙设置 route -n：查看路由表 netstat -lntp：查看所有监听端口 netstat -antp：查看所有已经建立的连接 netstat -s：查看网络统计信息 进程ps -ef：查看所有进程 top：是实现显示进程状态 用户w：查看活动用户 id USERNAME：查看用户USERNAME的信息 last：查看用户登录日志 cut -d: -f1 /etc/passwd：查看系统所有用户 cut -d: -f1 /etc/group：查看系统所有组 crontab -l：查看当前用户的计划任务 服务chkconfig --list：列出所有系统服务 chkconfig --list | grep on：列出所有启动的系统任务]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
      <tags>
        <tag>知识手册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04LTS修改软件源]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20181122-a661.html</url>
    <content type="text"><![CDATA[修改软件源能大大加快软件更新和下载速度。 1、备份原始源： 1sudo mv /etc/apt/sources.list /etc/apt/sources.list.bak 2、新建源： 1sudo vim /etc/apt/sources.list 写入以下内容（对于Ubuntu16.04LTS）： # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # 预发布软件源，不建议启用 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 其他版本的源列表可以在 这里 查看。 3、刷新：sudo apt-get update]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04LTS更新R到3.5.x]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20181122-ad7f.html</url>
    <content type="text"><![CDATA[重装R时踩得坑。 1、系统换源 2、检查旧版本R 检查是否有旧版本的R：R --verion 卸载旧版本的R：sudo apt-get remove r-base-core 3、安装R3.5需要先添加源： 1sudo apt-add-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/' 添加上面这个源意味着在Ubuntu16.04LTS上可以安装R3.5。 xenial 意为“非洲地松鼠”，Ubuntu的每一个发行版都会有一个奇怪的名字。 然后更新源： 1sudo apt-get update 4、安装R3.5： 1sudo apt-get install r-base r-base-dev]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python中运行shell命令]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20181121-b9c.html</url>
    <content type="text"><![CDATA[记录常用方法。 Method 1 123import subprocessstring = subprocess.check_output("COMMAND_STRING", [shell=True]).decode() Method 2 123import osresult = os.system("COMMAND_STRING")]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-vim设置优化]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20181121-5ec8.html</url>
    <content type="text"><![CDATA[设置tab宽度，显示行号，自动缩进… vim ~/.vimrc 打开文件（不存在则新建），键入（#后面为注释）： # 将tab替换为4个空格 set tabstop=4 # Backspace时删除一个tab set softtabstop=4 # 显示行号 set nu #自动缩进 set autoindent]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HttpRequest对象]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20181121-f755.html</url>
    <content type="text"><![CDATA[每个view函数函数的第一个参数都是HttpRequest对象，包含当前请求URL的一些信息。HttpRequest对象实例： 1234567891011121314151617181920212223## 属性request.path # str类型，请求页面的全路径，不包括域名request.method # str类型，值为'GET'或者'POST'request.GET # QueryDict实例request.POST # QueryDict实例，注意区分POST为空和POST的请求内容为空，判断是否为POST方法使用method属性request.COOKIES # 标准python字典对象(用&#123;&#125;表示)，&#123;str:str&#125;request.FILES # 类字典对象（？），包含所有上传文件# 形式为：XxxDict&#123;name: &#123;'filename':..., 'content-type':..., 'content':...&#125;&#125;# 上面的name变量的值是&lt;input type="file" name="..."&gt;中name属性的值# 只有POST请求并且啥啥啥的FILES属性才会有值，否则为空request.META # 可用的http头部信息字典request.user # django.contrib.auth.models.User对象实例，代表当前登录的用户# 用户未登录则为django.contrib.auth.models.AnonymousUser对象实例# 通过 request.user.is_authenticated() 判断用户是否登录# 需要激活django的AuthenticationMidlleware属性request.session # 当前会话的字典对象，需要激活啥啥啥request.raw_post_data # 未解析的原始post数据## 方法request.__getitem__(key) # 取出GET/POST中的值，优先POSTrequest.has_key(key) # 检查GET/POST是否包含keyrequest.get_full_path() # 返回包含查询字符串的请求路径字符串request.is_secure() # 如果发出的是HTTPS安全请求返回True]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-request(GET+POST)属性与QueryDict类.md]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20181121-8d33.html</url>
    <content type="text"><![CDATA[在HttpRequest对象中，GET和POST属性的值都是QueryDict的实例。 QueryDict用来处理单键对应多值的情况 123456789101112131415161718qd = django.http.QueryDict(...)#与普通字典一致的方法qd.__getitem__(key) # 返回key对应值列表的最后一个值qd.__setitem__(key, value_list) # GET和POST属性值字典不允许被直接修改，因此此方法只能用于该字典的拷贝上qd.get(key, IF_NONE) # 如果key存在返回key对应值列表的最后一个值qd.update(d) # D为QD(查询字典)或者D(python字典)都可以,如果key存在，执行添加而不是替换qd.items() # 返回键值对，值是key对应的值列表的最后一个值（单值）qd.values() # 跟items一样使用单值逻辑#特有方法qd.copy() # 返回可更改的拷贝（比如可以使用__setitem__)qd.getlist(key) # 返回key对应的python列表qd.setlist(key, value_list) # 无须拷贝直接修改？？？？？qd.appendlist(key, value) # 给已经存在的key的列表中添加一个值qd.setlistdefault(ket, value_list)qd.lists() # 作用与items类似，不执行单值逻辑，也就是说键值对的值是所有值的列表qd.urlencode(key) # 返回查询字符串格式的字符串，如'a=3&amp;a=4&amp;a=5']]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-模板系统-自动转义]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20181121-b01.html</url>
    <content type="text"><![CDATA[自动转义的产生背景当对模板变量进行替换时，替换字符串中可能含有能产生非预期影响的元素。例如&quot;&lt;script&gt;alert(&#39;hellp&#39;)&lt;/script&gt;&quot;将弹出警告框用户利用这个特点做一些不可描述的事情 —— 跨域脚本（XSS）攻击。 自动转义的详细操作为了防止这种情况出现，django引入自动转义机制，默认开启。以下5 个字符尤其重要： &lt;自动转为&amp;lt &gt;自动转为&amp;gt &#39;自动转为&amp;#39 &quot;自动转为&amp;quot &amp;自动转为&amp;amp 手动关闭自动转义有时候我们确实是希望模板变量被替换成一段HTML代码 来自用户的数据将进行自动转义 使用safe过滤器关闭单个模板变量的自动转义功能 1&#123;&#123; data | safe&#125;&#125; 控制模板块的自动转义 123&#123;% autoescape off %&#125; ...&#123;% endautoescape %&#125; 是否自动转义可根据模板间的继承进行转移 default过滤器的参数（字符串）的自动转义因为这个常亮字符串是由模板作者定义的，默认已经通过了safe过滤器。所以模板作者需要人工转义：定义1&#123;&#123; data|default:"3 &amp;lt; 2" &#125;&#125; 而不是 1&#123;&#123; data|default:"3 &lt; 2" &#125;&#125;]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-模板系统-扩展模板系统]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20181121-c04f.html</url>
    <content type="text"><![CDATA[绝大部分的模板定制是以自定义标签/过滤器的方式来完成的。 怎么编写自己的模板库文件？这里的自定义模板库指的就是自定义模板标签/过滤器。 模板库文件显然有他固定的书写格式，我们需要在模块开头写上： 12from django import templateregister = template.Library() 模块级变量register是自定义模板标签/过滤器的基础数据结构。 实际上我们在定义自己的模板库文件时可以参考官方的写法: django/template/defaultfilters.py文件 django/template/defaulttags.py文件 然后我们基于register来自定义模板标签/过滤器: 自定义模板过滤器：定义 + 注册过滤器本质上就是带参数的python函数（哈哈，没想到吧）。第一个参数应该传递管道符(|)入口的值，其他参数通过:进行传递。例如下面的这个过滤器： 12def cut(value, arg): return value.replace(arg, '') 在模板中可以用来去掉模板变量值中的字符’A’: 1&#123;&#123;somevalue|cut:"A"&#125;&#125; 过滤器总是有可正常使用的返回值，不能触发异常（关于触发异常我的理解是：使用raise抛出一个异常，而不是使用try...except...进行异常捕获）。 不知道你发现没有，在上面我们定义模板过滤器的过程中还没有用到register，实际上当我们定义好模板过滤器后需要对他们进行注册才能正常使用，使用一下语句注册： 1register.filter('cut', cut) //(过滤器名称，函数本身) 实际上我们可以还可以使用@修饰器在定义的时候进行注册： 1234//无参时直接@register.filter@register.filter(name='cut') def cut(value, arg): return value.replace(arg, '') 自定义模板标签先略过,有需要再研究。。POTAL 模板库文件放在哪里好？模板库是Django能够导入的基本结构。 建议的目录结构如下： 为自定义的模板库单独建一个app并在INSTALLED_APPS中注册（只有注册的模板库才能被导入）。 在合适的app根目录下为模板们建一个单独的文件夹。 在这个文件夹下建立__init__.py文件和自定义模板库文件。 自定义的模板库怎么导入到模板中？ 这里注意对模板库和模板的概念进行区分 我们在编写模板时可以使用如下语句来导入我们自定义的模板库： 1&#123;% load 模板库名 %&#125; load模板标签会检查INSTALLED_APPS，只有已安装的app内的模板库才能被加载。这里模板库虽然是存放在某个特定的APP内的，但是load加载模板库时并没有涉及到这个APP。]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模板系统-RequestContext和Context处理器]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20181121-276f.html</url>
    <content type="text"><![CDATA[视图函数需要传递一个context给模板完成渲染，然后返回给用户完整页面。当向多个不同的模板中传入大量相同的键值对时会多写很多重复的代码，可以使用 RequestContext。 首先定义这些可复用的键值对 12def common_items(request): return &#123;...&#125; 这个函数接受request纯粹是因为里面构造返回值字典时可能会用到换言之，它不是必须的：当你确定返回值字典用不着request完全可以不传参 构造视图函数 12345from django.template import RequestContext def view_1(request, *args, **kwargs): rcontext = RequestContext(request, d&#123;...&#125;, processors=[...]) ... 视图函数的第一个参数必须是request（HttpRequest对象实例）这里我们使用 RequestContext 对象代替 Context 对象构造RequestContext需要注意： 第一个参数时request 第二个参数是字典，代表非公用的键值对 processors是包含Context处理器的列表/元组 将RequestContext对象传递给render_to_response方法进行渲染与Context对象不同，使用RequestContext除了传递处理器外还需要传递一个额外的字典（见上第二个参数）使用关键字context_instance: 1234return render_to_response(模板文件, 非公用字典, context_instance=RequestContext(request, processors=[...]))#这里在构建RequestContext对象的时候并没有传递非公用字典,#而是将该字典传递给了render_to_response方法！！！ 但是频繁的键入processors还是会产生大量的代码（这个是真的懒。。）所以Django设计了全局context处理器：一般在settings.py的类似于’context_processors’的列表中声明，不同版本可能关键字不一样激活相应的处理器RequestContext将自动包含相对应的一部分变量到context中，具体如下： django.core.context_processors.auth: user：一个django.contrib.auth.models.User/AnonymousUser实例 message：当前登录用户的消息列表 perms：当前登录用户的权限 django.core.context_processors.debug: debug：settings.DEBUG值，检测是否处于debug模式，貌似一直为True？ sql_queries：顺序记录每个SQL查询以及耗费时间 这个处理器还需要满足一些其他的条件… django.core.context_processors.i18n: LANGUAGES LANGUAGE_CODE django.core.context_processors.request django.core.context_processors.messages]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[方差分析2-单因素方差分析]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20181118-3a7c.html</url>
    <content type="text"><![CDATA[单因素实验是最简单的，人活在世，肯定不会自己难为自己，因此掌握最基础、最根本的单因素方差分析是十分必要的。 单因素方差分析的目的是：分析某个因素的不同水平之间的差异。 前面在讲方差分析的基本原理时使用的例子实际上就是单因素的方差分析。 在进行单因素的实验设计时，不同处理（组）的样本量一般都是相同的（n），请钻牛角尖的同学不要自己难为自己。 下面这张公式表展示了单因素方差分析的精华： 下面用一个例子详细阐释单因素方差分析的步骤。 题目如下： 步骤： 1、搞清楚 $n$ 和 $k$：$n$ 是每个组的样本量，$k$ 是分组个数，这里 $n=4,\ k=5$ 2、计算每组对应的 和($T_i=\sum{x}$) 和 平方和($\sum{x^2}$) &emsp;&ensp;计算所有组的 总和($T=\sum\sum{x_{ij}}=530.5$) 和 总平方和($\sum\sum{x_{ij}^2}=14258.21$) 3、计算 $C=\frac{T^2}{nk}=\frac{530.5^2}{4{\times}5}=14071.51$ &emsp;&ensp;计算 $SS_T=\sum\sum{x_{ij}^2}-C=14258.21-14071.51=186.7$ &emsp;&ensp;计算 $SS_t=\frac1n\sum_{i=1}^{k}{T_i^2}-C=\frac{126.4^2+…+91.4^2}{4}-14071.51=173.71$ &emsp;&ensp;计算 $SS_e=SS_T-SS_t=186.7-173.71=12.99$ &emsp;&ensp;计算 $df_T=nk-1=4{\times}5-1=19$ &emsp;&ensp;计算 $df_t=k-1=5-1=4$ &emsp;&ensp;计算 $df_e=k(n-1)=5{\times}(4-1)=15$ &emsp;&ensp;计算 $S_t^2=\frac{SS_t}{df_t}=\frac{173.71}{4}=43.43$ &emsp;&ensp;计算 $S_e^2=\frac{SS_e}{df_e}=\frac{12.99}{15}=0.866$ &emsp;&ensp;计算 $F=\frac{S_t^2}{S_e^2}=\frac{43.43}{0.866}=50.15$ 查F值临界表知，$F_{0.05\ (4,15)}=3.06$，$F_{0.01\ (4,15)}=4.89$ 比较知，5个地区黄鼬冬季针毛长度差异极其显著。 结果表示如下： 到这里整体的方差分析就做完了，为了详细比较具体两个地区之间的差异是否显著，在进行完整体的方差分析之后可以进行多重比较。 以LSD检验为例解释多重比较的步骤： 计算平均数差数的标准误 $S_{\overline{X_1}-\overline{X_2}}=\sqrt{\frac{2S_e^2}{n}}=\sqrt{2{\times}0.866}{4}=0.658$ 注意，在LSD检验中，不管我们选择哪两个处理进行差异分析，这个 标准误 $S_{\overline{X_1}-\overline{X_2}}$ 都是一样的！相应的，它们使用的最小差数 $LSD_\alpha$ 也是通用的！ $$LSD_{0.05}=t_{0.05}{\cdot}S_{\overline{X_1}-\overline{X_2}}=2.131{\times}0.658=1.402$$ $$LSD_{0.01}=t_{0.01}{\cdot}S_{\overline{X_1}-\overline{X_2}}=2.947{\times}0.658=1.939$$ 然后愉快的进行比较就可以啦 选择一种结果表示法，这里选用字母标记法，表示结果如下： 结果表明，东北与其它地区，内蒙古与安徽、贵州，河北与贵州黄鼬冬季针毛长度差异均达到极显著水平，安徽与贵州差异达到显著水平，而内蒙古与河北、河北与安徽差异不显著。]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写论文值得收藏的8个网站]]></title>
    <url>%2F%E5%BF%AB%E6%8D%B7%E6%B8%85%E5%8D%95%2F20181106-e8e2.html</url>
    <content type="text"><![CDATA[① OALib 免费论文搜索引擎 ( http://www.oalib.com/ ) ② HighWire 斯坦福学术文献电子期刊 ( https://www.highwirepress.com/ ) ③ Intute 学术资源搜索工具 ( https://www.jisc.ac.uk/ ) ④ FindaRticles 文献论文站点 ( http://findarticles.com/ ) ⑤ Intechopen 免费科技文献 ( https://www.intechopen.com/ ) ⑥ LolMyThesis 哈佛毕业论文分享网站 ( http://lolmythesis.com/ ) ⑦ 万方数据库 ( http://www.wanfangdata.com.cn/index.html ) ⑧ 全国图书馆论文搜索网 ( http://www.ucdrs.superlib.net/ ) 以上资料来自知乎同学，点击查看图片和详情。]]></content>
      <categories>
        <category>快捷清单</category>
      </categories>
      <tags>
        <tag>资源列表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方差分析1-基本原理、F检验、多重比较]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20181022-111b.html</url>
    <content type="text"><![CDATA[一、问题引入1.什么是方差分析？方差分析用于两个及两个以上样本的均数差异的显著性检验。 中文名 方差分析 中文别名 变异数分析、变量分析、F检验 外文名 Analysis of Variance, F test 简称 ANOVA 提出者 R.A.Fisher 2.t检验为什么不能代替方差分析？t检验只适用于两个样本均数间的差异分析，当设计两个以上的样本时只能进行二元拆分，这种操作并不优雅。 1.检验过程繁琐。m个样本需要进行$C_m^2$组t检验，自寻烦恼。 2.无统一的试验误差，误差估计的精确性和检验的灵敏性低。（？？？） 3.二元切分后进行多次t检验会使犯第Ⅰ类错误（假阳性错误）的概率大大增加。t检验只能将每次的检验犯错误概率控制在$\alpha$以内，在多次检验时这个概率会逐渐变大。例如一个三样本的均值差异检验共需要进行$C_3^2=3$次t检验，每一次犯错误的概率为$\alpha$，这意味着每一次检验不犯错误的概率为$1-\alpha$，连续三次不犯错误的概率为$(1-\alpha)^3$，所以这三次t检验犯第一类错误的概率为$1-(1-\alpha)^3=0.142615$（当$\alpha=0.05$时），这个概率相比于0.05已经大了很多了。由此可知，当$n{\to}\infty$时，犯第一类错误的概率将会趋近于1。 二、方差分析的基本原理1.思想多个样本一般可以来自多个不同的处理(treatment)，比如注射不同的药物等，这些处理是我们可以控制的。 不同的处理有可能影响并致使实验结果产生差异，这种影响一般是可以人为控制的，这种效应叫做处理效应(treatment effect)。 对每个个体的观测值不同，除了处理效应施加影响，还包括实验过程中偶然性因素和测量误差的干扰。 2.目的方差分析可以确定上面提到的处理效应和实验误差对总差异的贡献程度。 一般来说，实验误差是合理不可控的，同时它对总差异的影响也是很小的。因此我们可以比较处理效应和实验误差。 而实验误差显然不是我们关心的东西，我们关心的应该是处理间（组间）均值是否有差异，即我们关心应该是处理效应：● 处理效应和实验误差相差不大，意味着处理效应对指标影响不大。● 处理效应比实验误差大很多，处理效应对实验结果差异有很大影响。 3.用途 多样本均数的比较 多因素间相互作用的分析 回归方程的假设假设 方差的同质性检验 三、数学模型根据上面对总差异的定性分析（由处理效应和实验误差构成）建立定量的数学模型。 观测数据记录形式一般如下： 其中每列（1,…,k）代表k个不同的处理，即k个样本；行数代表每种处理的重复次数，即一个样本的样本容量。 1.线性等权加和模型这里采用最简洁的线性模型定量描述上述关系： $$\begin{split}x_{ij}&amp;=\mu+\tau_i+\epsilon_{ij}\\i&amp;=1,…,k\\j&amp;=1,…,n\end{split}$$ $\mu$是总体均数； $\tau_i$是第$i$次处理的处理效应； $\epsilon_{ij}$是第$i$次处理中第$j$个样本的实验误差； $x_{ij}$是第$i$次处理中第$j$个样本的观测值； 上面的线性模型只是简单的对总体均值、处理效应和实验误差进行等权加和进而构造出观测值。这种构造形式可能不是最优的形式，但却是最简单最易理解的形式。这进一步印证了解决问题总是一个化繁为简、由简入繁的过程。（当前更优的模型构建方式？？？） 2.处理效应不总是人为可控的虽然说大多数情况下通过实验设计我们可以控制处理的种类，即控制处理效应，但是这在某些实验中$\tau_i$却是不可控的。根据$\tau_i$是否可控将上面的数学模型分为三类： $\tau_i$取值 泛化能力 固定模型 $\tau_i=\mu_i-\mu$为常数且$\sum_{i=1}^{k}{\tau_i}=0$ 结果不能扩展到其他处理 随机模型 $\tau_i$从$N(0,\sigma^2)$中采样 结果可以扩展到其他处理 混合模型 略略略 略略略 3.样本方差表达式求解首先我们要明确方差求解式的结构：$\frac{平方和}{自由度}$ 下面分别求解分子和分母。 (1) 求分子（平方和）上面的模型虽然说简洁，但毕竟还是个花架子，我们需要把它的各个单项同真正的样本联系起来。 那么怎么将处理效应和随机误差同已知样本联系起来呢？这里有两句话自行体会： 组间的平均数差异是由处理效应引起的：$\overline{x_i}-\overline{x}$ 组内的差异是由随机误差引起的：$x_{ij}-\overline{x_i}$ 然后简单变换： $$\begin{split}(x_{ij}-\bar{x})&amp;=(x_{ij}-\overline{x_i})+(\overline{x_i}-\bar{x})\\(x_{ij}-\bar{x})^2&amp;=[(x_{ij}-\overline{x_i})+(\overline{x_i}-\bar{x})]^2\\&amp;=(x_{ij}-\overline{x_i})^2+(\overline{x_i}-\bar{x})^2+2(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})\end{split}$$ 针对某个特定的处理对样本内所有观测数据求和： $\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2+2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})$ 这里有个玄学项 $2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})$，我们将与 $j$ 无关的项提前： $2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})=2(\overline{x_i}-\bar{x})\sum_{j=1}^{n}(x_{ij}-\overline{x_i})$ 均值具有这样一个特性：一个样本内所有观测数据到均值的距离（包含正负号）和为0。 所以有这样一个结论：$2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})=0$ 即$\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2$ 然后我们将不同处理的观测数据的离均差平方进行累加： $\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{i=1}^{k}\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2$ 化简得： $\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\bar{x})^2}_{SS_T}=\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2}_{SS_e}+\underbrace{n\sum_{i=1}^{k}(\overline{x_i}-\bar{x})^2}_{SS_t}$ 即：$SS_T(总平方和)=SS_e(组内平方和)+SS_t(组间平方和)$ 预算 $$\begin{split}T&amp;=\sum_{i=1}^{k}\sum_{j=1}^{n}x_{ij}\\T_i&amp;=\sum_{j=1}^{n}x_{ij}\\C&amp;=\frac{T^2}{kn}\end{split}$$ 得 $$\begin{split}SS_T&amp;=\sum_{i=1}^{k}\sum_{j=1}^{n}x_{ij}^2-C\\SS_t&amp;=\frac{1}{n}\sum_{i=1}^{k}T_i^2-C\\SS_e&amp;=SS_T-SS_t\end{split}$$ (2) 求分母（自由度）分母就是与$SS_T$、$SS_t$和$SS_e$相关的自由度$df$: $$\begin{split}df_T&amp;=nk-1\\df_t&amp;=k-1\\df_e&amp;=df_T-df_t\\&amp;=k(n-1)\end{split}$$ (3) 求方差组间方差 $$\begin{split}s_t^2&amp;=\frac{SS_t}{df_t}\\s_e^2&amp;=\frac{SS_e}{df_e}\end{split}$$ 四、$F$ 检验1.$F$ 值的定义$F$ 值的定义：$$F=\frac{s_1^2}{s_2^2}$$这里 $s_1^2$ 和 $s_2^2$ 都是随机采样于正态总体($\mu$, $\sigma$)，它们可能具有不同的样本容量（自由度）。习惯上，我们让 $F$ 值大于1，即大方差做分子，小方差做分母。 2.公式迁移在方差分析中，我们计算出了两个方差：组间方差 $S_t^2$ 和组内方差 $S_e^2$，它们分别代表了处理效应和实验误差，即$$F=\frac{S_t^2}{S_e^2}=\frac{处理效应}{实验误差}$$ 对于上面的定义需要解释两点： (1) 按照 $F$ 值的定义，分子和分母应该来自同一个正态总体 如果我们假设处理效应和实验误差相差不多（这将作为我们假设检验的原假设/无效假设/零假设），即 $S_t^2{\approx}S_e^2$，此时我们可以认为 $S_t^2$ 和 $S_e^2$ 来自于同一个正态总体。 (2) 按照 $F$ 值的定义，分子是大方差，分母是小方差 方差分析的目的是确定 $S_t^2$（处理效应）和 $S_e^2$（实验误差）的相对大小，由于实验误差这种东西一般情况下都是不可控的，而且也不会很大，所以我们认为一般只存在一下两种情况： 处理效应比实验误差大得多 处理效应与实验误差相差不多 3.$F$ 临界值表 五、多重比较比较其中两组处理平均数间差异的显著性，本质上与t检验没有任何区别 1.最小显著差数法（LSD）Least Significant Difference (1) 检验方法本质上是两均数的t检验。 计算达到差异显著的最小差数，记为 $LSD_\alpha$ $$LSD_\alpha=t_\alpha{\cdot}S_{\overline{X_1}-\overline{X_2}}$$ 其中 $S_{\overline{X_1}-\overline{X_2}}$ 为平均数差数的标准误： $$\begin{split}S_{\overline{X_1}-\overline{X_2}}&amp;&amp;=\sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}\\&amp;&amp;=\sqrt{S_e^2{\cdot}(\frac{1}{n_1}+\frac{1}{n_2})}\\&amp;&amp;=\sqrt{\frac{2S_e^2}{n}}{\quad}if{\quad}n_1=n_2=n\end{split}$$ ☺ 为什么令 $S_1^2=S_2^2=S_e^2$ ?上述公式的推导本身基于我们做出的无效假设（处理效应与实验误差相差不多）！☺ 查表的自由度依据？因为本质上是两个均数差异的t检验，因此需要查t临界值表。因为均数差数标准误最终化归到了 $S_e^2$，因此查表的自由度依据的是 $df_e$ 而不是 $df_t$。 将两个处理平均数的差值绝对值 $\overline{x_1}-\overline{x_2}$ 与 $LSD_\alpha$ 进行比较 (2) 结果表示结果表示方法仅仅是用于优化两两比较次数，在使用结果表示方法之前都需要计算 $LSD_\alpha$。 a. 标记字母法 b. 梯形比较法/三角形法 2.最小显著极差法（LSR）Least Significant Ranges (1) 新复极差检验（SSR）新复极差法用于方差分析后的两两比较，有助于减少第二类错误，但是会增加第一类错误。 原假设仍然是假设两个均值无差异：$\mu_A-\mu_B=0$ 步骤如下： 1.平均数降序排序 2.计算平均数标准误：$$S_{\bar{X}}=\sqrt{\frac{S_e^2}{n}}$$ 这里两组处理的样本容量是一致的：$n_1=n_2=n$，实验设计时一般也不会设计不一致这种骚操作。 3.根据自由度 $df_e$ 和排序平均数中相应两个数之间包含的平均数个数M查SSR值表，计算最小显著极差值（LSR值）： $$LSR_\alpha=SSR_\alpha{\cdot}S_{\bar{X}}$$ M值是个什么东西？排序平均数中相应两个数之间包含的平均数个数，即索引值差+1，相邻两个均数的M值为2。 (2) q检验q检验也叫NK检验，与SSR检验十分相似，不同的是在第二步计算 $LSR_\alpha$ 时使用的是 $q_\alpha$ 值，而不是 $SSR_\alpha$ 值，因此需要查 $q_\alpha$ 值表。 $$LSR_\alpha=q_\alpha{\cdot}S_{\bar{X}}$$ 3.各个方法应用场景上面提到的LSD检验、SSR检验（新复极差检验）、q检验分别有自己的适用场景 因此应用时 对于精度要求高的检验，请使用q检验； SSR检验适用于一般的检验； LSD检验适用于将各个实验组与对照组进行比较。 六、方差分析的基本步骤从上面的过程我们可以总结出方差分析的三个大步骤： 对样本数据的总平方和和总自由度分解为各个变异因素的平方和和自由度； 进行 $F$ 检验，检验各个变异因素在总变异中的重要程度； 组间均数进行两两比较。]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验-显著性假设检验]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20181018-327a.html</url>
    <content type="text"><![CDATA[不管是下面的单样本还是双样本都有两个大前提：总体服从正态分布、方差齐性！ 1. 单样本平均数$\bar{x}$的差异显著性检验这里指的实际上是一个样本（的平均数）和总体（的平均数）间的差异显著性，因为我们只有一个样本，这就要求总体参数（这里是均值）已知，这通常是一个理论值、经验值或者期望值。 (1). 大样本（n≥30）的均数检验大样本通常指的是n≥30，此时样本的均值将服从于正态分布，我们可以将其标准化为标准正态分布，进而使用u检验进行假设检验。 a. 总体方差$\sigma^2$已知时总体方差$\sigma^2$已知时直接计算相应的u值： $$\sigma_{\bar{x}}=\sqrt{\frac{\sigma^2}{n}}=\frac{\sigma}{\sqrt{n}}$$ $$u=\frac{\bar{x}-\mu_0}{\sigma_{\bar{x}}}=\frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}{\sim}N(0,1)$$ 式中$\bar{x}$是样本均值，$\mu_0$是总体均值，$\sigma$是总体方差，$n$是样本容量。 b. 总体方差$\sigma^2$未知时在大样本的条件下直接用样本方差$s^2$代替总体方差$\sigma^2$，计算u值的方法同上： $$s_{\bar{x}}=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$$ $$u=\frac{\bar{x}-\mu_0}{s_{\bar{x}}}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}{\sim}N(0,1)$$ 样本方差$s^2$的计算： $$s^2=\frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\bar{x})^2}$$ (2). 小样本（n&lt;30）的均数检验小样本只能使用t检验，此时我们要求的是t值而不是u值。 小样本的方差$s^2$与总体的方差$\sigma^2$往往差距较大。 t检验时查临界值表依据的是自由度df，这个值通常是n-1。 $$s^2=\frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\bar{x})^2}$$ $$s_{\bar{x}}=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$$ $$t=\frac{\bar{x}-\mu_0}{s_{\bar{x}}}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}{\sim}t(n-1)$$ 2. 两个样本平均数$\bar{x_1}$和$\bar{x_2}$间的差异显著性检验(1). 两个大样本（$n_1\ge30$且$n_2\ge30$）的均数检验当两个样本都是大样本时，与单样本检验类似，仍可以使用u检验。 a. 如果两个样本的方差$\sigma^1$和$\sigma^2$已知$$\sigma_{\bar{x_1}-\bar{x_2}}=\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$$ $$u=\frac{\bar{x_1}-\bar{x_2}}{\sigma_{\bar{x_1}-\bar{x_2}}}{\sim}N(0,1)$$ b. 如果两个样本的方差$\sigma^1$和$\sigma^2$未知使用$s_{\bar{x_1}-\bar{x_2}}$来代替$\sigma_{\bar{x_1}-\bar{x_2}}$: $$s_{\bar{x_1}-\bar{x_2}}=\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}$$ $$u=\frac{\bar{x_1}-\bar{x_2}}{s_{\bar{x_1}-\bar{x_2}}}{\sim}N(0,1)$$ (2). 两个小样本（$n_1\le30$且$n_2\le30$）的均数检验小样本使用t检验。 按照实验设计的不同，两个样本可以是独立样本，也可以是配对样本。 A. 配对样本均数的t检验此时两组数据将形成对子，因此两个样本的样本容量是一样的，即$n_1=n_2$。 配对可以减少个体差异对实验的影响，但是也会增加数据处理的复杂度。 配对情况通常有以下三类： 两个相似个体组成一对进行不同的实验处理：此时并没有完全消除个体差异的影响。 同一个个体同时进行两种不同的处理：处理的空间位置不同。 同一个个体先后进行两种不同的处理：不同时间个体的状态可能不同。 简单理解就是：第一类使用相似的两个个体，第二类同时使用同一个个体不同的部位，第三类先后使用同一个体相同的部位。 计算过程如下： $$d_i=x_i-y_i$$ $$\bar{d}=\frac1n\sum_{i=1}^{n}d_i$$ $$s_d^2=\frac{1}{n-1}\sum_{i=1}^{n}(d_i-\bar{d})^2$$ $$t=\frac{\bar{d}}{s_d/\sqrt{n}}$$ $$df=n-1$$ 查t检验临界值表时注意使用df值而不是n值。 B. 独立样本均数的t检验对于独立样本不存在配对情况，因此$n_1$和$n_2$往往不同。 两独立样本对应的总体方差可能不同，因此： (a). 当两样本方差相同或者假设相同时此时计算自由度$v=n_1+n_2-2$ 计算两个样本的均值$\bar{x_1}$和$\bar{x_2}$、方差$S_1^2$和$S_2^2$。 $$S_c^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{v}$$ $$S_{\overline{X_1}-\overline{X_2}}=\sqrt{(\frac{1}{n_1}+\frac{1}{n_2})S_c^2}$$ $$t=\frac{\overline{x_1}-\overline{x_2}}{S_{\overline{X_1}-\overline{X_2}}}$$ (b). 经过F检验两样本方差不同时如果$n_1=n_2=n$: 计算方法与假设方差相同时一致，只是自由度为$n-1$而不是$2n-2$。 没看懂，先抄下来…… 3. u分布与t分布的区别和联系u检验理论上要求大样本（即$n\to+\infty$），但是这在实际情况中是不可能的，所以教科书设置标准为n=30。 按照中心极限定理：在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。这就是说，只有在大（无穷）样本时，样本的均值才会服从正态分布。按照正态分布的概率密度函数： $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2{\sigma}^2}}$$ 它只与总体的参数$\mu$和$\sigma$有关，而与样本容量n无关。 有限样本特别是小样本下样本均值是不严格服从正态分布的，此时有个更好的分布可以刻画样本均值，这就是t分布。 $$f(t)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{\frac{-(\nu+1)}{2}}$$ $$\nu=n-1$$ 它只与样本容量n有关！ 但是请注意，当$n\to+\infty$时，t分布将会十分接近标准正态分布，有图为证： 所以我们可以说，标准正态分布只是t分布在$n\to+\infty$时的一个特例！ 因此在有限样本容量情况下，t检验应该更合理和精确，所以在t检验可行的情况下应该优先采用t检验。什么时候不可行呢？t检验是要根据自由度查表的，如果自由度值不合适导致无法查表，此时就比较尴尬了。也不知道现在这个问题是怎么解决的~。 下面是一张常见的t分布临界值表，我们可以发现当自由度超过30以后就不连续了~（不是绝对的，但道理大抵如此吧） 同时由上图我们知道，当n=30的时候，t分布和u分布已经十分近似了，所以才有了这个样本容量大于30时用u检验的规则。 所以 ▶ u检验适用于大样本的情景，t检验适用于小样本的情景。 ▶ t分布更加符合有限样本的场景，理论上t检验更加精确。 4. 两独立样本的方差的齐性检验原假设$H_0$: $\sigma_1=\sigma_2$ (方差齐性) 备择假设$H_1$: $\sigma_1\ne\sigma_2$ (方差不齐性) $$F=\frac{max(S_1^2,S_2^2)}{min(S_1^2,S_2^2)}$$ $$v_1=n_1-1$$ $$v_2=n_2-1$$ 查F检验临界值表。]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验-假设检验的基本原理]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20181017-aa04.html</url>
    <content type="text"><![CDATA[显著性检验的意义以均数差异显著性检验来说，两样本平均数为$\bar{x_1}$和$\bar{x_2}$，样本背后对应的总体均值相应的为$\mu_1$和$\mu_1$。下式十分容易理解： $$\bar{x_1}=\mu_1+\bar{\epsilon_1}$$ $$\bar{x_2}=\mu_2+\bar{\epsilon_2}$$ 其中$\bar{\epsilon_1}$和$\bar{\epsilon_2}$均表示实验误差。 当我们检验这两个样本的均数是否存在差异时依据的实际上是下面这个式子： $$\bar{x_1}-\bar{x_2}=(\mu_1-\mu_2)+(\bar{\epsilon_1}-\bar{\epsilon_2})$$ 即样本均数间的差异（表面差异）是由总体间的真实差异和抽样误差共同决定的。 所以显著性检验的目的就是：判明表面差异$\bar{x_1}-\bar{x_2}$是来源于总体真实差异$\mu_1-\mu_2$还是抽样误差$\bar{\epsilon_1}-\bar{\epsilon_2}$。 原假设与备择假设的选取统计学为实际目的服务，我们对某个总体进行抽样都是有确切目的的。换句话说，我们往往希望通过自己的努力搜集一定的证据去证明一些东西，这实际上是一个立场问题。譬如质量检测人员希望检测出劣质产品而不是合格产品，因为其工作内容就是努力找出不合格的产品；给作物施加改良药物希望的是作物产量提高而不是一成不变… 一般情况下我们都希望总体的参数存在差异（当然这也不是绝对的~），这是我们假设检验的方向，此时我们进行试验的初衷也是希望改变总体的参数已完成我们的工作目标。我们将与假设检验方向相同的假设称为备择假设，与备择假设再逻辑上完备互斥的假设称为原假设（无效假设、零假设）。 所以原假设往往希望总体参数不发生变化，即表面差异由抽样误差决定。我们假装接受原假设，然后对原假设进行检验，实际上就是对表面差异由抽样误差决定这一基于原假设的结论进行检验。后续通过求解P(表面差异由抽样误差决定)通过小概率事件不可能发生原理对原假设予以接受或者拒绝。 ◆ 由于原假设假定总体参数未发生变化，所以等号往往出现在原假设中 ◆ 原假设与备择假设逻辑上完备互斥，且原假设由备择假设确定 ◆ 备择假设的选择往往和实验人员的立场息息相关 下面对案例进行详细分析： 案例1 零件质量问题背景：一汽车配件生产企业生产的某种汽车零件长度标准为70毫米，为对零件质量进行控制，质量监测人员需要对生产线上的一台加工机床进行检查，以确定这台机床生产的零件是否符合标准要求。如果零件的平均长度大于或小于70毫米，则表明该零件质量不正常，必须对机床进行检查。分析：“70毫米”是（合格产品）总体的参数，质量检测人员将进行抽样，需要比较这个样本对应的总体和合格产品总体。质量检测人员希望通过搜集证据证明的是样本不合格（其工作任务总是希望找出不合格的产品），因此备择假设为“零件的平均长度大于或小于（不等于）70毫米”（这是检测人员希望通过努力证明的结果）。由备择假设确定原假设为“零件的平均长度等于70毫米”。 案例2 合格率问题背景：一采购商需要采购一批构件，某供应商称其提供的构件合格率超过95%，为了检验其可信度，采购商随机抽取了一批样本进行检验。 分析：“95%”是总体的参数。采购商的立场总是尝试证明构件合格率不超过95%，这也是总体参数发生改变的假设，因此我们将之作为备择假设。由备择假设确定的原假设为“构件合格率≥95%”。 案例3 成分含量问题背景：一乳制品生产商生产的奶粉被媒体曝光某种营养成分大大低于国家规定的2%的含量标准。 ▶ 现质量监督部门从保护消费者权益角度出发，对曝光的奶粉进行抽查，请对检验做出假设。 分析：质量监督部门的立场是通过搜集证据证明该成分含量低于2%，因此备择假设为“成分含量&lt;2%”，原假设为“成分含量≥2%”。 ▶ 如果生产商相信其产品不存在上述问题，判断这是由竞争对手操纵的不正当竞争手段，并委托市场上的第三方检测机构进行检测，请对这一检验作出假设。 分析：第三方检测机构的立场是通过自己的努力证明成分含量是合格的（因为这也是生产商希望看到的，如果这个检测机构跟金主爸爸做对就是有病了~），因此备择假设为“成分含量≤2%”，原假设为“成分含量&gt;2%”。 ◆ 有趣的是，上面同一个情景却因为立场和初衷的不同导致备择假设和原假设完全相反，这表明统计学真的源于生活啊hahaha （注：尊重原创，以上案例均来自于博客。） 显著性检验的一般步骤1.提出假设大多数情况下都是： $$H_0: \mu=\mu_0$$ $$H_1: \mu\ne\mu_0$$ 2.确定显著水平显著水平$\alpha$（significance level）即否定$H_0$的概率标准，人为规定的小概率事件分界线。生物统计学中通常取0.05和0.01。 3.计算显著概率在假定原假设的条件下求表面差异是由抽样误差造成的的概率。 对于总体参数μ已知、单样本均值$\bar{x}$可求条件下，表面差异为$\bar{x}-\mu$，这个不难理解哈。 ◆ 判定是两尾检验还是单尾检验。 4.统计推断根据小概率事件实际不可能性对原假设予以接受或者否定。 检验结果被表述为“在α水平上$\bar{x}$与μ差异不显著/显著/极其显著”。 不显著：|u|&lt;1.96，即p&gt;0.05 显著：1.96≤|u|&lt;2.58，即0.01&lt;p≤0.05 极其显著：|u|≥2.58，即p≤0.01 显著水平与两种类型的错误实际上原假设正确，但我们却否定了原假设而接受备择假设时会犯第Ⅰ类错误：原假设正确意味着总体参数本来是相同的，但是我们推断认为他们不同，即将抽样误差错判为了真实差异。 犯第Ⅰ类错误的概率是α，因此减少犯第Ⅰ类错误的概率只需要减少α的值即可。 本来我们应该否定原假设，但是推断后却没有否认原假设时会犯第Ⅱ类错误：本应该否认原假设意味着总体参数间是存在差异的，但是我们没有否认原假设说明我们将这个真实差异错判为了抽样误差。 当表面差异很小或者抽样误差很大时就十分容易犯第Ⅱ类错误。 表面差异或许不可控制，但是我们可以通过增大样本量来减小抽样误差 ◆ 第Ⅰ类错误拒绝了正确的原假设，第Ⅱ类错误接受了错误的原假设。 ◆ 第Ⅰ类错误又叫做假阳性错误：因为我们接受了错误的备择假设。 两尾检验和一尾检验◆ 两种检验的适用情况？如果仅仅检验是否存在差异，即不在乎谁大谁小，采用两尾检验； 凭经验和专业知识断定取大（或者取小），采用一尾检验。 ◆ 一尾检验的临界值怎么确定？ 对于两位检验可以直接查表获取临界值，一尾检验需要先转化为两尾检验再求临界值，如上图所示：$u_{2\alpha}$的值可以直接通过两尾检验的表获取。所以有下面的这个公式： $$一尾检验的u_{\alpha}=两尾检验的u_{2\alpha}$$ $$一尾检验的u_{0.05}=两尾检验的u_{0.10}=1.64$$ $$一尾检验的u_{0.01}=两尾检验的u_{0.02}=2.33$$]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[latex数学符号表]]></title>
    <url>%2F%E5%BF%AB%E6%8D%B7%E6%B8%85%E5%8D%95%2F20181017-aedb.html</url>
    <content type="text"><![CDATA[Latex，展现公式的美！ 重音符号 $\overline{m+n}$，代码：\overline{m+n} → \overline{} $\underline{m+n}$，代码：\underline{m+n} → \underline{} $\overbrace{a+b+\cdots+z}^{26}$，代码：\overbrace{a+b+\cdots+z}^{26} → \overbrace{}^{} $\underbrace{a+b+\cdots+z}_{26}$，代码：\underbrace{a+b+\cdots+z}{26} → \underbrace{}&#95;{} 为避免markdown将 _..._ 解释为斜体，可用 &amp;#95; 替换其中一个 _。 符号顶部箭头，常用来表示向量： 显示 代码 $\vec{a}$ \vec{a} $\overrightarrow{AB}$ \overrightarrow{AB} $\overleftarrow{AB}$ \overleftarrow{AB} 独立符号希腊字符 其他数学符号 其他AMS数学符号 非数学符号 字体 关系符号关系符 AMS关系符 AMS否定关系符 运算符号运算符 大运算符 AMS运算符 箭头箭头 AMS箭头 括号及定界符定界符 AMS定界符 空格 函数 函数 举例 代码 分数 $\frac{x-y}{x+y}$ \frac{x-y}{x+y} 累加 $\sum_{i=1}^{100}{x_i^2}$ \sum_{i=1}^{100}{x_i^2} 累乘 $\prod_{i=1}^{100}{x_i}$ \prod_{i=1}^{100}{x_i} 偏微分 $\partial{y}$ \partial{y} 一重积分 $\int_{-1}^{1}{x_3}dx$ \int_{-1}^{1}{x_3}dx 排列组合 $\binom{n}{k}$ \binom{n}{k} 排列组合 $\mathrm{C}_n^k$ \mathrm{C}_n^k 其他小技巧基本运算的格式控制 显示 代码 $\frac{a}{b}$ \frac{a}{b} $a_{1}^{(2)}$ a_{1}^{(2)} $\sqrt{5}$ \sqrt{5} $\sqrt[3]{5}$ \sqrt[3]{5} 公式末尾序号 自动标签（不建议使用）：\begin{equation}...\end{equation} 手动标签：...\tag{...} 单个公式的连等换行12345$$\begin&#123;split&#125;y&amp;=x^2-4x+5\\\\&amp;=(x^2-4x+4)+1\\\\&amp;=(x-2)^2+1\end&#123;split&#125;\tag&#123;xx&#125;$$ $$\begin{split}y&amp;=x^2-4x+5\\&amp;=(x^2-4x+4)+1\\&amp;=(x-2)^2+1\end{split}\tag{xx}$$ 要点如下： 公式代码用begin{split}...\end{split}标识以说明需要多行显示一个单行公式； \tag{...}需要放到split区域外面，因为它不属于公式代码； 换行的地方用\\标识，如果在windows系统下可能是\\\\； 在所有行中使用且仅使用一个&amp;标明对齐位置。 分段函数Sign(x)= \begin{cases} 1, & \text{if $x$ > 0;}\\\\ 0, & \text{if $x$ = 0;}\\\\ -1, & \text{else} \end{cases} $$Sign(x)=\begin{cases} 1, &amp; \text{if $x$ &gt; 0;}\\ 0, &amp; \text{if $x$ = 0;}\\ -1, &amp; \text{else}\end{cases}$$ 矩阵12345$$\left[\begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\\\4 &amp; 5 &amp; 6 \\\\7 &amp; 8 &amp; 9 \end&#123;matrix&#125;\right]\tag&#123;2&#125;$$ $$\left[\begin{matrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{matrix}\right]\tag{2}$$]]></content>
      <categories>
        <category>快捷清单</category>
      </categories>
      <tags>
        <tag>知识手册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[估计量选择的原则]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20180928-e722.html</url>
    <content type="text"><![CDATA[以点估计为例，我们可以通过矩估计法和最大似然估计法对总体的某个参数进行估计。我们通过不同的方法对同一个参数的估计结果可能不同，因此我们需要选择一个最好的估计量。 下面是选择估计量时常用的三个标准： 无偏性什么是无偏性？若${X_1,X_2,\cdots,X_n}$是总体的一个样本，样本容量为$n$，$\theta$是与总体分布相关的一个待估参数。 我们基于样本${X_1,X_2,\cdots,X_n}$构造了一个估计量$\hat{\theta}=\theta(X_1,X_2,\cdots,X_n)$，当这歌估计量的期望$E(\hat{\theta})$存在且$E(\hat{\theta})=\theta$时称$\hat{\theta}$是$\theta$的无偏估计量。 简而言之，估计量的期望等于待估参数就是无偏估计。 无系统误差无偏估计意味着没有系统误差，那么有偏估计就意味着有系统误差，而系统误差是可以通过校正去除的，所谓我们可以对有偏估计进行无偏化处理得到完成无偏估计，典型的例子就是方差估计。 期望是无偏的不论总体的分布形式如何，k阶样本矩（样本的k阶原点矩）$A_k=\frac1n\sum_{i=1}^{n}X_i^k$都是k阶总体矩（总体的k阶原点矩）$\mu_k$的无偏估计。只需证明$E(A_k)=\mu_k$即可。 样本常见的数字特征有：样本均值、未修正样本方差、修正样本方差、样本k阶原点矩和样本k阶中心矩。 这意味着：样本均值总是总体期望的无偏估计。 方差是有偏的对于方差存在的总体，用未修正的样本方差$\hat{\sigma}^2=\frac1n\sum_{i=1}^{n}(X_i-\bar{X})^2$来估计总体方差$\sigma^2$是有偏的。 证明：针对样本，由$E(X^2)=[E(X)]^2+D(X)$知$$\begin{split}\hat{\sigma}^2=D(X)&amp;=E(X^2)-[E(X)]^2\\&amp;=A_2-\bar{X}^2\end{split}$$因为期望是无偏的，所以$$\underbrace{E(A_2)=}_{期望无偏}\mu_2\underbrace{=\sigma^2+\mu^2}_{总体期望和方差的关系}$$上式中 $\mu_2$ 是总体的二阶原点矩，$\mu$ 是总体的一阶原点矩（期望）。对于复合随机变量$\bar{X}^2$有$$E(\bar{X}^2)=D(\bar{X})+[E(\bar{X})]^2=\frac{\sigma^2}n+\mu^2$$证明$D(\bar{X})=\frac{\sigma^2}n$:$$\begin{split}D(\bar{X})&amp;=D(\frac{X_1+X_2+\cdots+X_n}n)\\&amp;=\frac1{n^2}D(X_1+X_2+\cdots+X_n)\\&amp;=\frac1{n^2}[D(X_1)+D(X_2)+\cdots+D(X_n)]\\&amp;=\frac1{n^2}(\underbrace{\sigma^2+\sigma^2+\cdots+\sigma^2}_{n个})\\&amp;=\frac{n\sigma^2}{n^2}\\&amp;=\frac{\sigma^2}n\end{split}$$所以$$\begin{split}E(\hat{\sigma}^2)&amp;=E(A_2-\bar{X}^2)\\&amp;=E(A_2)-E(\bar{X}^2)\\&amp;=(\sigma^2+\mu^2)-(\frac{\sigma^2}n+\mu^2)\\&amp;=\frac{n-1}n\sigma^2\ne\sigma^2\end{split}$$即证非修正的样本方差$\hat{\sigma}^2$是总体方差的$\sigma^2$的有偏估计。 我们令$$S^2=\frac{n}{n-1}\hat{\sigma}^2=\frac1{n-1}(X_i-\bar{X}^2)$$此时有$$E(S^2)=E(\frac{n}{n-1}\hat{\sigma}^2)=\frac{n}{n-1}E(\hat{\sigma}^2)=\sigma^2$$即得修正的样本方差$S^2$是总体方差的$\sigma^2$的无偏估计。 这就是为什么样本方差为什么总是除以n-1而不是n的原因~ 有效性当我们通过无偏性判断出多个无偏估计量$\hat{\theta_1}$、$\hat{\theta_2}$、…时我们需要进一步筛选。 由于无偏估计量的期望总是待估参数，因此方差越小越好 $$\hat{\theta_{opt}}=\mathop{\arg\min}_{i=1,2,\cdots}\hat{\theta_i}$$ 相合性相合性指的是当样本容量$n\to\infty$时$\hat{\theta}(X_1,X_2,\cdots,X_n)$依概率收敛于$\theta$。 相合性是估计量最基本的特性，估计量可以有偏但不能不依概率收敛。]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数估计-点估计]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20180921-469f.html</url>
    <content type="text"><![CDATA[参数估计参数估计作为一个名词短语，实际含义指的是估计参数，意为估计总体分布参数。事实上我们能观测到的数据成为样本，所以参数估计的内容就是通过样本对总体分布的某些指标/参数进行估计。 参数估计分为点估计和区间估计。 其中点估计主要包含矩估计、极大似然估计。 矩估计基于矩的概念对总体分布的均值μ和方差σ进行估计，总体分布的其他参数通过μ和σ进行推断。 我们不需要知道总体分布的分布形式就可以进行矩估计，但是当总体分布已知时矩估计却没有彻底利用已知信息，显然不是一种较好的估计方法。 矩随机变量X的期望是E[X] 令k是自然数，a为任意实数：期望$E[(X-a)^k]$称为随机变量X对实数a的k阶中心矩，期望$E[X^k]$称为随机变量X的k阶原点矩。 矩估计矩估计基于总体的k阶矩等于样本的k阶矩，即假设样本的期望和方差与总体的期望与方差相同。 为什么呢？这里引用一个不加证明的专业说法： 样本的k阶原点矩$\frac{1}{n}\sum_{i=1}^nX_i^k$依概率收敛于总体的k阶原点矩$\mu_k$同理，样本矩的连续函数也将依概率收敛于总体矩的连续函数 什么叫依概率收敛？ $X_n$依概率收敛于$X$，记做$X_n\xrightarrow{P}X$，表示：对$\forall\epsilon&gt;0$，当$n{\rightarrow}{\infty}$时，$P(|X_n-X|\leqslant\epsilon)\rightarrow1$。 定义式如下： 样本 总体 对$\overline{X}$的k阶中心矩 $\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})^k$ $E[(X-E[X])^k]$ k阶原点矩 $\frac{1}{n}\sum_{i=1}^nX_i^k$ $E[X^k]$ 基于一阶样本原点矩推测总体期望： $$EX=E[X]\approx\frac{1}{n}\sum_{i=1}^{n}X_i=\overline{X}$$ 基于二阶样本中心矩推测总体方差： $$DX=D[X]\approx\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})^2$$ 通过上式我们发现，因为在估计期望和方差的过程中并未涉及到总体分布的形式，因此上述估计式实际上是通用的。 极大似然估计使用极大似然估计，总体的分布形式一定要已知，不然没法进行计算。 极大似然估计基于极大似然原理：概率更大的事件在一次观测中更容易出现。 因此，我们求的是使当前样本出现概率最大的那组参数。 我们要想求出这个/这组参数，就必须先构建一个/一组方程。 通常这个/这组方程由某个函数求导/求偏导得出，这个函数就叫做似然函数，常记做 $L(\theta)$。 构造似然函数1.离散型随机变量：$P(X=x)=p(x;\theta)$ 联合概率 $ L(\theta)=\prod_{i=1}^{n}(x_i;\theta)=p(x_1;\theta)p(x_2;\theta){\cdots}p(x_n;\theta) $ 2.连续性随机变量：$P(X=x)=\int_{-\infty}^{x}f(t;\theta)dt$ 联合概率密度 $ L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)=f(x_1;\theta)f(x_2;\theta){\cdots}f(x_n;\theta) $ 求解似然函数取对数 -&gt; 求导/求偏导 -&gt; 求解得到驻点 对于一般属于指数分布族的分布形式（正态分布、泊松分布、伯努利分布…），令导函数为0的点（驻点）一般都是极值点，但是也有奇葩特例。 贝叶斯估计这里只简述贝叶斯估计的思想。 最大似然估计将参数θ看做一个确定变量，而贝叶斯估计将参数θ看做一个服从一定先验分布的随机变量。 我们期望通过p(x|θ)求出后验概率p(θ|x)，同时p(θ|x)在真实的θ处有一个峰，于是乎我们就求得了真正的θ，耶耶耶 搬运个更加直观的图：]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程思想-闭包]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180912-a712.html</url>
    <content type="text"><![CDATA[什么是闭包（Closure）？ 先看看wiki百科是怎么解释的： In programming languages, a closure(also lexical closure or function closure) is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created. A closure (unlike a plain function) allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope. 之我见：我们一般说起将属性和方法进行封装就会想到类。闭包的作用类似于类，只是它的专能性似乎更强。 用栗子来解释闭包为什么会和类相似： 123456def closure_def(x): y = 1 z = 2 def fn(): print((x+y)*z) return fn 对于上面这个简单的栗子，我们发现以下几个特点： y 和 z 是函数 closure_def 作用域下的局部变量 函数 fn 定义在 函数 closure_def 的内部，同时它将直接调用 y 和 z 函数 closure_def返回的是函数 fn 的引用 我们获取一个闭包： 123closure_1 = closure_def(4)closure_1()#输出结果为 10 上述的 closure_1 就是传说中的闭包。因此，闭包本质上还是一个函数,我们通过 closure_1() 调用这个函数后将会输出结果10. 其实到这里我还是没弄清楚闭包与类到底相似在哪里。但是，这里有个很有意思的东西：我们通过 closure_def(4) 调用了闭包定义函数后，该函数内的局部变量（y和z)应该就失效了，但是函数fn实际上却被我们保存了下来（在closure_1中)。然而，这个closure_1需要使用y和z，按照我们前面的逻辑，这两个变量已经失效了，这就产生了一个十分尴尬的问题。 真实情况并非这样，y和z变量明显还存在于内存中，并且它们与这个fn函数绑定在一起了。这就是闭包的特性：将y和z这两个变量和函数fn绑定在一起，就好比类中将属性与方法绑定一样。 在python中，函数同基本数据类型一样同属于一级类对象，函数名就是这个一级类实例的引用。对于闭包，函数需要用到的变量值实际上存在于其中，我们可以这样访问这些变量： .__closure__ 返回一组存有变量值的cell对象 .__closure__[0].cell_centents 将获取相应变量的值 从上面我们发现，闭包就是将一组变量和一个函数进行封装，如下所示： 我们保存的只有蓝色框框部分，但是蓝色框框需要使用红色框框内的变量，闭包就将这两个框框进行封装。橙色框框返回闭包中的那个主函数。 而且，当我们只是对一个函数进行简单的封装（修饰）时，使用闭包将更加优雅。 闭包的一个重要应用就是python的装饰器（Decorator），且等下回之我见。]]></content>
      <categories>
        <category>python编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫-网页解析工具-BeautifulSoup包API速查]]></title>
    <url>%2F%E5%BF%AB%E6%8D%B7%E6%B8%85%E5%8D%95%2F20180911-ef0f.html</url>
    <content type="text"><![CDATA[强大的网页解析工具！ 创建、查看导入BeautifulSoup： 1from bs4 import BeautifulSoup 创建BeautifulSoup对象： 1.传入字符串: html是源代码文本字符串。如 123import requestshtml = requests.get(URL, headers=HEADERS, cookies=COOKIES).textbsobj = BeautifulSoup(html, 'lxml') 2.传入本地文件对象： 1bsobj = BeautifulSoup(open(LOCAL_FILE)) 格式化打印BeautifulSoup对象内容： 1print( bsobj.prettify() ) 四大对象 bs4.element.Tag: 标签，具有 name 属性（标签名）和 attrs属性 （标签的属性字典）：如 bsobj.title 或者下文的 SOME_TAG; bs4.element.NavigableString: 标签内的字符串，通过 SOME_TAG.string 访问； bs4.BeautifulSoup: BeautifulSoup对象，即bsobj，相当于内容是html标签的一个tag，name值为[document]; bs4.element.Comment: 注释掉的内容，容易与NavigableString混淆 遍历文档树 1.标签对象的属性遍历标签的直接子节点，返回列表：SOME_TAG.contents 遍历标签的直接子节点，返回listiterator对象：SOME_TAG.children 遍历标签的所有子孙节点，返回listiterator对象：SOME_TAG.descendants 标签内的字符串只有一个时获取这个字符串：SOME_TAG.string 标签内的字符串有很多个时获取这些字符串：SOME_TAG.strings 标签内的字符串有很多个时获取这些字符串并去掉其中的空白部分：SOME_TAG.striped_strings 父节点(还是一个tag对象)：SOME_TAG.parent 全部父节点，返回generator：SOME_TAG.parents 兄弟节点：SOME_TAG.previous_sibling 或 SOME_TAG.next_sibling 全部兄弟节点：SOME_TAG.previous_siblings 或 SOME_TAG.next_siblings 前后节点：SOME_TAG.previous_element 或 SOME_TAG.next_element 全部前后节点：SOME_TAG.previous_elements 或 SOME_TAG.next_elements 2.标签对象的方法(1) find簇方法find_all( name , attrs , recursive , text , **kwargs )在直接子节点/所有子孙节点中搜索，返回一个tag对象的列表 ● 形参name限制搜索的html标签范围可以是： 直接指定标签，如 name=&#39;title&#39;限制为title标签； 正则表达式，如 name=re.compile(r&#39;^b&#39;) 限制为所有以b开头的标签; 标签的列表，如 name=[&#39;a&#39;,&#39;p&#39;]限制为a标签和p标签； 布尔值True，name=True将返回所有子孙标签； 一个特定规则的函数：形参为SOME_TAG，返回布尔值，如下： 这个函数接受一个SOME_TAG，判断它是否具有class属性并且没有id属性：12def has_class_but_no_id(tag): return tag.has_attr('class') and not tag.has_attr('id') ● 形参attrs对name限定的标签进行属性上的过滤 实际上为一个html属性字典，如 attrs={&#39;class&#39;:&#39;goodslist&#39;, &#39;id&#39;: &#39;gold&#39;} ● 形参recursive为布尔值 True（默认）表示搜索子孙节点，False表示只搜索直接子节点 ● 形参text对标签内的字符串内容进行限制以达到过滤目的 与形参name相似，可以传入字符串、正则表达式、字符串列表、布尔值、特定规则的函数 NavigableString和Comment都在text参数过滤的范围内 ● kwargs中常用关键字参数 非内置的过滤形参会被解析为属性值过滤行为 上面的attrs={&#39;class&#39;:&#39;goodslist&#39;, &#39;id&#39;: &#39;gold&#39;}还可以写做 class_=&#39;goodslist&#39;, id=&#39;gold&#39; 并不是所有html属性都可以这样写的，比如 data-* 属性，因为不符合python变量命名规范 我觉得还是用attrs字典比较靠谱！！！ limit关键字限制搜索结果的数量，当达到指定数量时即停止搜索，避免在文档树十分庞大时拖慢程序速度 find( name , attrs , recursive , text , **kwargs )在直接子节点/所有子孙节点中搜索，只返回找到的第一个结果 find_parents(…) or find_parent(…)通过 .parents 或者 .parent 对父节点进行遍历 find_previous_siblings(…) or find_previous_sibling(…)通过 .previous_siblings 或者 .previous_sibling 对前文兄弟节点进行遍历 find_next_siblings(…) or find_next_sibling(…)通过 .next_siblings 或者 .next_sibling 对后文兄弟节点进行遍历 find_all_previous(…) or find_previous(…)通过 .previous_elements 或者 .previous_element 对前文所有节点进行遍历 find_all_next(…) or find_next(…)通过 .next_elements 或者 .next_element 对所有后文节点进行遍历 (2) select方法遵循CSS标签选择器的语法规则 select方法通过一定的规则构造查询字符串，而find类方法则是通过关键字参数传入信息 通过标签名查找，返回tag列表，如 SOME_TAG.select(&#39;title&#39;) 通过class名查找，返回tag列表，如 SOME_TAG.select(&#39;.goodslist&#39;) 通过id名查找，返回tag列表，如 SOME_TAG.select(&#39;#gold&#39;) 上述基本规则可以进行组合： 满足标签从属关系的查找(使用&gt;)： 1SOME_TAG.select('head &gt; title') 标签、类名、id名联合查找(1)： 1SOME_TAG.select('table .goodslist #gold') 标签、类名、id名联合查找(2)： 1SOME_TAG.select('table[class="goodslist",id="gold"]') 标签从属、类名、id名联合查找： 1SOME_TAG.select('div[class="containor"] &gt; table[id="gold"]')]]></content>
      <categories>
        <category>快捷清单</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性代数-逆矩阵的存在条件]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180815-dbe6.html</url>
    <content type="text"><![CDATA[我们现在要求解一个线性方程组： $$ A{\cdot}x=b $$ 其中$A{\in}R^{m{\times}n}$是一个已知矩阵，$x{\in}R^{n{\times}1}$是一个n维未知列向量，$b{\in}R^{m{\times}1}$是一个m维已知列向量。 按照矩阵×向量的定义，A中的每一行和b中的每一个对应元素形成约束，即b中对应元素都是由A中对应行向量和x运算（內积）而来。 上式的本质还是 $ A_{i,:}{\cdot}x=b_i $ 定义单位矩阵$I_n{\in}R^{n{\times}n}$使得对任意的$x{\in}R^n$都有 $$ I_n{\cdot}x=x $$ 定义逆矩阵$A^{-1}$: $$ A^{-1}{\cdot}A=I_n $$ 这样就可以得到$A{\cdot}x=b$的解为： $$ x=A^{-1}{\cdot}b $$ 那么重点来了，我们怎么求定义的$A^{-1}$？换句话说，$A^{-1}$什么时候存在？（毕竟如果不存在我们期望的这个$A^{-1}$的话怎么求都是徒劳的）因此我们需要对$A^{-1}$的存在条件进行分析。 $A^{-1}$的存在条件分析对于方程组$A{\cdot}x=b$，如果$A^{-1}$存在，那么$x$具有唯一解且这个解为$x=A^{-1}{\cdot}b$。 但是我们的b向量是在$R^m$空间任意取的，对于某些b向量实际实际上可能不存在解，而对另一些b向量则可能存在无穷多的解。那么是否存在一些$b$向量使得x存在不止一个但是又是有限个解呢？答案是不存在的，因为假设向量$x_1$和向量$x_2$是方程组$A{\cdot}x=b$的解，那么向量${\alpha}{\cdot}x_1+(1-{\alpha}){\cdot}x_2$（$\alpha$是任意实数）一定是方程组$A{\cdot}x=b$的解！ 与点的运动关联从空间路径的角度考虑方程组的解的可能个数： $A{\in}R^{m{\times}n}$，即A是一个m行n列的矩阵，A的列向量都是$R^m$空间里的向量，向量b也是$R^m$空间里的一个向量。 我们可以这样想，现在有一个$R^m$空间，它应该有m个坐标轴（直角坐标系），A的每一个列向量在这个坐标系里面实际上都指明了一个方向。 下图是一个例子： $$\begin{pmatrix}A_{1,1} &amp; … &amp; A_{1,n} \\ … &amp; … &amp; … \\ A_{m,1} &amp; … &amp; A_{m,n} \\ \end{pmatrix}{\cdot}\begin{pmatrix}x_1 \\ … \\ x_n \\ \end{pmatrix}=\begin{pmatrix}b_1 \\ … \\ b_m \\ \end{pmatrix}$$ 注意 $$\begin{pmatrix}b_1 \\ … \\ b_m \\ \end{pmatrix}=\begin{pmatrix} A_{1,1}{\cdot}x_1+…+A_{1,n}{\cdot}x_n \\ … \\ A_{m,1}{\cdot}x_1+…+A_{m,n}{\cdot}x_n \\\end{pmatrix}=x_1{\cdot}\begin{pmatrix}A_{1,1} \\ … \\ A_{m,1} \\ \end{pmatrix}+…+x_n{\cdot}\begin{pmatrix}A_{1,n} \\ … \\ A_{m,n} \\ \end{pmatrix}$$ 其中A的某个行向量$A_{i,:}$将控制向量b的某个分量$b_i$，而A的列向量$A_{:,j}$的对应分量将分别影响向量b的分量$b_j$（这里的M影响N意味着M是N的一部分，M决定N表示在其他因素不变的情况下N完全可以由M推导出来！）。有趣的是，A的列向量在影响向量b的分量时都是乘以了某一个常数（x对应的分量），这可以看做是对A的列向量在其本来的方向上进行缩放。 根据上面这种解释，A的所有列向量对b的影响的和（与x的內积）实际上就决定了向量b，即等效完成了A的行向量对向量b分量的控制过程。 上面提到，我们在$R_m$空间里建立了直角坐标系，A的所有列向量和向量b也位于这个空间中。从空间轨迹角度我们可以这样思考： O是$R_m$空间直角坐标系的原点，它首先沿着$A_{:,1}$列向量所指方向运动，运动距离是$x_1{\cdot}|A_{:,1}|$，然后沿着$A_{:,2}$列向量所指方向运动……以此类推，最终到达了向量b表示的B点。 换个角度看，上述过程实际上就是在对A的列向量进行线性组合，使之结果等价于b。 关于A的形式讨论在这里我们发现可以对A进行一些特殊处理，我们让A的列向量仅仅表示方向而不参与点的运动距离计算，距离计算将仅仅依赖于x的分量值，因此我们对A的列向量单位化（使其模为1），当然这不是必须的。 我们对A的一些可能情况进行讨论：假设我们将A的n个列向量沿$R_m$空间的坐标轴进行正交分解 当n&lt;m时（即列向量个数少于列向量的维度数，A的列数少于行数，A是瘦矩阵）：我们只知道一维空间最少只需要一个非零向量就可以通过线性变换覆盖整个空间，二维平面最少需要两个线性无关的向量才能通过线性组合覆盖整个平面，三维空间至少需要三个线性无关的向量才能通过线性组合覆盖整个三维空间。A最多也就只能有n个线性无关的列向量，因此A的列向量的生成子空间根本不可能覆盖整个$R_m$空间，结果就是，对于那些在A列向量生成子空间外的向量b，没有办法通过对A的列向量进行线性组合来等效。结论：A列向量的生成子空间的有效维度少于A列向量的维度时，对于部分向量b，x无解。 当n=m时（即列向量的个数等于列向量的维度数，A的列数等于行数，A是方阵）：从第一点我们可以推断，当且仅当A的列向量线性无关时其列向量的生成子空间才能覆盖A的列向量空间，此时对于任意的$b{\in}R^{m{\times}1}$都能够找到一组x使得原点运动到B点。结论：A列向量的生成子空间的有效维度等于A列向量的维度时，对于任意的$b{\in}R^{m{\times}1}$都有唯一的x解。 当n&gt;m时（即列向量的个数大于列向量的维度数，A的列数等于行数，A是胖矩阵）：注意A的列向量在$R_m$空间中，所以这一组n个列向量中只能找到一组或多组不超过m个列向量的向量组线性无关，剩余列向量一定能被这组里的部分列向量所表示。有了这些列向量的参与，分解方式瞬间变为无数种，如下图(${\alpha}{\in}(0,180^{\circ})$)： 唯一解问题因为我们定义的$A^{-1}$（如果存在）是通过A唯一确定的，基于$A^{-1}$我们求得的$x=A^{-1}{\cdot}b$也应是唯一解。 这就是说，当x有唯一解时我们就可以找到这样一个$A^{-1}$。 从上面的讨论中我们可以知道，当且仅当A的列向量的个数等于列向量的维度且线性无关时，即A是方阵且A的列向量线性无关时，x将有唯一解。 于是乎我们得到结论：只有矩阵A是方阵且列向量线性无关时才存在$A^{-1}$。 奇异矩阵（方阵）：Singular Matrix，列向量相关的方阵 于是乎，只有非奇异矩（方）阵才存在$A^{-1}$。]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法-排序-鸡尾酒排序]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180814-6125.html</url>
    <content type="text"><![CDATA[鸡尾酒排序本质上就是双向冒泡排序。 冒泡排序（Bubble Sort）元素分为已经排序的元素和待排序的元素，每轮排序之增加一个已经排序的元素。 每轮排序都要从数组中待排序的那一端进行相邻元素的逐个比较，比较的结果是减少一个待排序元素，因此最多的比较次数将会是 (n-1)+…+1, 时间复杂度为 o(n^2)。 对于大部分有序的数组将会产生大量没有必要的比较，即当一轮比较不产生交换时说明已经全部有序，因此需要设置一个变量标识是否全部有序及时退出循环。 鸡尾酒排序（Cocktail Sort） 经典的冒泡排序实际上是单向的，而鸡尾酒排序是双向的。 但是，在鸡尾酒排序中每一轮只能在待排序元素中确定一个最大值或者最小值，在最坏情况下它并不能减少比较次数，它的时间复杂度和空间复杂度与经典的冒泡算法相同，只是在大部分有序时可能少于经典冒泡排序的比较次数。 在大部分元素无序时两种排序半斤八两。]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网络基础-TCP数据包结构及TCP建立连接时的三次握手]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20180814-5f47.html</url>
    <content type="text"><![CDATA[三次握手，礼尚往来！ 什么是Socket？Socket意为“插座”，可以说这是一个十分形象的比喻了，更专业的称呼叫“套接字”。网络上的两个程序（一般位于两台机器上）需要进行通讯，就必须在它们之间建立一个双向通信通道：一端连接服务器Server，一端连接者客户端Client。但是与一台Server（或者Client）进行通信的网络程序可能不止一个，它们怎么区分开呢？答案是通过端口（port）进行区分：不同的端口事实上会绑定不同的服务，与该主机的通讯实际上是通过某个特定的端口与该主机进行数据交流。端口就像主机身上的插孔，供外界取用信息。ip:port的格式就称之为一个套接字。显然，连接两个网络程序的这个双向通信通道需要有两个套接字：一个连接Client，一个连接Server。 TCP数据包结构 源始端口：16位，发送数据包的端口，取值 0 ~ 65535 目的端口：16位，接受数据包的端口，取值 0 ~ 65535 数据序号（Seq）：32位，数据包发送端指定的序号（可能是随机产生的） 确认序号（Ack）：32位，数据包发送端指定的序号 如果是第一次提出连接请求，例如Client请求连接Server，Seq可以随机产生，由于标志位A为0所以Ack值无效 如上，如果是Server回应Client的连接请求，当前数据包的Seq可以随机产生，Ack值必须是上述Seq值加1 UAPRSF：六个标志位，共6位 U：URG，urgent，为1时表示 紧急指针有效 A：ACK，acknowledge，为1时表示 确认序号（Ack）有效 P：PSH，为1时表示 接受端在接收到数据包后应该优先处理交付应用端 R：RST，reset，为1时表示 TCP连接出现严重问题需要释放当前连接创建新的连接 S：SYN，synchronous，为1时表示 当前数据包是一个与建立连接有关的数据包（请求或接受） F：FIN，finish，为1时表示 当前数据包已是发送端的最后一个数据包，连接需要关闭 三次握手 客户端调用 socket() 函数创建套接字，客户端连接处于 CLOSED 状态。 服务器调用 listen() 函数使服务器端的套接字进入 LISTEN 状态：监听客户端的请求。 客户端调用 connect() 函数构建数据包：请求连接服务器。 这个数据包的 SYN=1，ACK=0：SYN为1表示当前数据包为一个有关建立连接的数据包，ACK为0表示该数据包的作用是客户端请求连接服务器。 这个数据包的 Seq 由客户端产生，用于标识当前数据包的序号。 发送 请求连接 数据包后客户端进入 SYN-SEND 状态：连接请求已发送。 服务器收到 SYN=1, ACK=0 的数据包，判断这是客户端发来的连接请求。 服务器构建表示 接受连接 的数据包： 这个数据包的 SYN=1，ACK=1：当前数据包是一个关于建立连接的数据包，ACK有效意为着这是一个回应性的数据包，即接受客户端的连接请求。 这个数据包的 Seq 由服务器生成。 这个数据包的ACK=1意味着确认序号有效，这个确认序号应该为 收到的客户端的请求性的数据包的数据序号加1。 发送 接受连接 的数据包之后服务器进入 SYN-RECV 状态：连接已接受。 客户端收到 SYN=1，ACK=1 的数据包，判断这是某个服务器发来的接受自己连接的数据包。 客户端检查这个数据包的确认序号是否是自己请求连接数据包的数据序号加1，如果是，则客户端构建 确认 数据包：确认收到服务器接受了连接这一条消息： 这个数据包的 ACK=1：客户端需要最后向服务器发送确认消息因此需要设置ACK=1. 这个数据包的 确认序号 值为服务器发来的 接受连接 数据包的 数据序号。 客户端进入 ESTABLISED 状态：通道客户端已就绪。 服务器收到 ACK=1 的数据包，判断这是客户端发来的最后的确认包。 服务器检查这个数据包的 确认序号，检查是否为自己发送的 接受请求 数据包的 数据序号 加1，如果是则连接建立，服务器进入 ESTABLISED 状态：通道服务器端已就绪。 至此，通道两端都已就绪，下面就可以收发数据了。 从上图我们可以发现，整个连接建立的过程一共需要发送三个数据包，即三次握手。 其中两个由客户端发送，分别表示“请求连接”和“通告服务器自己已知晓服务器接受了请求”；一个由服务器发送，表示“接受连接”。 三个数据包的区分通过SYN和ACK的组合来判定。 服务器只会收到两个数据包：一个ACK为0表示这是始发包，表示客户端的“请求连接”；一个ACK为1表示这是客户端的确认包。 客户端只会收到一个数据包：与建立连接有关的（SYN=1）、属于回应性（ACK=1）的数据包。]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python读取json文件时书写的语法差异]]></title>
    <url>%2F%E5%B0%8F%E9%BB%84%E9%B8%AD%E8%B0%83%E8%AF%95%E6%B3%95%2F20180809-575c.html</url>
    <content type="text"><![CDATA[使用json文件作为配置文件时如果涉及到列表（list）类型，最后一个元素后面千万不能加逗号。 对于较多参数的程序我们可以通过命令行进行参数传入，也可以定义额外的配置文件。 这里我使用json格式的文件作为额外的配置文件。 json.decoder.JSONDecodeError使用json文件作为配置文件时如果涉及到列表（list）类型，最后一个元素后面千万不能加逗号。 即 [&quot;a&quot;] 是正确的，[&quot;a&quot;, ] 是不正确的。这个规则适用于所有json文件。 加逗号将报错如下： Traceback (most recent call last): File "", line 1, in File "......\lib\json\__init__.py", line 268, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File "......\lib\json\__init__.py", line 319, in loads return _default_decoder.decode(s) File "......\lib\json\decoder.py", line 339, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File "......\lib\json\decoder.py", line 357, in raw_decode raise JSONDecodeError("Expecting value", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 16 column 9 (char 491) 其中 raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None 应该是指，json解析函数认为逗号后面应该还有元素，因此尝试去解析后面的元素，结果却什么也没得到，抛出 json.decoder.JSONDecodeError 错误，告诉你这里本该有值的（”Expecting value: …”）。模块作者有可能受到了其他语言习惯的影响？]]></content>
      <categories>
        <category>小黄鸭调试法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[warnings内建模块]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180730-5f87.html</url>
    <content type="text"><![CDATA[不同于异常，不想影响到程序的正常执行，同时又想让使用者知道点什么 使用警告：warnings.warn(...) 忽略RuntimeWarning警告：warnings.filterwarnings(action=&#39;ignore&#39;, category=RuntimeWarning)]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之飘逸的format格式化字符串控制]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180730-ee75.html</url>
    <content type="text"><![CDATA[标志性符号 {:} 传参 通过{}内的索引取值 &#39;{}-{}&#39;.format(111,222) &lt;=&gt; ‘111-222’ &#39;{0}-{1}&#39;.format(111,222) &lt;=&gt; ‘111-222’ &#39;{1}-{0}&#39;.format(111,222) &lt;=&gt; ‘222-111’ &#39;{0}-{1}-{0}&#39;.format(111,222) &lt;=&gt; ‘111-222-111’ 通过{}内的关键字取值 &#39;{one}-{two}&#39;.format(one=111,two=222) &lt;=&gt; ‘111-222’ &#39;{two}-{one}&#39;.format(one=111,two=222) &lt;=&gt; ‘222-111’ 传入对象 &#39;{specific_obj.attr1}-{specific_obj.attr2}&#39;.format(specific_obj) 传入序列（元组，列表，…） L = [111,222] P = [333,444] &#39;{0[0]}-{0[1]}&#39;.format(L) &lt;=&gt; ‘111-222’ &#39;{0[1]}-{0[0]}&#39;.format(L) &lt;=&gt; ‘222-111’ &#39;{0[0]}-{1[1]}&#39;.format(L,P) &lt;=&gt; ‘111-444’ 格式控制 {:}（借助于：） 填充 &#39;{:w&gt;8}&#39;.format(1314) &lt;=&gt; ‘wwww1314’ : 格式控制必须字符w 是填充符，必须为单字符，可以不指定，默认为空格，例如 {:&gt;8}&gt; 左对齐，相应地’&lt;’为右对齐, ‘^’为居中对齐8 域宽 浮点数精度 &#39;{:.2f}&#39;.format(1.2345) &lt;=&gt; ‘1.23’ .2 代表保留两位小数，而不是两位有效数字 &#39;{:.{}f}&#39;.format(1.2345, 3) &lt;=&gt; ‘1.234’ {}可以嵌套使用！！ &#39;{k:.2f}.format(1.2345) &lt;=&gt; ‘1.23’ 配合关键字使用 进制转换 {:b}: 二进制 {:d}: 十进制 {:o}: 八进制 {:x}: 十六进制 数字分隔符 {:,}.format(1234567) &lt;=&gt; ‘1,234,567’]]></content>
      <categories>
        <category>python编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[时间内建模块]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180728-42ac.html</url>
    <content type="text"><![CDATA[包括time, datetime, calendar, business_calendar, pytz, dateutil等等模块。 time模块ticks = time.time() 返回一个表示当前时刻的浮点数，只能表示1970年到2038年。 struct_time = time.localtime(time.time()) 返回本地时间 返回时间元组对象 struct_time，并不能自动转化为字符串 返回例如time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0) tm_wday 表示一周的第几天 tm_yday 表示一年的第几天 tm_isdst 表示是否是夏令时 time.asctime(struct_time) 返回给定时间的字符串表示，只能传入时间元组而不是ticks time.strftime(FORMAT, STRUCT_TIME) 可以定制时间字符串格式 时间格式化符号 %y (两位数年份00-99)，%Y (四位数年份0000-9999) %m (月份01-12)，%d (月中的第几天01-31) %H (24小时制00-23)，%I (12小时制01-12) %M (分钟数00-59)，%S (秒数00-59) %w (周几0-6，0代表周日)，%a (本地简化星期名称，如Sat)，%A (本地完整星期名称，如Saturday) %U (一年中的第几周00-53，周日始)，%W (一年中的第几周00-53，周一始) %a (本地简化月份名称，如Jul)，%A (本地完整月份名称，如July) %j (一年内的第几天001-366) %p (值为AM或者PM) 常用格式： %Y-%m-%d %H:%M:%S time.strptime(STRING[, FORMAT]) 将字符串转换为struct_time对象。如果STRING是标准的时间字符串可以不用指定FORMAT，否则还是指明的好。 time.mktime(struct_time_object) 将struct_time对象转化为一个浮点数（时间戳） time.clock() 返回当前CPU时间，用于计算程序耗时。 datetime模块是time模块的高级封装，同时解除了time模块对年份（1970 - 2038）的限制。 datetime模块包含了5个类，实际常用的是datetime类。 ddnow = datetime.datetime.now() 获取当前时间，是一个datetime.datetime对象 datetime.datetime(2018, 7, 28, 9, 49, 22, 994058) 这个可比time模块舒服多了… API Function ddnow.year 获取相关字段信息，类似的还有.month, .day, .hour, .minute, .second, .tzinfo, .microsecond等等 ddnow.replace(...) 修改上述字段的值 ddnow.strftime(FORMAT) 格式化时间字符串，与time.strftime()作用相同 datetime.datetime.strptime(STRING, FORMAT) 相对的datetime模块也能从时间字符串重构datetime对象 ddnow.date()ddnow.time() 和datetime模块其他两个类（date，time）交互 ddnow.timetuple() 返回 time.struct_time 对象 calendar模块与日历相关 business_calendar模块工作日相关 pytz模块时区相关 dateutil模块dateutil.parser 非常强大的时间字符串解析函数，返回datetime.datetime对象 dateutil.rrule 根据定义的规则生成datetime.datetime对象 pandas.to_datetime略]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logging内建模块（入门）]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180724-1605.html</url>
    <content type="text"><![CDATA[Python版本：Python 3.7 logging基础教程日志指的是一些软件运行时我们记录下的事件的发生轨迹。软件的开发者在他们的代码里加入日志调用来查看哪些事件跟随着软件的运行发生了。一个事件通过一条描述性的消息进行记录，这条消息可以选择性地包括各种各样的数据（例如这些数据可能描述了某个事件在不同时刻的差异等）。事件也是开发者归咎错误等级的重要手段，这也叫做严重等级（日志等级）。 什么时候使用日志？日志模块提供了一些简单便利的函数以实现简单的日志功能，他们是debug(), info(), warning(), error(),和critical()。什么时候使用什么等级的日志函数见下表： 你想要执行的任务 该任务最适合的工具 命令行脚本或程序简单地将输出显示到控制台 报告在程序正常运行期间发生的事件，例如状态监控、断层调查 报告在程序正常运行期间发生的事件，例如状态监控、断层调查 logging.info()（或者以诊断为目的获取更加详细的输入的logging.debug() 对一个特定的程序运行时产生的事件发出警告 使用库函数warnings.warn()，警告是可以避免的，应该修改程序代码以消除这样的警告logging.warning()，当我们对这样的情况不能作出任何修改而且警告必须被保存下来时就需要使用这个函数 将特定程序运行时产生的事件报告为错误 抛出异常即可 不抛出异常而报告一个错误（例如长期运行的服务器需要错误处理器） logging.error() logging.exception()logging.critical() 默认的等级是logging.WARNING，这意味着只有当前等级以及大于当前等级的事件才会被跟踪，除非额外配置日志处理器的行为。 被跟踪的事件可以通过不同的方式进行处理，处理跟踪事件最简单的方式是把它们打印在控制台上。而另外一种常见的方式是把它们写入磁盘文件。 一个简单的例子一个非常简单地例子如下： 123import logginglogging.warning('Watch out!') # 将会打印信息到控制台logging.info('I told you so') # 将不会打印任何东西 如果你将这些语句写入脚本并且运行它，你将看到： WARNING:root:Watch out! 当打印到控制台时，INFO信息并没有出现，原因是默认的严重等级是WARNING。我们看到的打印消息实际上包含了严重等级以及调用日志函数时提供的描述信息，比如”Watch out!”。别担心打印消息中的’root’部分，我们接下来将会对其进行解释。实际的输出结果可以灵活地定制格式以满足各自的需要；格式的定制也将在后面部分进行介绍。 将日志写入到文件一种非常典型的情景就是在文件中记录日志事件，我们接下来详细说说这种情况。确保接下来的语句在一个新打开的Python解释器中运行，千万不要使用上一部分的环境，否则你可能得到与我们不一样的结果： 12345import logginglogging.basicConfig(filename='example.log', level=logging.DEBUG)logging.debug('This message should go to the log file')logging.info('So should this')logging.warning('And this, too') 现在如果我们打开文件看看里面有什么，我们会发现以下日志信息： DEBUG:root:This message should go to the log file INFO:root:So should this WARNING:root:And this, too 这个例子也展示了你应该如何设置日志的等级，这个等级将作为是否跟踪事件的阀门。在这个例子中因为我们将阀门设置为了DEBUG，所有的信息都将被打印出来。 如果你想在命令行选项参数里设置日志等级，请使用如下格式： --log=INFO 你也可以通过变量 loglevel 获取--log参数传递的值： 1getattr(logging, loglevel.upper()) 获取你想要传递给basicConfig()的 level 参数的值，检查以下用户从命令行传入的参数是否合法： 123456# 假设loglevel一定是从命令行参数获取的字符串，后台自动将其转换为大写的形式，# 这样用户就可以在命令行中指定 --log=DEBUG 或者 --log=debugnumeric_level = getattr(logging, loglevel.upper(), None)if not isinstance(numeric_level, int): raise ValueError('Invalid log level: %s' % loglevel)logging.basicConfig(level=numeric_level, ...) 调用basicConfig()应该在调用debug(), info()等方法之前。这种方式被有意设计成一次性的配置，只有第一次配置发挥作用，接下来再进行子参数的配置将起不到任何作用。 如果你多次运行上面的脚本，来自连续运行脚本的日志信息都将被添加到文件 example.log 中。如果你想要每个程序运行时都使用全新的日志文件而不是记住之前的日志信息，你可以指定 fielmode 参数，我们修改上面对basicConfig()的调用： 12345logging.basicConfig( filename='example.log', filename='w', level=logging.DEBUG) 输出将和以前一样，但是日志文件将不再是末尾添加模式，先前运行的日志信息已经丢失。 多个模块的日志信息如果你的程序是由多个模块组成的，这里有个例子展示你该如何在多个模块中组织的你的日志记录： 123456789101112# myapp.pyimport loggingimport mylibdef main(): logging.basicConfig(filename='myapp.log', level=logging.INFO) logging.info('Started') mylib.do_something() logging.info('Finished') if __name__ == '__main__': main() 1234# mylib.pydef do_something(): logging.info('Doing something') 如果你运行 myapp.py，你将在 myapp.log 中看到如下内容： INFO:root:Started INFO:root:Doing something INFO:root:Finished 见心之所想实在是一件鼓舞人心的事情。你可以使用myapp.py中的方法将这种模式推广到更复杂的多模块程序中去。需要注意的是，仅仅通过这种简单的方式你并不能根据日志文件的内容来判断日志信息来自哪一个模块，除非借助于消息描述本身。如果你想要跟踪你的日志消息的源位置，你需要放下这个基础教程去看看高级日志教程 记录变化的数据为事件的描述信息使用格式化字符串并将变量作为参数传递给日志函数： 12import logginglogging.warning('%s before you %s', 'look', 'leap!') 将会显示： WARNING:root:Look before you leap! 正如你所看到的，将变量中的信息合并到日志描述信息中可以使用老式的%风格的格式化字符串。事实上，格式化字符串是向后兼容的，也就是说，你也可以使用str.format()或者string,Template。这些新的格式化字符串也被支持，这里不做详细探讨。 改变消息显示的格式为了改变用于显示的消息的格式，你需要指定你想用的格式： 12345678import logginglogging.basicConfig( format='%(levelname)s:%(message)s', level=logging.DEBUG)logging.debug('This message should appear on the console')logging.info('So should this')logging.warning('And this, too') 将打印： DEBUG:This message should appear on the console INFO:So should this WARNING:And this, too 注意在前面出现的’root’在这里又出现了。对于控制格式的字符串的设置可以参见文档。如果只是简单的使用日志，你只需要指定 levelname（日志等级），message（事件描述，可以包含变量）以及当事件发生时需要在哪里显示消息，这部分将在接下来详细介绍。 在消息中显示时间如果需要在日志消息中显示时间，你需要在格式化字符串中放置 ‘%(asctime)s’： 123import logginglogging.basicConfig(format='%(asctime)s %(message)s')logging.warning('is when this event was logged.') 将打印如下消息： 2010-12-12 11:41:42,612 is when this event was logged. 上面这种默认时间显示格式被称之为 ISO8601。如果你想要更加自由的控制时间格式，请提供一个 datefmt 参数给 basicConfig ： 123456import logginglogging.basicConfig( format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')logging.warning('is when this event was logged.') 将打印如下消息： 12/12/2010 11:46:36 AM is when this event was logged. datefmt 的格式控制原理与 time.strftime() 相同。 小结上面的作为基础教程，应该能够使你开始将日志功能添加到自己的代码中。事实上，logging包还提供了许多更为丰富的内容，如果你想更加熟练的应用日志功能，你需要花一点时间阅读高级教程(英)。 如果你对日志功能的需求十分简单，你可以阅读上面的例子，将简单的日志记录融入到自己的代码中去。如果遇到任何问题或者对某些东西不太理解欢迎光临论坛。 如果你看过了高级教程，也可以读读 Logging Cookbook。 注注注：本文手撸自官方文档 Author: Barwe(YinChen)Email: chenyinbarwe@qq.com]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[getpass内建模块]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180723-77b1.html</url>
    <content type="text"><![CDATA[getpass模块用于命令行程序获取密码，输入密码时不会回显在屏幕上 该模块主要包括两个可用的函数： getpass.getpass() getpass.getuser() getpass.getpass(提示字符串)getpass.getuser()该函数实际上会首先检索环境变量LOGNAME（Linux系统可通过echo $LOGNAME查看该值）及其他环境变量的值]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库-mysql-删除(DROP+TRUNCATE+DELETE)]]></title>
    <url>%2FIT%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F20180718-3284.html</url>
    <content type="text"><![CDATA[这里的删除分为： 删除表中的部分数据 删除表的所有数据：下次添加数据前不需要新建表 同时删除表的数据和表的定义：下次添加数据之前必须新建表 主要关键词有：TRUNCATE DROP DELETE 下面按删除类型介绍操作语句： 删除一行或部分行1.DELETE FROM 表 WHERE 字段 = 值：选择性地删除一行 有趣的是，MySQL对DELETE语句添加了标准SQL语句没有的一些功能： 2.DELETE FROM 表 WHERE 字段 = 值 LIMIT 数量：当WHERE过滤结果有多个时删除前面的几个，即删除过滤结果中的部分数据 如果上述语句2对删除部分结果的排序标准不明确可以使用ORDER BY指定排序字段，再使用LIMIT限制删除部分数据： 3.DELETE FROM 表 WHERE 字段 = 值 ORDER BY 排序字段 LIMIT 数量：由小到大 4.DELETE FROM 表 WHERE 字段 = 值 ORDER BY 排序字段 DESC LIMIT 数量：由大到小 删除一列UPDATE 删除所有行但保留表的定义1.DELETE FROM 表：返回被删除的记录数，自增字段起始值恢复为1 2.TRUNCATE TABLE 表：不返回被删除的记录数，自增字段起始值恢复为1 3.DELETE FROM 表 WHERE true：返回被删除的记录数，自增字段起始值不变 注意： 语句3由于加了WHERE将进行逐行扫描（尽管不进行WHERE判断），而语句1直接删除所有数据就好了，所以语句3的执行成本高于语句1 语句2相对于语句1虽然不能返回被删除的记录数，但是非常快 删除指定表（包括内容和定义）DROP TABLE 表 删除指定数据库的所有表数据（保留结构）首先要能遍历到所有表：SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA=’数据库名’ 然后我们使用内置连接语句CONCAT生成多个命令： SELECT CONCAT(‘TRUNCATE TABLE ‘, TABLE_NAME, ‘;’) FROM information_schema.TABLES WHERE TABLE_SCHEMA=’数据库名’：注意TRUNCATE TABLE后面留一个空格 其他DROP、DELETE、TRUNCATE的适用情况]]></content>
      <categories>
        <category>IT开发笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[玻尔兹曼机和受限玻尔兹曼机]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F20180717-601f.html</url>
    <content type="text"><![CDATA[原文：A Tutorial on Energy-Based Learning 基于能量的模型(EBM, Energy Based Model)：在以计算概率为目标时我们需要将直接计算结果进行归一化和非负处理，完成这两个目标的方式有很多种，我们需要选择计算较为简单的，比如自然指数e。例如，对于离散型随机变量，x经过f变换后再经过自然指数的变换，最后归一化，结果可以被视为一组概率值： 其中 借用原博主的一句话，“哇，是不是很像softmax” 知乎回答 为什么称之为能量？按照原博主的介绍，“能量”的概念应该来自于统计物理学中相似的分布形式： 玻尔兹曼机(Boltzmann Machine)：由观察节点和隐藏节点组成，但是任意两个节点间都有无向边连接，这样的结构在应用中没有实际效果。 受限玻尔兹曼机(RBM, Restricted Boltzmann Machine)：顾名思义，在玻尔兹曼机的基础上去掉观察节点间和隐藏节点间的无向边，观察层和隐藏层由无向边进行连接。RBM定义了神经网络连续两层的形式，可以说是神经网络之砖瓦了。 受限玻尔兹曼机实际上是一个二分图：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[统计模型估计-噪音对比估计NCE]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20180716-beb2.html</url>
    <content type="text"><![CDATA[噪声对比估计(NCE, Noise-Contrastive Estimation) 指数有着十分好的性质，非负、求导快…但同时，指数概率分布的配分函数有时会很难计算： 配分函数 配分函数难以计算主要原因有两个： 由于Z需要对所有样本求加和，当样本数庞大时计算量将超过预期 对于某些G(x)，Z实际上是不可计算的 如果我们要求p(x)就必须计算配分函数，但是当配分函数实际不可计算时P(x)也就不可求了。此时我们必须曲线救国，然后中间是看不太懂的巴拉巴拉…。总之，NCE的思想就是将概率生成问题转化为二分类问题：真实样本和从简单分布随机采样的错误样本进行对比，试图找到真实样本与错误样本的差异。 原文证明了为什么γ能够替换配分函数 当负样本比正样本多的多时可以考虑NCE]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python从requirements.txt安装包]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180423-48b5.html</url>
    <content type="text"><![CDATA[requirements.txt记录的是当前程序的依赖包和版本号用来在另一台电脑上重构运行环境。 生成requirements.txt文件查看包列表： pip freeze 保存只需要借助定向符即可： pip freeze &gt; requirements.txt 从requirements.txt文件进行安装pip install -r requirements.txt pip freeze保存的是当前环境下所有的包实际上我们可能只需要与项目有关的包这里建议使用另一个工具： pipreqs安装它： pip install pipreqs 使用它： pipreqs 项目路径 [选项] 默认保存为项目根路径下的requirements.txt]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10下在右键菜单中添加获取管理员权限入口]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20180420-f8ae.html</url>
    <content type="text"><![CDATA[右键菜单改造。 添加“管理员权限”效果如下： 新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 ;取得文件修改权限 [HKEY_CLASSES_ROOT\*\shell\runas] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\*\shell\runas\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F /t&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F /t&quot; 双击运行 添加“恢复原始权限” 新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 ;恢复原始权限 [HKEY_CLASSES_ROOT\*\shell\runas-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; ; &amp;&amp; takeown /f \&quot;%1\&quot; [HKEY_CLASSES_ROOT\*\shell\runas-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; 双击运行 取消上面两个选项取消之后就像从未发生过一样~~~新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 [-HKEY_CLASSES_ROOT\*\shell\runas] [-HKEY_CLASSES_ROOT\exefile\shell\runas2] [-HKEY_CLASSES_ROOT\Directory\shell\runas] [-HKEY_CLASSES_ROOT\*\shell\runas-] [-HKEY_CLASSES_ROOT\exefile\shell\runas2-] [-HKEY_CLASSES_ROOT\Directory\shell\runas-] 双击运行]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下给目录右键菜单添加cmd入口]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20180420-38fd.html</url>
    <content type="text"><![CDATA[你是否厌倦了在cmd中切换工作路径？现在教你设置在图形界面中通过右键菜单直接以某个目录作为工作路径即，在目录的右键菜单中添加“进入cmd”选项核心方法，还是修改注册表~~~ Ⅰ.打开注册表，路径栏搜索HKEY_CLASSES_ROOT\Folder\shell Ⅱ.右键shell新建项，名称为cmdPrompt实际上这个名字可以随便取，这就意味着我们可以添加多个项新建完成如下： Ⅲ.双击cmdPrompt下右侧的名称设置值，该字符串用于显示在右键菜单上，所以可以取个响亮的名字，比如“在cmd中打开” Ⅳ.右键cmdPrompt新建项，名称为command Ⅴ.双击command下右侧的名称设置值，值为cmd.exe /k cd %1 /k 表示执行后面的命令并保留终端 Ⅵ.保存退出即可 最终效果如下：]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下隐藏资源管理器中六大用户文件夹]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20180420-6264.html</url>
    <content type="text"><![CDATA[打开资源管理器或者我的电脑你会看到下面这六个用户文件夹如果你没有把文件存在这个目录下的打算，他们会显得特别碍眼这里我们可以隐藏这六个目录隐藏通过修改注册表实现：我们即可已直接修改注册表，也可以通过.reg脚本修改注册表因为涉及到的值较多，我们使用后一种方式修改注册表 隐藏在你喜欢的目录下新建reg文件，文件名随便取，但是必须以.reg结尾右键编辑，千万不能直接双击！！输入以下内容： Windows Registry Editor Version 5.00 [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{d3162b92-9365-467a-956b-92703aca08af}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{B4BFCC3A-DB2C-424C-B029-7FE99A87C641}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3dfdf296-dbec-4fb4-81d1-6a3438bcf4de}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{088e3905-0323-4b02-9826-5d99428e115f}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{24ad3ad4-a569-4530-98e1-ab02f9417aa8}] 保存后双击运行即可，弹出的一切提示均点击是即可 恢复恢复只要报上面语句的负号去掉即可新建另一个.reg文件，输入： Windows Registry Editor Version 5.00 [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{d3162b92-9365-467a-956b-92703aca08af}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{B4BFCC3A-DB2C-424C-B029-7FE99A87C641}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3dfdf296-dbec-4fb4-81d1-6a3438bcf4de}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{088e3905-0323-4b02-9826-5d99428e115f}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{24ad3ad4-a569-4530-98e1-ab02f9417aa8}] 保存后双击即可恢复那六个文件夹]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下使用doskey在cmd中建立类似于linux中alias的宏]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20180420-1a3f.html</url>
    <content type="text"><![CDATA[在linux系统中可以通过修改.bashrc文件十分简便的设置alias宏命令（macro)在win10下cmd中实现相同的功能要复杂一点 新建宏文件首先你需要一个文件存放宏，假设我们在C盘根目录下建立了文件cmd-alias.bat 修改注册表然后你需要在启动cnd时自动加载文件中的宏，那么问题来了，怎么自动加载宏？修改注册表：1、摁下win+R输入regedit回车2、在菜单栏下的路径栏输入HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Command Processor回车3、右侧新建字符串值，数值名称填AutoRun，数值数据填C:\cmd-alias.bat（第一步那个文件的路径）4、关闭即可。。 修改宏文件右键第一步的文件，选择编辑（默认用记事本打开） ◎ 在windows系统下不是alias命令，而是doskey命令 doskey程序路径为C:\Windows\System32\doskey.exe ◎ 与.bashrc文件一样，一行一个doskey，语句以doskey开头 ◎ 以@doskey开头的宏在cmd打开时不会显示在屏幕上 单个命令的宏@doskey ls=dir：列出当前目录下的子文件/子目录信息 win10默认为dir，linux默认为ls，这里我们设置ls起到与dir相同的作用 @doskey ls=dir $*：$*表示后面可能还有其他参数，参考ls 多个命令的宏多个命令的宏用$t隔开，命令间不用加空格@doskey hexocgd=hexo clean$thexo g$thexo d：顺序执行hexo clean, hexo g, hexo d doskeydoskey /MACROS 可查看所有已经定义的宏命令]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法-动态规划-钢材切割经典问题]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180313-6ba0.html</url>
    <content type="text"><![CDATA[钢条切割问题 什么时候用动态规划?动态规划（Dynamic Programming）问题的显著特征： 最优子结构 （optimal substructure） 如果一个问题的解结构包含其子问题的最优解，称此问题具有最优子结构问题。如上的钢条切割问题，长度为n的钢条显然存在最优解，而且最优解一定可以分为两组，假设为i和n-i，那么长度分别为i和n-i的钢条显然也存在最优解，而且可以对这两种钢条独立求解（对长度分别为i和n-i的钢条求解完全是另外的不同的问题）。当然，这里还存在另外一种情况，长度为n的钢条不一定非要切割才能产生最优解，事实上当pn足够大时最优解发生在不切割时，因此需要取不切割和切割成两组中较大的那个值。事实上所谓的切割成两组可能存在不止一种切法：长度为n的钢条存在n-1种切法。因此实际上的递归式可以写为： $$r_n=\max{(p_n,r_1+r_{n-1},r_2+r_{n-2},…,r_{n-1}+r_1)},\text{共n项}$$ 考虑到重复元素，可进一步精简为：$[\frac{n}2]$表示取整 $$r_n=max{(p_n,r_1+r_{n-1},r_2+r_{n-2},…,r_{[frac{n}2]}+r_{n-[\frac{n}2]})},\text{共$1+\frac{n}2$项}$$ 重叠子问题（overlapping subproblems） 当根据递归式自顶向底进行计算时，如果不运用任何技巧（如备忘录），将会浪费大量的资源重复计算相同的子问题，很明显这些子问题是重叠的。递归本来就需要额外的空间对中间状态进行存储，如果还要浪费大量时间重复计算重叠子问题，大大的不划算。解决重叠子问题大概有两种方法： 备忘录：自顶向下计算时将中间结果存下来，下次遇到时直接查询即可。备忘录解决了重叠子问题的计算。 自底向顶计算：这个就厉害了，连递归栈也省略了。利用递推式从最小的同类问题开始计算，你会发现意外的惊喜。在钢条切割问题中最简单的同类问题不外乎$r_0=0$或者$r_1=r_p$。 java中可以使用 Arrays.stream(ARRAY).max().getAsInt() 获取最大值 动态规划的经典模型线性模型状态的排布呈线性：？？ 【例题】在一个夜黑风高的晚上，有n（n &lt;= 50）个小朋友在桥的这边，现在他们需要过桥，但是由于桥很窄，每次只允许不大于两人通过，他们只有一个手电筒，所以每次过桥的两个人需要把手电筒带回来，i号小朋友过桥的时间为T[i]，两个人过桥的总时间为二者中时间长者。问所有小朋友过桥的总时间最短是多少。 区间模型背包模型]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[三大相关系数 Pearson Kendall Spearman]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180312-2d85.html</url>
    <content type="text"><![CDATA[简单区分一下三大相关系数： Pearson积矩相关系数r度量两变量间的线性相关关系，用r表示，定义为两变量协方差与标准差积的商（公式网上有的是。。）。 高斯采样：因为公式要用到方差和均值，而已方差和均值作为参数的分布显而易见是高斯分布，因此两变量都要求从高斯分布采样 成对采样：因为需要计算协方差矩阵（方阵） 线性相关：各变量的离散值趋势大致呈直线，计算的是两条近似直线间的差异程度 显然计算Pearson距离属于参数统计方法 而且以上条件较强，实际应用时难度较大 Spearman秩相关系数ρ度量两个变量之间关系的强弱 非参数统计方法：因为不需要假设总体的分布 只要两变量间具有单调的函数关系，则两变量完全Spearman相关 通常被认为是排列后的Pearson相关系数：参与Spearman系数计算的实际上是原始数据的等级值： 一般原始数据是能排序且不重复的数据 对于重复数据另外处理 很明显处理的是两列数据 计算公式 下表清晰定义了di2的求法： 参考博客 Kendall等级相关系数τ略 参考]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[参数模型和非参数模型]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180312-62c1.html</url>
    <content type="text"><![CDATA[任何模型的建立都会做出一定的假设 参数模型(parametric models)要先假设总体服从某个分布（不同的分布有自己的分布参数，比如高斯分布的参数是均值和方差），然后通过样本来估计总体分布的参数 参数模型通过估计出的分布进行检验和预测。 非参数模型(nonparametric models)有时候样本不足以支撑我们取估计总体，或者总体本身没有明显的特征。 非参数模型对总体的分布没有要求，也就是说在计算分析过程中不会用到总体的概率分布表达式。 但是数据必须可以进行排序，因此能影响排序的因素（数据中有相等值）对模型估计结果影响巨大。 非参数模型通过数据本身进行检验和预测。]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas模块-加载数据]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180308-ca22.html</url>
    <content type="text"><![CDATA[文本文件读取pd.read_csv(…)详细记录pandas读取表格型数据文件的方法：pd.read_csv() 其实pytohn内置了一个可以处理简单规整csv文件的模块：import csv给 csv.reader() 传入文件型对象即可什么？你不知道什么是文件型对象？ open(fp,&#39;r&#39;) 返回的就是文件型对象 重要参数解析： path: 这个参数不解释，可以是本地路径，也可以是URL sep: 行内字段(列)分隔符，csv文件默认为逗号，还可以为正则表达式 header: 标识列名的行号。默认为0，代表文件第一行是列名，没有列名时需要设为None //当header=None时: names: 字符串组成的列表，指定列名 index_col: 标识行索引 简单行索引：列编号（整数）或者列名（字符串） 层次化索引：列编号/列名zucheng的列表 什么是层次化索引？将多个列作为索引，列之间将产生层次关系 a a1 a2 b b1 b2 b3 ... skiprows: 如果传入整数，表示需要从文件开始出算起需要忽略的行数 如果传入整数列表，表示需要跳过的行号列表（从0开始） skip_footer: 整型，从文件末尾算起需要忽略的行数 nrows: 整型，需要读取的行数（从开始算起） na_value: 用于替换NaN的值 comment: 当行末有注释信息时指定注释信息标志字符 parse_dates: 尝试将数据解析为日期，默认为False 传入True：尝试将所有列都解析为日期（一般是行不通的） 传入列号/列名的列表：仅尝试将指定的（一些）列解析为日期 传入上一行列表的列表：尝试将多个列进行组合后进行解析（当日期不同部分分散在不同的列中时） keep_date_col: 当组合多列解析时是否保留原始列，默认为False dayfirst: 当日期出现歧义时（如7/6/2018究竟是6月还是7月？)指定day的位置，默认为False date_parser: 或者传入一个用于解析日期的函数 converters: 列值预处理，传入一个字典d={列号/列名: 函数, ...}，表示对相应地列的值将执行相应地函数 iterator: 用于逐块读取文件 chunksize: 文件块的大小 verbose: 是否打印日志 encoding: 文本编码格式 squeeze: 数据经过解析后只剩下一列，返回Series而不是DataFrame thousands: 千分位分隔符 逐块读取当文件特别大时我门可能只是想读取文件的一小部分或者逐块迭代 如果我们只是想读文件开始几行，使用关键字参数 nrows。（Q: 那中间几行呢？） 逐块迭代：read_csgv 指定 chunksize 后将返回一个TextParser用于后续迭代Q: 按固定字符数读取？If so, 貌似只适用于行字符数相同的文件? XML文件读取略 二进制文件读取pickle序列化首先得了解python内置的pickle序列化保存pandas对象到磁盘：PD_OBJ.save(FILE_NAME)从磁盘取回pandas对象：PD_OBJ.load(FILE_NAME) HDF5格式 高效读写磁盘上以二进制形式存储的科学数据HDF: Hierarchical Data Format高效分块读写python中有两个接口：PyTables + h5pypandas建立HDFStore类，通过PyTables存储对象 存储HDFStore类类似于字典，创建HDFStore对象后按照字典方式赋值即可。 //&lt;class &apos;pandas.io.pytables.HDFStore&apos;&gt; store = pd.HDFStore(OUTPUT_FILE) store[&apos;obj1&apos;] = dataframe store[&apos;obj1_col&apos;] = dataframe[&apos;a&apos;] 对于IO密集型数据（而非CPU密集型数据），使用HDF5能够大大提升效率 Excel格式panda中相关的类：ExcelFile依赖包：xlrd, openpyxl 创建实例：xls_file = pd.ExcelFile(XLS_FILE) 读取数据：table = xls_file.parse(&#39;Sheet1&#39;) 使用HTML和Web API使用数据库存取MongoDB中的数据]]></content>
      <categories>
        <category>python编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas模块-Series类和DataFrame类]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180307-3057.html</url>
    <content type="text"><![CDATA[pandas是基于numpy构建的高级数据结构和操作工具。 pandas的数据结构Series类似于一维数组的对象，索引即标签导入： from pandas import Series初始化： obj = Series([-1, 3, 1, 4]) obj = Series([-1, 3, 1, 4], index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])pandas可以被当做字典使用~ obj = Series({&#39;a&#39;:-1, &#39;b&#39;:3, &#39;c&#39;:1})：不指定index使用传入字典的键 obj = Series({&#39;a&#39;:-1, &#39;b&#39;:3, &#39;c&#39;:1}, index=[&#39;a&#39;,&#39;b&#39;])：使用index时将按照index去字典中挑选，如果该字典中键值对不存在则设为NaN（缺失值）。 使用 obj.isnull() 和 obj.notnull() 检测是否为缺失值 obj.isnull() 等价于 pd.isnull(obj) , notnull() 类似 基本属性： obj.values 查看值 obj.index 查看索引 obj.name obj.index.name DataFrame表格型数据结构数据以二维块存放，行列操作比较平衡，因为存放结构并不是简单的复合一维数据结构（如原生的嵌套列表，行的操作比列简单）导入：from pandas import DataFrame初始化： 传入字典：键为列的名称，值为等长的列表或者ndarray此时索引默认为自然数0到N-1；或者通过 index 传入索引值。索引值对应的列如果没有值将设为NaN ↓↓↓ 未指定列的顺序，将按照列名称自动排序：传入的列名是name age, 实际存储的顺序是age nameframe = DataFrame({&#39;name&#39;:[&#39;aa&#39;,&#39;cc&#39;,&#39;bb&#39;], &#39;age&#39;:[21, 24, 18]}) ↓↓↓ 或者通过 columns 指定列的顺序：按照columnszhiding的顺序进行存储frame = DataFrame({&#39;name&#39;:[&#39;aa&#39;,&#39;cc&#39;,&#39;bb&#39;], &#39;age&#39;:[21, 24, 18]}, columns=[&#39;name&#39;, &#39;age&#39;]) 传入嵌套字典：外键被解释为列名(columns)，内键被解释为索引(index)，自动填充缺失数据内键将会被重新排序形成有序的索引，可以使用index=显示指定索引的顺序，同时还能起到筛选的作用 实际上可以传入由列表、元组、ndarray、Series、字典复合而成的多种数据结构 基本操作： 获取列：frame[&#39;name&#39;] or frame.name，实际上是一个Series索引获取的列是原数据的视图，修改Series将反馈到原数据，复制请使用Series.copy() 修改列：直接给列赋值标量或者向量 赋标量将自动广播 赋列表或者数组一定要保证长度一致 赋Series会对索引进行精确匹配，空位填上NaN 添加列：用frame[&quot;new_column_name&quot;]=...进行赋值将自动添加新列，frame.new_column_name不可以 删除列：del frame[&quot;existed_column_name&quot;]，del frame.existed_column_name不可以 获取列名：frame.columns 表的转置：frame.T，Q:是否是视图？ 获取ndarray数组：frame.values当列的数据类型不一致时返回数组的数据类型将使用能兼容所有列的数据类型的类型 索引对象管理轴标签、轴名称等额外数据字符串索引：&lt;class &#39;pandas.core.indexes.base.Index&#39;&gt;自然数索引：&lt;class &#39;pandas.core.indexes.range.RangeIndex&#39;&gt; index可以整体替换掉，但不能单个修改(immutable) → 安全共享 。。。。。。 操作Seies和DataFrame中的数据重新索引 obj.reindex(…) 如果新索引中包含原索引中没有的值将产生缺失值，可以使用关键字fill_value指定缺失默认值 时间序列的前向插值：时间轴上缺失的索引对应的值与比它大的原来存在的索引对应的值的相同（前向插值） 使用前向差值：obj.reindex(NEW_INDEX, method=&#39;ffill&#39;) 使用后向差值：obj.reindex(NEW_INDEX, method=&#39;bfill&#39;) 实际上还有更精确的插值方式… DataFrame的重新索引 行重新索引: frame.reindex(NEW_INDEX) 列重新索引: frame.reindex(columns=NEW_INDEX) 行列同时重新索引：frame.reindex(NEW_ROW_INDEX, columns=NEW_COLUMN_INDEX) 插值：只能按行插，即按行索引插 丢弃指定轴上的项 obj.drop(…)将返回一个新对象！！ 原来的索引：[‘a’,’b’,’c’,’d’,’e’]丢弃：obj.drop(&#39;c&#39;)，新索引：[‘a’,’b’,’d’,’e’]丢弃：obj.drop([&#39;c&#39;,&#39;d&#39;])，新索引：[‘a’,’b’,’e’] 索引、选取、过滤 直接索引：obj[&#39;c&#39;] 切片包含末端！！！ 结合列的切片和布尔运算来选取行 frame[frame[&#39;three&#39;] &gt; 3]将以”three”列为依据选取”three”列中所有值大于3的行 → 所以实际选取的是特定的某些行 frame[frame &lt; 5 = 0]给表中所有小于5的位置重新赋值为0 frame &lt; 5 本身应该是布尔值（逻辑运算），所以会得到一个由布尔值组成的新DataFrame布尔运算的特性实际上应该继承自numpy 索引关键字ix 算数运算和数据对齐数据对齐：Series或者DataFrame对象间的加法会自动合并相同轴上的索引并自动填充NaN → 取并集 DataFrame对象的算数运算（以加法为例）* `df = df1 + df2` 以df1为主表，将df2的行、列索引加到df1里面去：df1中存在的将执行加法，不存在的为NaN * `df = df1.add(df2, fill_value=0)` 可以这样理解： * 先将df2的行列索引合并到df1中得到并集表df（暂时假设将所有值设为NaN） * 再将主表df1中的存在的值填入df（去掉一部分NaN） * 再将df中剩余的NaN位置改成`fill_value`（去掉所有NaN） * 再将df2中所有元素加到df的相关位置（df2中所有索引必定已经存在于df中） * （括号内的内容纯属方便理解，尚未验证，算不得数） * 其他运算类似： * 减法 sub - * 乘法 mul * * 除法 div / DataFrame和Series间的运算涉及广播机制（broadcasting），熟悉numpy的话应该也熟悉广播机制。。。 * 最简单的广播：Series的索引与DataFrame的行/列索引相同 * 上述索引不完全相同时，先对索引取并集，再进行广播 函数应用和映射* numpy中操纵数组元素的方法（如np.abs)也可以用来操纵pandas对象 * 操纵pandas对象的行/列数组使用 `obj.apply(lambda x: x.min(), axis=1)`（一般用不上） * 使用元素级的python函数操纵pandas元素：例如`obj.applymap(lambda x: &quot;{:.2f}&quot;.format(x))` 排序* Series按index排序：`obj.sort_index()` * Series按values排序：`obj.order()`,缺失值将按index排序后丢到末尾 * DataFrame按指定轴的index排序：`df.sort_index(axis=0)`,默认为0 * DataFrame按某列值排序：例如 `df.sort_index(by=&apos;a&apos;)` * DataFrame按多列值排序：例如 `df.sort_index(by=[&apos;a&apos;,&apos;b&apos;])` * 默认升序，降序设置 `ascending=False` 排名(ranking)根据某种规则破坏评级关系。原著是这么说的，不过很费解不是吗 以 obj = Series([7, -5, 7, 4, 2, 0, 4]) 为例，obj.rank() 返回的实际上是 Series([6.5, 1.0, 6.5, 4.5, 3.0, 2.0, 4.5])。首先我们可以获取一个类似于argsort的序列[6, 1，7, 4, 3, 2, 5]，然后考虑相同的值：两个‘7’的位置分别为7和6，因此都改为均值(7+6)/2=6.5，两个‘4’的位置分别为7和6，因此都改为均值(5+4)/2=4.5，因此算出结果。（至于有什么用，现在我还不知道。。。） 关键词 method : {&#39;average&#39;, &#39;min&#39;, &#39;max&#39;, &#39;first&#39;, &#39;dense&#39;} 指定排名方法： average：默认值，如上取相同值索引的平均值以保证每个相同的元素具有相同的排名 return Series([6.5, 1.0, 6.5, 4.5, 3.0, 2.0, 4.5]) first：与argsort功能相同，按值出现顺序排名，结果应该是连续的正整数 return Series([6.0, 1.0, 7.0, 4.0, 3.0, 2.0, 5.0]) min：相同值的排名全部取最小的那个 return Series([6.5, 1.0, 6.0, 4.0, 3.0, 2.0, 4.0]) max：相同值的排名全部取最大的那个 return Series([7.0, 1.0, 7.0, 5.0, 3.0, 2.0, 5.0]) dense：min和max会跳值，dense不会 return Series([5.0, 1.0, 5.0, 4.0, 3.0, 2.0, 4.0]) 带有重复值的轴索引一般情况下轴索引的值唯一（类似于“键”的概念），但这并不是硬性要求。也就是说，轴索引可以重复@_@。（一般应该也用不上吧？） 判断pandas对象的索引值是否唯一：obj.index.is_unique: bool。（pandas对象指的是Series对象或者DataFrame对象） 对不唯一的索引进行访问将返回所有符合值构成的pandas对象，访问唯一索引返回标量 常用的数学和统计方法约简方法 求和：默认按列求和，自动排除NaN值：df.sum(axis=0, skipna=True)。按行求和指定 axis=1;禁用自动跳过NaN指定 skipna=False count,min,max,argmin,argmax,idxmin,idxmax,sum,mean,media,mad,var,std,… 累计方法 累计求和：df.cumsum() 特殊方法 df.describe()：一次性产生多个统计结果（统计量) 有趣的统计量 corr、corrwith：相关系数 cov：协方差（矩阵） 。。。。 改天再看，顺便练习盲打。。。]]></content>
      <categories>
        <category>python编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回归模型结果度量指标]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180306-c444.html</url>
    <content type="text"><![CDATA[回归模型因为预测值连续，很难用‘准确度’这类指标去衡量。回归模型评估方法主要有： MAE (Mean Absolute Error, 平均绝对误差)sklearn API: from sklearn.metrics import mean_absolute_errorformula:$$MAE=\frac1n\sum_{i=1}^n{|f_i-y_i|}$$ 这个很矬，一般不用！ MSE (Mean Squre Error, 均方误差)sklearn API: from sklearn.metrics import mean_squared_errorformula:$$MSE=\frac1n\sum_{i=1}^n{(f_i-y_i)^2}$$ 这个很流行！ RMSE (Rooted Mean Square Error, 均方根误差)formula:$$RMSE=\sqrt{MSE}$$ 与上面的MSE没有区别，一般使用MSE即可！注意：如果是在迭代过程中作为loss函数，需要合理选择MSE/RMSE R2 (R-Squared)sklearn API: from sklearn.metrics import r2_scoreformula:$$y_m=\frac1n\sum_{i=1}^n{y_{t_i}}$$$$R^2=1-\frac{\sum_{i=1}^n{(y_{p_i}-y_{t_i})^2}}{\sum_{i=1}^n{(y_{t_i}-y_m)^2}}$$$y_p$ means prediction while $y_t$ means truth. 将预测值与只使用均值的情况进行比较 当 $y_{p_i}=y_m$时，$R^2=0$ $\rightarrow$ 完全不沾边，还不如使用均值 当 $y_{p_i}=y_{t_i}$ 时，$R^2=1$ $\rightarrow$ 完全匹配（实际上应该是达不到的） 据说 $0{\le}R^2{\le}1$，尚未亲身验证]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类结果评估]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F20180302-90e6.html</url>
    <content type="text"><![CDATA[一个好的聚类结果要求较高的簇内相似度 (intra-) 和较低的簇间相似度 (inter-) 。根据模型比较的对象不同，聚类结果评估指标可以分为外部指标 (external) 和内部指标 (internal) 两类： 外部指标：需要参考模型，比较样本在不同模型中的分类情况； 内部指标：基于自己的聚类结果，不借助模型间的比较。 外部指标外部指标一般用于： 以他人的模型作为参考模型进行比较； 以标签作为基准模型进行比较。 首先，将任意两个样本配对，因此n个样本将组成 $N=C_n^2=\frac{n(n-1)}{2}$ 对。然后，将上述对子分成四类 (S means Same while D means Different)： 集合A=|SS|: 在本模型中属于同一簇，在参考模型中也属于同一簇 集合B=|SD|: 在本模型中属于同一簇，但在参考模型中属于不同的簇 集合C=|DS|: 在本模型中属于不同的簇，但在参考模型中属于同一簇 集合D=|DD|: 在本模型中属于不同的簇，在参考模型中也属于不同的簇 如果参考模型是理想模型，上述：A=TP(真阳性)，B=FP(假阳性),C=FN(假阴性),D=TN(真阴性)。 Jaccard系数定义杰卡德系数 (Jaccard Coefficient, JC) 用于比较有限集合$X$和$Y$之间的相似性与差异性：$$J(X,Y)=\frac{X{\cap}Y}{X{\cup}Y}=\frac{X{\cap}Y}{X+Y-X{\cap}Y}$$特别地，当 $X=Y=\emptyset$ 时，$J(X,Y)=0$。 Jaccard系数描述集合相似性：Jaccard系数越大，集合相似性越高；Jaccard距离描述集合差异性：Jaccard距离越大，集合差异性越大。 Jaccard距离定义为：$$d_J(X,Y)=1-J(X,Y)=1-\frac{X{\cap}Y}{X{\cup}Y}=\frac{X{\triangle}Y}{X{\cup}Y}$$记 $X{\triangle}Y = X{\cup}Y - X{\cap}Y$ 集合与矩阵的转换 例如对于集合 $X={a,b,c}$ 和 $Y={b,c,d}$ 有 $X{\cup}Y={a,b,c,d}$ 和 $X{\cap}Y={b,c}$。 那么求并和求交的操作怎么通过矩阵的逻辑运算实现呢？ 基于特征集{a,b,c,d}将两个集合表示为增广向量：$$X^\ast=(1,1,1,0)$$$$Y^\ast=(0,1,1,1)$$ 求交 等效于逻辑与，求并等效于逻辑或：$$X{\cap}Y{\leftarrow}X^\ast\&amp;Y^\ast=(0,1,1,0){\rightarrow}{b,c}$$$$X{\cup}Y{\leftarrow}X^\ast|Y^\ast=(1,1,1,1){\rightarrow}{a,b,c,d}$$ X和Y为矩阵时，计算方法类似。 迁移 我们将两两配对表示为一个矩阵，例如：$$X=\left[\begin{matrix}0 &amp; 0 &amp; 0 &amp; 1 \\0 &amp; 0 &amp; 1 &amp; 1 \\0 &amp; 1 &amp; 0 &amp; 1 \\1 &amp; 1 &amp; 1 &amp; 0\end{matrix}\right],Y=\left[\begin{matrix}0 &amp; 0 &amp; 1 &amp; 1 \\0 &amp; 0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0 &amp; 1 \\1 &amp; 1 &amp; 1 &amp; 0\end{matrix}\right]$$其中0表示两个样本被预测为同一簇，1表示两个样本被预测为不同簇。 Jaccard系数定义为：$$ JC = \frac{X\&amp;Y}{X|Y} = \frac{|DD|}{|DD|+|SD|+|DS|} $$ 应用Jaccard系数主要用于计算文本相似度，例如新闻去重、网页去重、考试作弊检测、论文查重等。适合稀疏度很高的数据。 缺点Jaccard系数计算的总是二值矩阵，不能解决信息更丰富的矩阵。 广义系数广义Jaccard系数能处理多值向量：$$\begin{split}J(x,y)&amp;=\frac{\sum_{i=1}^{n}{\min{(x^{(i)},y^{(i)})}}}{\sum_{i=1}^{n}{\max{(x^{(i)},y^{(i)})}}} \\J(X,Y)&amp;=\frac{\int{\min{(f,g)}du}}{\int{\max{(f,g)}du}}\end{split}$$ FM指数FM指数 (Fowlkes and Mallows lndex，FMI) 多用于两个层次聚类间的比较或者与基准参考聚类模型间的比较，FM指数越大说明两个聚类结果越相近。 Fowlkes, E. B., &amp; Mallows, C. L. (1983). A method for comparing two hierarchical clusterings. Journal of the American statistical association, 78(383), 553-569. 与基准聚类模型比较FM指数 (Fowlkes and Mallows lndex，FMI) 定义为：$$FMI = \sqrt{ \frac{TP}{TP+FP} {\times} \frac{TP}{TP+FN} }$$ 层次聚类模型的比较略 RI 和 ARI给定 $n$ 个样本的集合 $S$，$X={X_1,X_2,{\cdots},X_r}$ 和 $Y={Y_1,Y_2,{\cdots},Y_s}$ 是关于 $S$ 的两个的划分。因为聚类的过程就是再对数据集进行划分，所以一个划分就等效于一个聚类模型。X和Y同时也意味着两个聚类模型 (clustering)。 RI定义兰德指数 (Rand Index, RI)：$$RI = \frac{|SS|+|DD|}{|SS|+|SD|+|DS|+|DD|} = \frac{||SS|+|DD|}{C_n^2}$$直观上理解，分子是X和Y认可一致的配对，分母是所有可能的配对。当Y是基准划分时，RI表示真阳性数据和真阴性数据所占的比重。 ARI为了实现“在聚类结果随机产生的情况下，指标应该接近零”提出了 调整兰德指数 (Adjusted Rand Index, ARI) 。 构建列联表 (contingency table):其中 $n_{ij}$ 表示X的第i簇与Y的第j簇共有的元素，即 $n_{ij}=|X_i{\cap}Y_j|$。 ARI 定义如下：$$ARI= \frac{RI-E[RI]}{\max{(RI)}-E[RI]} = \frac { \overbrace{ \sum_{ij}{C_{n_{ij}}^2}}^{\text{Index}} - \overbrace{\frac{\sum_iC_{a_i}^2\sum_jC_{b_i}^2}{C_n^2}}^{\text{Expected Index}} }{ \underbrace{\frac{\sum_iC_{a_i}^2+\sum_jC_{b_i}^2}2}_{\text{Max Index}} - \underbrace{\frac{\sum_iC_{a_i}^2\sum_jC_{b_i}^2}{C_n^2}}_{\text{Expected Index}} }$$ ARI 的取值范围扩展到了[-1, 1]，起到了衡量 两组数据分布的吻合程度 的作用。 MI 和 NMI互信息 (Mutual Information, MI) $$MI(X,Y)=\sum_{x,y}{ p(x,y)\log{ \frac{p(x,y)}{p(x)p(y)} } }$$ 标准互信息 (Normalized Mutual Information, NMI) $$U(X,Y)=2R=2\frac{I(X;Y)}{H(X)+H(Y)}$$ $$H(X)=\sum_{i=1}^{n}{p(x_i)I(x_i)}=\sum_{i=1}^{n}{p(x_i)\log_b{\frac1{p(x_i)}}}=-\sum_{i=1}^{n}{p(x_i)\log_b{p(x_i)}}$$ 内部指标所谓的内部指标就是基于本模型的聚类结果进行计算。 为了衡量样本之间的相似性和差异性，我们需要先定义一个距离公式 dist(a, b) ，距离的定义需要满足下面四个基本性质： $dist(a, b) {\ge} 0$ 恒成立 当且仅当 a = b 时 $dist(a, b) = 0$ 对称：$dist(a, b) = dist(b, a)$ 三角不等式： $dist(a, b) {\le} dist(a, c) + dist(c, b)$ 当然也可以使用经典距离，例如欧式距离、余弦距离、Hamming距离等等。基于距离，我们定义以下量： $avg(C)$：簇内两点间的平均距离 $diam(C)$：簇内样本间的最大距离 $d_{min}(C_i,C_j)$：两个簇最近样本间的距离 $d_{cen}(C_i,C_j)$：两个簇中心点（簇的重心）间的距离 Davies-Bouldin Index戴维森堡丁指数 (Davies-Bouldin Index, DBI) Dunn Validity Index邓恩指数 (Dunn Validity Index, DVI) 轮廓系数 (Silhouette coefficient)]]></content>
      <categories>
        <category>机器学习与算法基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-基于TFRecord加载图像数据]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F20180301-e69e.html</url>
    <content type="text"><![CDATA[什么是TFRecord文件？将一组图像数据和它们对应的标签存在一个二进制文件里，这个文件就是TFRecord文件。 创建自定义的牙骨项目数据加载器 核心步骤： 声明一个用于生成TFRecord文件的writerwith tf.python_io,TFRecordWriter(tfr_fp) as writer: …# tfr_fp是生成文件路径 获取图像的二进制数据：例如 PIL::Image 对象 img 可以使用 img.tobytes() 获取二进制信息 然后套用tensorflow提供的API： 红色下划线为关键APIA：标识图像标签的key，加载TFRecord文件的时候会用到B：注意类型对应：图像标签为int类型，图像内容为bytes类型形参 value 的值注意加 [] 使用1.老规矩，先定义队列q = tf.train.string_input_producer([tfr_fp]) 这里传入的是 `[tfr_fp]` ，代表建立的实际上是一个文件名队列 2.定义读取TFRecord文件的阅读器reader = tf.TFRecordReader() 3.用这个reader读取TFRecord文件的文件名队列得到序列化的样例：_, serialized_example = reader.read(q) 4.从样例中解析出图像数据和图像标签features = tf.parse_single_example( serialized_example, features = { &apos;label&apos;: tf.FixedLenFeature([], tf.int64), &apos;img_raw&apos;: tf.FixedLenFeature([], tf.string) } ) 5.从二进制数据中解码图像img = tf.decode_raw(features[&apos;img_raw&apos;], tf.uint8) img = tf.reshape(img, SHAPE) 6.千万别忘了在session中开启线程，被坑傻了传送门]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[假设检验原理2-卡方检验]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%2F20180228-5746.html</url>
    <content type="text"><![CDATA[什么是$\chi^2$分布？若k个独立随机变量 $Z_1,Z_2,…,Z_k$ 服从标准正态分布N(0,1)，则这k个随机变量的平方和$X=\sum{Z_i^2}$ 服从自由度为k的卡方分布。记作 $X\sim\chi^2(k)$。卡方分布的期望为n，方差为2n。 $\chi^2$适合性检验适用范围：比较观测数与理论数是否符合 检验假设：$H_0$: O - E = 0 (观测数与理论数间没有差异) $\chi^2$独立性检验适用范围：常用来判定两个变量之间是独立的还是相互影响的 列联表的独立性检验2x2列联表检验假设：$H_0$: 事件A和时间B无关$H_1$: 事件A和时间B有关 连续性矫正 $r{\times}c$ 列联表这是一个 $2{\times}3$ 列联表：上表中一共有两个变量： 性别：有2个取值（男、女） 肥胖程度：有3个取值（不肥胖、轻度肥胖、中/重度肥胖） $\chi^2$检验验证的是两个变量是否独立，即”性别和肥胖程度无关”]]></content>
      <categories>
        <category>统计学基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python自用进度条]]></title>
    <url>%2Fpython%E7%BC%96%E7%A8%8B%2F20180201-ffef.html</url>
    <content type="text"><![CDATA[自定义一个简单的进度条，并没有考虑性能怎么样。。。 import sys def process_bar(i, n, pref_str=&apos;&apos;, suff_str=&apos;&apos;, char=&apos;=&apos;, num_chars=100): &apos;&apos;&apos; :param i: 计数器,从1开始 :param n: 计数器最大值 :param char: 进度条符号 :param pref_str: 进度条前置字符串 :param suff_str: 进度条后置字符串 &apos;&apos;&apos; num = i * num_chars // n pre = char * num pro = &apos;&gt;&apos; + &apos; &apos; * (num_chars - 1 - num) if num - num_chars else &apos;&apos; numerator = &apos; &apos; * (len(str(n)) - len(str(i))) + str(i) if num - num_chars: sys.stdout.write(&quot;%s|%s/%d|%s%s|%s%%|%s\r&quot; % ( pref_str, numerator, n, pre, pro, &apos; &apos; + str(num * 100 // num_chars), suff_str)) else: sys.stdout.write(&quot;%s|%s/%d|%s%s|%s%%|%s\n&quot; % (pref_str, numerator, n, pre, pro, 100, suff_str)) i 取值从 0 到 n-1 ，符合 range 函数习惯]]></content>
      <categories>
        <category>python编程</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-基于队列和多线程加载数据]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F20180126-f29a.html</url>
    <content type="text"><![CDATA[建立如下的文件 fileslist.csv注：建议采用csv格式，第一列是图像路径，第二列是标签（建议标签预处理成数字），分隔符为 , 将 fileslist.csv 按行加载到python内置列表，假设为 imgNames# python with open(&apos;fileslist.csv&apos;,&apos;r&apos;) as fin: imgNames = fin.read().strip().split(&apos;\n&apos;) 建立文件名队列建队#python fn_queue = tf.train.string_input_producer(imgNames) #Other opt-parameters # num_epochs: 读取文件一次为一个epoch，默认无限次读取 # shuffle: 读取文件时是否乱序 # seed: 当shuffle为True时设置随机数种子(可选) # capacity: 队列容量，默认为32(很显然需要大于batch-size) # share_name, name, cancel_op 出队#python item = fn_queue.dequeue() 注： 出队的只是一个符号(参考tensorflow计算图的概念)，真正的值要在Session中获取 此时 item 相当于 fileslist.csv 中的一行（一个记录），每个记录具有相同的形式！ 显然 item 不是标准形式，还需要在后面进行进一步加工 单个记录(item)数据标准化：根据图像 路径字符串 解析出 数组对记录 item 解码得到路径 img_path 和标签 img_label#python img_path, img_label = tf.decode_csv(item, [[&apos;&apos;],[1]], field_delim=&apos;,&apos;) 参数解释 records : 必须参数，字符串类型的张量，这里就是 fn_queue 出队的结果 item record_defaults : 必须参数，指定返回值的类型(这里返回值类型的定义很有意思~~)这里直观返回值类型为 str, int , 因此形参赋值 [[&#39;&#39;], [1]] (惊奇)第一个 [] 内可以是任意 str 对象，第二个 [] 内可以是任意 int 对象 field_delim : 建议显示声明，csv文件列间分隔符，默认为 , use_quote_delim : unknown, unimportant name na_value : NA/NaN格式选择，没啥用 解析 img_path 的内容img_str = tf.read_file(img_path) img_dtr 实际上还是一个字符串，与包含文件名内容的 img_path 不同,img_str 包含的是文件内容 对图像内容字符串 img_str 进行解析，获取图像的数字信息img_data = tf.image.decode_jpeg(img_str, channels=3) 参数解释 content : 必须，字符串张量，此处为 img_str channels=0 ： 必须指定，常见图像通道数可能取值有 1, 3, 4 ratio=1 fancy_upscaling=True try_recover_truncated=False acceptable_fraction=1 dct_method=&#39;&#39; name=None 由flatten化的数字信息确定图像矩阵尺寸（说白了就是reshape）img_array = tf.image.resize_images(img_data, [HEIGHT， WIDTH]) 可选参数 method=ResizeMethod.BILINEAR : 变形方法 align_corners=False 此处图片的尺寸应写为[高，宽], 即行作为数组的第一维度 获取批次数据（batch subset）images, labels = tf.train.shuffle_batch([img_array, img_label], batch_size=8, capacity=100, min_after_dequeue=20) 完整参数解释 tensors : 必须，待打包的记录列表，此处为 [img_array, img_label]因为 img_array 和 img_label 在这里都只是个符号，也就不难理解上述赋值了~ batch_size : 必须，与训练过程的 batch_size 相同因此在使用队列训练模型时，batch_size需要作为超参进行预赋值 capacity : 建议，队列容量注意这里的队列与前面的文件名队列不同 min_after_dequeue : 建议，每次出队后队列中剩余的记录此参数可用来衡量shuffle水平，剩余记录数量越大，shuffle程度越高 num_threads=1 ： 可选，入队线程数，通常情况下1个就够用了 seed=None : 可选，设置shuffle随机种子 enqueue_many=False : 可选，一个记录一个记录的入队还是多个一起入队 shapes=None allow_smaller_final_batch=False : 当每个epoch最后一个batch数目不足时是否允许灵活调整batch大小 share_name=None : 多个Session共用数据加载队列时使用，一般用不上 name=None 然而，以上都是在定义队列加载的图 。。。 Session里面如何使用开启队列？with tf.Session(...) as sess: coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) sess.run(tf.global_variables_initializer()) X, y = sess.run([images, labels]) 利用feed_dict传入X,y进行训练... coord.request_stop() coord.join(threads=threads) Author: Barwe(YinChen)Email: 995488247@qq.com本文地址: http://barwe.top/deep-learning/2018-01-26/20180126111829/]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10系统将C盘的用户文件夹迁移至D盘]]></title>
    <url>%2FOS%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F20180125-f59f.html</url>
    <content type="text"><![CDATA[C盘终极清理！ 一、win10默认禁用管理员账户：1、打开“计算机管理”：① 右键“我的电脑”- 管理 ② cmd - 键入“计算机管理”- 打开 2、激活管理员账户：@ 右键“计算机管理（本地）&gt;系统工具&gt;本地用户和组&gt;用户&gt;Administrator”选择属性 @ 去掉“账户已禁用”的对勾 二、切换至管理员1、注销当前用户，切换至Administrator用户2、打开cmd，执行 robocopy &quot;C:\Users\USERNAME&quot; &quot;D:\Users\USERNAME&quot; /E /COPYALL /XJ3、可能需要安装文件解锁工具 LockHunter：程序接口位于右键菜单中！4、解锁 “C:\Users\USERNAME”：右键菜单选择API进入解锁界面5、删除”C:\Users\USERNAME”：cmd中执行 rmdir &quot;C:\Users\USERNAME&quot; /S /Q6、建立软连接：cmd中执行 mklink /J &quot;C:\Users\USERNAME&quot; &quot;D:\Users\USERNAME&quot;]]></content>
      <categories>
        <category>OS学习手册</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-计算资源控制]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F20180125-ff54.html</url>
    <content type="text"><![CDATA[控制GPU和CPU的使用。 tf.ConfigProto()的参数使用CPU计算时指定核数1device_count = &#123;'CPU': 4&#125; 使用GPU进行计算:123gpu_options = tf.GPUOptions( per_process_gpu_memory_fraction = 0.25, allow_growth = True) per_process_gpu_memory_fraction 指定每个显存占用比例 allow_growth 允许显存占用自适应增长 并行计算相关设置独立op间并行计算1inter_op_parallelism_threads = 1 op内部（如矩阵乘法）并行计算1intra_op_parallelism_threads = 1 其他参数是否打印设备分配日志1log_device_placement = True 如果你指定的设备不存在，允许TF自动分配设备1allow_soft_placement = True 构建config选择必要参数123config = tf.ConfigProto( device_count = ... 使用CPU核心，可选 gpu_options = ... GPU显存控制，可选 将config传递给Session1session = tf.Session(config=config)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
