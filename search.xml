<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[C++函数模板在分离式编译中的坑]]></title>
    <url>%2FCpp%2F20201019-1e61.html</url>
    <content type="text"><![CDATA[C++ 中 模板 是 泛型 编程的基础，但是模板在 分离式编译 中又有许多坑~ 函数模板函数模板（Function Template）虽然也定义了函数实现的具体过程，但是它并不能完全等同于普通函数。 在 C++ 中，类型 和 值 是 数据 的两个主要特征。普通函数限定了数据的类型，只对数据的值进行操作。 当我们需要对不同类型数据的值进行相同的操作时，就需要为每一种数据类型各自定义对应的普通函数，这意味着大量冗余重复的代码。 函数模板 就是为了解决这样一个问题：针对不同数据类型的相同操作只定义一次函数。 函数模板实际上是在普通函数的基础上，将数据的 类型 看作了参数，从而建立起了一个 模板，这个模板可以通过指定特定的数据类型而 实例化 为具体的普通函数。模板中传递数据类型的参数叫做 模板参数（Template Parameters）: 12template &lt;class T1, class T2&gt;returnType funcName(T1 t1, T2 t2) &#123; ... &#125; 通过 template 关键字定义一个模板，T1 和 T2 是模板参数，class 也可替换成 typename。 实例化函数模板与函数的区别在于：函数在定义时就已经自动实例化（实例化的意思就是编译时产生了对应的二进制代码），因为函数只处理数据的值，而数据的类型是已知的；函数模板需要指定模板参数转化为普通函数才能被实例化，这就意味着如果只是定义一个模板，编译器会忽略它，这是理所应当的，因为编译器不知道模板的具体数据类型。模板参数的值只有当函数模板被 调用 时才能确定，也就是说，调用函数模板会自动实例化一个特定数据类型的普通函数，这种依赖于实际调用和传输参数数据类型的函数模板实例化过程叫做 隐式实例化（Implicit Instantiation）。 除了隐式实例化之外，我们还可以在函数模板发生实际调用之前主动对函数模板进行实例化，即主动告诉编译器我们需要对哪些特定的数据类型（模板参数）生成普通函数，这种方式叫做 显式实例化（Explicit Instantiation）。 12345678910111213// 定义函数模板template &lt;class T&gt; void swap(T &amp;x, T &amp;y) &#123; T temp = x; x = y; y = temp;&#125;// 显式实例化template void swap(int&amp;, int&amp;);// 隐式实例化float x = 1.23, y = 3.21;swap(x, y); 特化有时候，函数模板并不适用于某些特殊的数据类型。例如上面的 swap 函数，如果 T 是复杂的类，而我们想交换的是对象的两个属性，则上面定义的模板并不能实现这个目的。此时我们可以针对特殊的数据类型对函数模板进行修改，即函数模板的 特化（Specialization）。 特化实现的是普通函数，它需要重写函数的定义过程，并自动实例化（因为数据的类型指定了）。 特化实际上相当于函数的重载：相同的函数名，形参具有不同的数据类型。 1234567891011121314// 函数模板template &lt;class T&gt; void swap(T &amp;x, T &amp;y) &#123; ... &#125;// 特化template &lt;&gt; void swap(Books &amp;x, Books &amp;y) &#123; string title = x.title; x.title = y.title; y.title = title;&#125;class Books &#123; public: string title;&#125; 分离式编译分离式编译（Separate Compilation）：C++ 以 cpp 文件为基本编译单元，将每个 cpp 文件单独编译成目标文件，最后通过连接器连接所有的目标文件生成可执行文件。 为什么要连接？一个目标文件中的函数/类/变量的定义可能在其它的目标文件中，并且程序的执行入口只在其中某一个目标文件中。 每一个 cpp 文件在被独立编译时都会 包含（include）相关的头文件，如果两个 cpp 文件同时引入了相同的头文件，意味着这个头文件会被编译两次，这表示头文件中的对象会被多次声明，这将导致 redefinition 的错误。正确的做法是在头文件中使用 条件编译。 条件编译相关宏指令：#if, #ifdef, #ifndef, #endif, #else, #elif 和 #define 因为函数模板的定义实际上不会被编译成二进制代码（实例化），所以函数模板的分离式编译就会出现一些问题。 下面是一个经典的分离式编译，三个文件分别实现了函数的声明、定义和调用： 函数声明 swap.h： 123#ifndef SWAP_H#define SWAP_Htemplate &lt;class T&gt; void swap2(T&amp;, T&amp;); 函数定义 swap.cpp： 123456#include "swap.h"template &lt;class T&gt; void swap2(T &amp;x, T &amp;y) &#123; T temp = x; x = y; y = temp;&#125; 函数调用 main.cpp： 1234567891011#include &lt;iostream&gt;#include "swap.h"using namespace std;int main() &#123; int x = 3, y = 5; swap2(x, y); cout &lt;&lt; "x = " &lt;&lt; x &lt;&lt; endl; cout &lt;&lt; "y = " &lt;&lt; y &lt;&lt; endl; return 0;&#125; 编译器分别编译 swap.cpp 和 main.cpp 两个文件。 由于两个文件都包含了 swap.h 文件，因此需要使用条件编译。 编译器在编译 swap.cpp 时虽然会检查函数模板的语法，但是却不会生成相应的函数代码，因为模板参数是未知的。 main.cpp 中调用了函数 swap2&lt;int&gt;，编译器只能在 swap.h 中找到函数模板 swap2&lt;T&gt; 的声明，而没有模板的定义。这导致函数模板不能被正确实例化，报错信息提示未定义的函数 void swap2&lt;int&gt;(int&amp;, int&amp;)： 1undefined reference to void swap2&lt;int&gt;(int&amp;, int&amp;) 因为 main.cpp 包含了 swap.h 文件，如果我们在 swap.h 文件中实现了函数模板，则不会报错： 123456789#ifndef SWAP_H#define SWAP_Htemplate &lt;class T&gt; void swap2(T&amp;, T&amp;);template &lt;class T&gt; void swap2(T &amp;x, T &amp;y) &#123; T t = x; x = y; y = t;&#125;#endif 因为此时编译 main.cpp 时能够在 swap.h 中找到 函数模板的定义，并且根据实际调用时的变量类型对函数模板进行 隐式实例化 生成 void swap2&lt;int&gt;(int&amp;, int&amp;) 相关的代码。但是，这种写法明显不符合分离式编译的规范：在头文件中声明，在源文件中定义。 一种较好的做法是在 swap.cpp 中对函数模板进行 显式实例化： 1234567#include "swap.h"template &lt;class T&gt; void swap2(T &amp;x, T &amp;y) &#123; T t = x; x = y; y = t;&#125;template void swap2&lt;int&gt;(int&amp;, int&amp;); 没想明白的是，显式实例化语句写在头文件中会提示找不到模板的定义，而写在源文件中的任何位置都可以~ 进行显式实例化后，就可以正常编译啦： 123&gt; g++ main.cpp swap.cpp &amp;&amp; a.exe &amp;&amp; del a.exeBefore swapping: x = 3, y = 5After swapping: x = 5, y = 3 类模板类模板 与 函数模板 类似： books.h 1234#ifndef BOOKS_H#define BOOKS_Htemplate &lt;class T&gt; class Books;#endif books.cpp 123#include "books.h"template &lt;class T&gt; class Books &#123; ... &#125;;template class Books&lt;int&gt;; 参考资料为什么C++编译器不能支持对模板的分离式编译 C++ 函数模板 实例化和具体化]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[高阶函数之装饰器(1)]]></title>
    <url>%2Fpython%2F20200928-95fe.html</url>
    <content type="text"><![CDATA[当我们实现了某个核心模块（函数）后，可能按照实际情况需要为这个模块添加额外的功能，例如统计程序运行时间、记录日志等等。此时，我们并不希望去修改模块本身的代码（想想都觉得很low），也不希望改变函数的调用方式（不然那么多调用每个都去改岂不是也很low）。装饰器 提供了解决这一问题的方案。简单来说，装饰器可以在核心代码的外面套一层壳，执行这段带壳的代码就能达到我们的目的。我将从一个 Python 渣渣的角度理解什么是 装饰器。 套壳这一操作通过 @语法糖 实现，@语法糖指定了使用什么壳（装饰器函数的引用），壳的内容通过 装饰器函数 定义。 我们在面向对象编程时用到的 @staticmethod 和 @classmethod 就属于装饰器的一种。 这里只以最简单的 ”壳“ 理解装饰器的执行逻辑。 最基础的装饰器装饰器会对目标函数进行包装，然后将目标函数名指向包装后的函数定义。一般语法是 12@decoratordef func(*args, **kwargs): pass 这里我们将 decorator 称之为 装饰器函数，将 func 称之为 目标函数。这里我们需要注意的是，使用 @ 语法时不仅仅是在定义目标函数，也是在调用装饰器函数，搞清楚这一点就能理解 @ 语法的作用机制。 一般来说，装饰器函数的 引用 紧跟在 @ 符号后面，紧接着下一行就是目标函数的定义。装饰器函数只接受一个参数，那就是目标函数。所以说，装饰器函数是一个 高阶函数，它以函数引用作为参数，同时返回另一个函数的引用。 当程序检测到 @ 符号时，会 调用（或者叫 执行） 装饰器函数，传入目标函数的引用，返回另外一个函数的引用，并将指向原来目标函数的变量（上面的 func 变量）指向返回的新函数。所以上面的示例代码就相当于： 12345## 定义一个函数（具体实现略），并定义一个变量func（函数变量）指向这个函数def func(*args, **kwargs): pass## 调用装饰器函数，并使func变量指向装饰器返回的那个函数func = decorator(func) 说完了 @ 符号，我们再来谈谈 装饰器函数 本身。上面提到了，装饰器函数接受目标函数作为参数并返回一个新的函数。 我们之所以称之为 装饰器，是因为我们想扩展目标函数的功能，例如统计目标函数的执行时间等等。 换言之，目标函数定义了模块的核心功能，而某些时候我们需要对该模块做一些拓展，例如统计运行时间。 所以装饰器函数的基本形式应该是这样的： 123456789101112def decorator(func): ## 我们加上下面这一句可以验证执行到@时是否执行了装饰器函数 print('run decorator ...') def wrapper(*args, **kwargs): ## do something, such as print('run wrapper ...') result = func(*args, **kwargs) ## do something, such as print('wrapper end!') return result print(wrapper) return wrapper 上面描述了一个装饰器函数的基本模型，它主要定义了两个东西： 局部的函数变量 func：decorator.&lt;locals&gt;.func，它指向了目标函数 内嵌的函数 wrapper：decorator.&lt;locals&gt;.wrapper，目标函数的函数变量将指向它 执行 decorator 函数返回了另外一个函数 wrapper 的引用，显然这个函数的 __name__ 属性是 wrapper。 下面是用装饰器修饰目标函数的一个例子： 123@decoratordef exp(x, y): return x ** y 运行这段代码的输出是： 12run decorator ...&lt;function decorator.&lt;locals&gt;.wrapper at 0x0000023AEDADC488&gt; 可以看到，@ 确实执行了装饰器函数 decorator，同时 定义 了一个函数 wrapper。 但是 wrapper 函数并没有执行，我们再次 1234print(exp(2,3))#&gt; run wrapper ...#&gt; wrapper end!#&gt; 8 可以看到，此时执行了 wrapper 函数，同时返回了计算结果，最终使用 print 函数打印了返回结果。 检查函数变量 exp 的 __name__ 属性，发现不再是 “exp”，而是 “wrapper”： 12exp.__name__#&gt; 'wrapper' 带参数的装饰器此外，还有一种带参数的装饰器，例如： 12345@auth('barwe')def access_database(*args, **kwargs): print("访问某个数据库...") access_database() 假定的执行结果是 123函数（access_database）准备授权给用户（barwe）...访问某个数据库...授权完成! 我们回忆一下上面提到的规则：@ 后面应紧跟 装饰器函数 的引用。这里很明显 auth(&#39;barwe&#39;) 是调用了 auth 函数，而且它的参数是一个字符串。那么可以预见的是，auth(&#39;barwe&#39;) 应该返回一个真正的装饰器函数的引用，然后再解释 @ 符号，再调用一次装饰器函数。另外，从打印结果我们知道，装饰器函数还可以访问 auth 函数的局部变量的值 “barwe”。不难定义： 12345678910111213141516171819def auth(name): #print('start auth ...') def _auth(func): #print('start _auth ...') def wrapper(*args, **kwargs): #print('start wrapper ...') print(f'函数（&#123;func.__name__&#125;）准备授权给用户（&#123;name&#125;）...') result = func(*args, **kwargs) print('授权完成!') #print('wrapper end!') return result #print('_auth end!') return wrapper #print('auth end!') return _auth@auth('barwe')def access_database(*args, **kwargs): print("访问某个数据库...") 如果去掉上面 print 函数的注释再次执行，就能得到 1234start auth ...auth end!start _auth ..._auth end! 这表示先调用了 auth(&#39;barwe&#39;) ，再解释 @ 时又调用了 auth.&lt;locals&gt;._auth(access_database)。 但是 access_database &lt;- auth.&lt;locals&gt;._auth.&lt;locals&gt;.wrapper 并没有执行！ 我们正式执行修饰后的目标函数： 1access_database() 输出结果是 12345start wrapper ...函数（access_database）准备授权给用户（barwe）...访问某个数据库...授权完成!wrapper end! 总结一下，@ 符号会调用一次装饰器函数。如果是带参的装饰器，先执行后面的函数，然后再解释 @，这就要求后面那个函数应该返回一个标准的装饰器函数（即以目标函数为参数，返回对目标函数进行修饰后的新函数）。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在线获取UserAgent]]></title>
    <url>%2Fspider%2F20200927-204d.html</url>
    <content type="text"><![CDATA[UserAgent 让你的爬虫以（冒充）浏览器的身份向服务器请求数据。更换 UserAgent 可以避免触发某些反爬机制。fake-useragent 是用来为爬虫提供 UserAgent 的第三方包。 基本用法使用 pip install fake-useragent 安装 fake-useragent 这个包，如果网络错误，就多试几次。 使用 import fake_useragent 导入。一般地，我们只需要 from fake_useragent import UserAgent 即可。 fake-useragent 的API 十分简单： UserAgent().random 即可获取一个随机的 UserAgent（一般用这个就行了） 获取子项的值：.ie, .msie, [&#39;Internet Explore&#39;], opera, chrome, google, [&#39;google chrome&#39;], firefox, ff, safari 下面是一个典型的 UserAgent： 1Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36 其每个字段的含义是： 这些信息可以通过在线网站 http://useragentstring.com/ 进行查询。 其它信息fake-useragent 会将数据缓存在系统的临时目下，例如 Linux 的 /tmp。 可在 python 脚本中随时更新本地缓存的数据库： 123from fake_useragent import UserAgentua = UserAgent()ua.update() 如果不想将数据缓存到本地，可设置： 1ua = UserAgent(cache = False) 如果 useragentstring.com 或者 w3schools.com 改变或者关闭了它们的资源，fake-useragent 会使用备用的 heroku。 如果你不想使用托管的缓存服务器（此设置在0.1.5版本添加），可设置： 1ua = UserAgent(use_cache_server = False) 在极少数情况下，如果托管的缓存服务器和数据源都不可用，fake-useragent 将不会下载任何数据（版本0.1.3添加）。此时调用 UserAgent() 会抛出 fake_useragent.errors.FakeUserAgentError 错误，可再次尝试。 你可以自定义数据缓存的路径： 12#version = fake_useragent.VERSIONua = UserAgent(path = "/path/to/xxx.json") 如遇报错可尝试更新 fake-useragent： 1pip install -U fake-useragent 参考资料项目地址：https://github.com/hellysmile/fake-useragent]]></content>
      <categories>
        <category>spider</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2G用户怎么装SnapATAC]]></title>
    <url>%2FR%2F20200903-2dec.html</url>
    <content type="text"><![CDATA[最近升级R到4.0版本，遂重新安装分析需要的包。大部分包都很顺利，唯独这个SnapATAC，从Rstudio中不管是用devtools::install_github还是用biocManager::install都不响应，等一段时间就timeout了。经过挠头皮思考之后，找到了一种安装方法。适合访问github速度慢或者从github下载源码速度慢的小可爱。想想还是挺有意思的，遂记下来以供参考~ 首先SnapATAC为什么下载这么慢？ 看github上的源码结构我们可以发现作者其实是把代码和数据混在一起，下载数据时经常卡死。 所以解决方法很简单，我们只下源码，不下demo数据。故而，这里我们排除下载 data, examples, images 和 inst/extdata 这四个目录。怎么做呢？ 安装一个叫做 GitZip 的chrome插件，这个插件实现了从github项目中下载部分文件或者目录的功能。 扩展迷 上可下载插件：https://www.extfans.com/ 安装完成之后双击想要下载的目录或者文件名称后的空白部分标记想要下载的目录或者文件，点击右下角的下载按钮即可。 点击下载后会保存到本地，压缩包名为 SnapATAC-.zip，解压并重命名目录为 SnapATAC。 已经整理好的源码：https://barwe.lanzous.com/insiGgbf7ba 打开cmd并进入到解压目录所在的目录，执行 1R CMD build SnapATAC 这一步编译SnapATAC并生成一个压缩包 SnapATAC_x.x.x.tar.gz，其中 x.x.x 是版本号。 然后继续执行 1R CMD INSTALL SnapATAC_x.x.x.tar.gz 如果提示缺少依赖就先装好依赖~ 大功告成~]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[实用的conda换源教程]]></title>
    <url>%2Fpython%2F20200827-862f.html</url>
    <content type="text"><![CDATA[官方的默认源在下载某些包（例如pytorch）时实在是太慢了，建议在新建一个虚拟环境前就换源。 因为不同源的包的默认版本可能不一致，如果中途换源可能会导致许多不必要的麻烦。 这里推荐使用清华大学提供的conda源，亲测好使。 在用户目录（linux为/home/USER/，windows为C:\Users\USER\）下新建 .condarc 文件，并改成以下内容： 1234567891011show_channel_urls: truechannel_priority: disabledchannels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ - default windows下不允许新建没有文件名的文件（即以点号开头的文件），有以下几种解决办法： 在cmd中使用 conda config --set show_channel_urls yes 会自动生成.condarc文件 使用模拟bash环境（例如git-bash或者msys2等）新建：touch .condarc 使用cmd下载：wget https://barwe.oss-cn-shenzhen.aliyuncs.com/config/.condarc 换源后需要重启cmd或者terminal才能使用新的源。 操作源的其它命令 查看当前使用的源： 12conda config --show channelsconda config --show-sources 重置为默认源： 1conda config --remove-key channels 手动添加一个源： 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ 另外，中科大也提供了源，但是在我电脑上不咋好使，用清华源足矣。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[将SpaceSniffer添加到右键菜单]]></title>
    <url>%2Fos%2F20200818-7e4.html</url>
    <content type="text"><![CDATA[SpaceSniffer 能够以网格形式展示指定目录的结构层次以及它的子目录和子文件的大小。 这里介绍怎么把应用添加到目录的右键菜单，使得我们可以在任意目录上右键分析这个目录的磁盘空间。效果如下： 下载和安装这是一个绿色包，所以不需要安装 ~ 链接：https://pan.baidu.com/s/186OKHlF_2IJ0VsnwDhvJnQ 提取码：83vc 启动程序直接打开 SpaceSniffer.exe 即可。 需要注意的是，如果待分析的目录中包含需要管理员权限操作的子目录或者子文件，需要以管理员身份启动软件才能正常分析，否则会抛出警告。 启动后依次点击 Help &gt; command line help 可以查看命令行帮助文档： 我们的任意目录右键分析洗盘空间需要用到这个。 观察一下，scan 参数可以指定我们想要分析的目录，如果是在右键上，我们应该通过变量实时获取目标目录。 添加右键入口 首先我们需要一个默认以管理员权限启动的 cmd.exe，cmd.exe 一般位于 C:\WINDOWS\system32 下，找到它然后复制一个新的程序并重命名为 cmda.exe 右键这个 cmda.exe，依次点击 属性 &gt; 兼容性，勾选 以管理员身份运行此程序，然后 确定，记下这个 cmda.exe 的路径 C:\WINDOWS\system32\cmda.exe 打开注册表编辑器（win + r 然后输入 regedit 确定），定位到 HKEY_CLASSES_ROOT\Directory\shell，这个文件夹代表了 一般目录的右键菜单： 可以看到我这里已经添加了四个入口，分别是 cmder, find, git bash 和将要添加的 SpaceSniffer 右键 shell，新建 项，取名为 SpaceSniffer 修改 SpaceSniffer 项的 (默认) 为 使用 SpaceSniffer 分析这个目录的磁盘空间，这个字符串将出现在你的右键菜单中，可个性化修改；在 (默认) 值的同级环境中右键依次点击 新建值 &gt; 字符串值，取名为 Icon。双击 Icon，修改其值为 SpaceSniffer.exe 程序的路径；最后再新建一个 项，取名为 command。最终效果如下： 双击 command 项，修改其默认值为 “C:\WINDOWS\system32\cmda.exe” “/C start D:\Portable\spacesniffer\SpaceSniffer.exe scan %1”： /C 表示执行其它程序 start 不会显示cmd的黑黢黢的窗口 scan 路径 作为 SpaceSniffer.exe 的参数 %1 变量动态获取当前目录路径 真香 ~]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R-下载github源码并离线安装]]></title>
    <url>%2FR%2F20200804-fcaf.html</url>
    <content type="text"><![CDATA[devtools::install_github 虽然好用，但是有时候R不能与github通讯时就很尴尬。 在jupyter中使用R时需要安装IRkernel包： 1devtools::install_github('IRkernel/IRkernel') 但是网络似乎出现了问题，but浏览器中还是可以访问这个repo的： 这时候可以考虑把repo克隆到本地后再离线安装。过程如下： 下载源码： 12345678$ git clone https://github.com/IRkernel/IRkernel.gitCloning into 'IRkernel'...remote: Enumerating objects: 12, done.remote: Counting objects: 100% (12/12), done.remote: Compressing objects: 100% (11/11), done.remote: Total 2359 (delta 3), reused 4 (delta 1), pack-reused 2347Receiving objects: 100% (2359/2359), 694.02 KiB | 30.00 KiB/s, done.Resolving deltas: 100% (1401/1401), done. 编译：需要用到命令行编译，如果git bash中能够检测到R可以直接在git bash中编译和安装 12$ which R/d/Program/R/R-4.0.2/bin/R 12345678910$ R CMD build IRkernel* checking for file 'IRkernel/DESCRIPTION' ... OK* preparing 'IRkernel':* checking DESCRIPTION meta-information ... OK* checking for LF line-endings in source and make files and shell scripts* checking for empty or unneeded directoriesRemoved empty directory 'IRkernel/example-notebooks'Removed empty directory 'IRkernel/tests/testthat/jkt'Removed empty directory 'IRkernel/tests/testthat/njr'* building 'IRkernel_1.1.1.9000.tar.gz' 编译会生成一个以 .tar.gz 结尾的压缩包，安装它即可。 安装：需要用到命令行 12345678910111213141516171819202122232425262728$ R CMD INSTALL IRkernel_1.1.1.9000.tar.gz* installing to library 'D:/Program/R/R-4.0.2/library'* installing *source* package 'IRkernel' ...** using staged installation** R** inst** byte-compile and prepare package for lazy loading** help*** installing help indices converting help for package 'IRkernel' finding HTML links ... ▒▒▒▒ Comm-class html CommManager-class html IRkernel-package html comm_manager html installspec html log html main html** building package indices** testing if installed package can be loaded from temporary location*** arch - i386*** arch - x64** testing if installed package can be loaded from final location*** arch - i386*** arch - x64** testing if installed package keeps a record of temporary installation path* DONE (IRkernel)Making 'packages.html' ... ▒▒▒▒]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++标准库-fstream文件流]]></title>
    <url>%2FCpp%2F20200710-111f.html</url>
    <content type="text"><![CDATA[C++标准库 fstream 用来读写文件。 文件流C++标准库头文件 #include &lt;fstream&gt; 中包含了三个重要的文件流类： fstream：既可以读文件，也可以写文件 ifstream：只能读文件 ofstream：只能写文件 文件读写的基本步骤是： 初始化文件流对象：声明，例如 fstream f 声明了读写文件流对象f 打开文件：调用 open 方法，例如 f.open(&quot;xxx.txt&quot;, ios::in) 读/写文件：读取内容或者写入内容 关闭文件：调用 close 方法，f.close() 声明文件流对象如果只是对文件进行 读 操作，请使用 ifstream；如果只是对文件进行写操作，请使用 ofstream。语法如下： 123std::ifstream in;std::ofstream out;std::fstream in_out; 打开文件open 方法打开open 是三个文件流类的成员方法，可被文件流对象调用以打开文件，实现文件流对象与具体文件的绑定。 open 方法接收两个参数：文件路径，文件操纵模式。 123in.open("xxx.txt", std::ios::in);out.open("yyy.txt", std::ios::app);in_out.open("zzz.txt", std::ios::in | std::ios::out) 读文件流对象的模式默认为 ios::in，可以缺省。 ios标准库 内置了日常所需的文件操纵模式，包括： ios::in 打开文件用于读取 ios::out 打开文件用于写入 ios::ate 文件打开后定位到末尾 ios::app 追加模式，所有写入添加到文件末尾 ios::binary 二进制方式 ios::trunc 如果文件已存在则先删除文件 常见的文件操纵模式组合： 读取文本文件：ios::in 读取二进制文件：ios::in | ios::binary 写入为文本文件，文件存在时覆盖写入：ios::out 写入为文本文件，文件存在时追加写入：ios::out | ios::ate 或者 ios::app 写入为文本文件，文件存在时先删除再写入：ios::out | ios::trunc 写入为二进制文件，文件存在时覆盖写入：ios::out | ios::binary 关于写入文件的三种情形，假设我有一个文件内容如下，想要写入的内容是 Fuzhou 和 Guangzhou： 123NanjingShanghaiShenzhen 文件写入方式 结果 覆盖式 FuzhouGuangzhouShenzhen 追加式 NanjingShanghaiShenzhenFuzhouGuangzhou 删除式 FuzhouGuangzhou 在声明时打开除了使用 open 方法，在声明文件流对象时可以直接打开文件，语法如下： 12std::ifstream in("xxx.txt");std::ofstream out("yyy.txt", std::ios::out); 检查文件打开是否成功通过 if(in_out) 来判断文件是否打开成功。 读取文件在某些编程语言中，文件流对象打开文件后又叫做 文件句柄。文件句柄本身不存储文件的内容，但我们可以从文件句柄访问整个文件的内容或者写入内容到文件。文件句柄相当于程序和文件之间的一个通道。 文件句柄的读取方法很多。 逐个单词读取至string变量流运算符的一个特点就是从空白符后的第一个非空白字符开始，读取到下一个空白符为止。所以只能一个单词一个单词的提取。 常见的空白符有空格、制表符、换行符。 12345678910111213#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;vector&gt;using namespace std;int main()&#123; int x; vector&lt;int&gt; vec; ifstream in("data.txt"); if (in) while (in &gt;&gt; x) vec.push_back(x); for (int x : vec) cout&lt;&lt;x&lt;&lt;" "; return 0;&#125; 逐行读取至string变量getline 函数可以按行（换行符分割）读取文件，并将每行的内容保存在字符串类型的变量var里面： 1while (getline(文件句柄fh, 迭代变量var)) &#123; ... &#125; 🐖：var中的行字符串不包含末尾的换行符。 这个getline函数从哪里来的？ getline函数用于从 ifstream 中按行读取文件内容，ifstream 派生自 istream，&lt;istream&gt; 头文件中就定义了这个 getline 函数。 逐行读取至char数组此外，文件输入流本身也提供了一个 getline 方法： 123const int LINE_LENGTH = 100;char var[100];while (in.getline(var, LINE_LENGTH)) &#123; ... &#125; 这个 getline 方法就显得笨重了许多，我们需要先预设一个 LINE_LENGTH，这个值要比文件最长的那一行的长度还要长（假设最长行的长度为99，则这个值最小应该设置为100）。然后我们需要声明一个迭代变量var，它是一个长为 LINE_LENGTH 的 字符数组。 getline 方法的第一个参数是迭代变量，第二个参数是 LINE_LENGTH。 所以这个 getline 方法实际上是将文件的每一行读进了一个等长的字符数组里面。 写入文件流运算符流运算符可以用于写入文本文件，用于二进制文件可能会产生错误。 12ofstream out("output.txt");out &lt;&lt; setw(20) &lt;&lt; setfill('#') &lt;&lt; setiosflags(ios::left) &lt;&lt; "name: " &lt;&lt; "barwe" &lt;&lt; endl; 关闭文件尽管程序结束时会自动关闭打开的文件，但为了安全起见，建议手动关闭。 123in.close();out.close();in_out.close();]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++标准库索引]]></title>
    <url>%2FCpp%2F20200710-e3c6.html</url>
    <content type="text"><![CDATA[C++标准库总共51个头文件，按照内容可分为10个类，按照来源可分为3个类。 51 C++ （20） C (18) STL (13) 语言支持 (11) limits：C++数值类型特性new：动态内存管理typeinfo：运行时内存信息exception：异常处理 cstddef：C标准定义climits：整型大小cfloat：浮点型特性cstdlib：C标准实用工具cstdarg：可变参数csetjmp：非局部跳转csignal：C中断处理 - 输入输出 (10) iostream：标准输入输出流istream：标准输入流ostream：标准输出流sstream：字符串流fstream：文件流iomanip：输入输出操纵器ios：iostream的基类iosfwd：输入输出前向声明streambuf：流缓存 cstdio：C标准输入输出 - 诊断功能 (3) stdexcept：异常类 cassert：C断言验证cerrno：C出错码 - 通用工具(4) - ctime：C时间日期 utility：实用元件functional：函数对象memory：内存管理器 字符串 (6) string：字符串类strstream：C字符串流类 cstring：C字符串cctype：单字节字符类型cwctype：多字节字符类型cwchar：扩展多字节宽字符 - 容器 (8) bitset：位集 - vector：向量list：列表queue：队列deque：双队stack：堆栈map：映射set：集合 迭代器 (1) - - iterator：迭代器 算法 (2) - cios646：ISO646字符集替换 algorithm：算法 数值操作 (4) complex：复数valarray：数值矢量 cmath：C数学库 numeric：数学运算 本地化 (2) locale：本地化 clocale：C本地化 -]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OD的基本使用2 - demo修改]]></title>
    <url>%2Fcracking%2F20191130-9c79.html</url>
    <content type="text"><![CDATA[ODBG110: https://pan.baidu.com/s/1VklVhE7wH7qrEW2OOLElqw 提取码：uuhgODBG110BasicPlugins: https://pan.baidu.com/s/1QG0ealpvKTTtjd26U9himQ 提取码：6fymODBG Very More Plugins: https://pan.baidu.com/s/1VQoodK3ArkiGYzJazUToiw 提取码：daoo[教程]使用OllyDbg从零开始Cracking: https://pan.baidu.com/s/1l094huv-bQvka-QOhgNhhg 提取码：65jv 了解软件我们直接运行打开CRACKME.EXE文件进行注册，随机输入用户名和注册码，点击注册，软件报错，序列号不对，好吧，这才是正常的逻辑。现在尝试着修改软件，使我们输入任意的用户名和序列号，均通过验证。 程序弹出验证错误的消息框，调用的是系统的 MessageBoxA 方法，其它软件应该也是类似的吧。 消息框的标题是“No luck!”，文本是“No luck there, mate!”。 以上这是我们已知的重要信息，也是我们破解的关键地方。 开始破解使用OD打开CRACKME.EXE，界面如下： 我们通过插件Command Bar（位于OD最下方）对消息弹窗函数 MessageBoxA() 加断点，断点的位置位于该函数的第一个指令处，这意味着如果程序遇到调用该函数的指令，会在跳转到该函数所在的内存区域后停留在第一个指令处。 加断点的命令为：bp MessageBoxA 加断点后点击B按钮查看断点标记情况，可以看到背景自动变灰的哪一行就是我们上一步加断点的地方。 我们可以看出，MessageBoxA() 函数的一些基本信息： 该函数存储的位置为 750F8700 到某个未知位置 该函数的代码位于 USER32 模块中 该函数的第一个指令是 MOV EDI, EDI 我们在该条目上右键选择 Follw in Disassembler 或者直接按回车键进入该函数存储的指令区域。 如下图，指令区域跳转至 750F8700 处并自动高亮显示。第一列地址处红色高亮，说明此处添加了断点。 我们往下数直到第一个 RETN 出现的区域为 MessageBoxA() 函数的内存区域。 如果我们此时执行程序，程序会运行到端点处暂停，因为没有实际执行 MessageBoxA() 函数，所以不会有注册成功或者注册失败的消息弹出。 第一次按下 F9 会跳转到 00401000，这个位置是程序执行的入口。 再次按下 F9 运行程序。 此时弹出了一个笑脸窗口，这个就是模拟软件的主窗口。 同时我们观察 EIP = 777BA8FC，说明程序并未执行到我们设置的断点（750F8700）处，为什么呢？因为执行 MessageBoxA() 函数的前提是进行注册，而注册是用户手动完成的。因此我们需要在模拟软件中手动注册。 点击 Help 按钮选择注册，这里我们特定随机输入用户名“123qwe”，密码为“asdzxc”，点击 OK。 接着OD中程序暂停在我们设置的断点上，注意此时 EIP = 750F8700。 观察堆栈的信息： 当前栈顶位置为 0019FDB8，存储在寄存器 ESP 中 当前子程序执行完之后会跳回到 004013C1 程序是从 004013BC 跳转到当前断点位置的，004013BC 处进行了 MessageBoxA() 函数的调用（CALL） 在调用 004013BC 处的 MessageBoxA() 函数之前，该函数的参数也被压入栈中。调用该函数提供了四个参数，分别是 Style, Title, Text, hOwner。观察它们的值可以发现，调用这个函数将会告诉用户注册失败。 我们在栈顶位置右键选择 Follw in Disassembler 或者直接按回车打开 004013BC 处调用 MessageBoxA() 函数的位置 指令区域显示的并不是 004013BC 处调用 MessageBoxA() 函数的位置，而是调用结束后跳转继续执行的位置 004013C1 滑动滚轮或者拉动滑条，观察 004013C1 上面的几行命令 004013C1 处调用了 MessageBoxA() 函数，该函数需要的参数在 004013AD ~ 004013B9。 可以预见的是，程序会在这里调用 MessageBoxA() 函数，对我们输入的注册码做出回应，并且告诉我们很遗憾。 那么是什么导致程序会选择在这里调用 MessageBoxA() 函数呢？ 因为指令都是按顺序执行的，我们去 004013C1 上面找寻答案。 因为 004013AD ~ 004013BC 是在定义 MessageBoxA() 函数的参数和调用，我们点击它们上面的 POP ESI，哎，我们在下面的信息栏中发现了一条重要信息：“Jump from 0040138B”，说明 POP ESI 指令从 0040138B 跳转而来，并且之后继续执行了 MessageBoxA() 函数。 我们跳转到 0040138B 位置，查看附近的指令： 0040138B 是一条 JB 跳转指令，而且在它的上面有这么一条 CMP AL, 41 指令。这表示：当前仅当寄存器 AL 的值 小于 41 时才会跳转到 004013AC 处执行 POP ESI 和调用 MessageBoxA() 函数。 那如果不管 CMP AL, 41 的结果如何，下面都不进行跳转是不是就不会弹出让人遗憾的消息框了？ 这里我们将 CMP AL, 41 篡改成 CMP AL, AL ，这使得 JB 跳转永远不会执行。 双击 CMP AL, 41 指令即可修改。修改后结果如下： 第二列变为红色字体，表示改行经过修改；指令也已经发生变化。 按下 F9 执行程序，弹出消息框： 呀，还是注册不通过~ 按下“确定”按钮，OD中程序中又暂停在了 750F8700 的位置，是不是有点眼熟这个位置？ 唯一不同的在这个地方： 这表示有两个不同的地方调用了 MessageBoxA() 函数，一个是 004013BC，一个是 00401378。 前者我们已经解决了，后者是新出现的。 这表示有两个甚至多个验证方法检查我们的注册信息，而且明显 004013BC 处的检查排在 00401378 前面。 下面跳转到 00401378 附近： 我们逐级向上检查，发现在 00401362 的 PUSH 0 处出现了 “Local call from 00401245”，接着调用了另一个USER32库中的函数 MessageBeep，然后就调用了即将给我们抛出遗憾信息的 MessageBoxA 函数。 我们跳转到 00401245： 这里首先比较了 EAX 和 EBX 的值，然后根据比较结果进行跳转： 如果相等，JE 指令被执行，跳转到 0040124C 执行 CALL ~.0040134D 如果不等，JE 指令不执行，直接执行 CALL ~.00401362 不管怎样，最终都会跳转到 004011E6 我们知道，如果 EAX 和 EBX 的值如果不相等，程序就会跳转到 00401362 继续执行，从而调用 MessageBoxA 函数使用户感到遗憾。那么如果 EAX 和 EBX 的值如果相等会怎么样？0040134D 又是什么东西？我们定位到 0040134D 瞅一下： 我们可以看到，0040134D ~ 0040135C 区域依然在单纯地调用 MessageBoxA 函数，只是函数参数变得善良了，也就是说，这块区域只有在我们的注册信息有效的前提下才会被调用。而下面的 00401362 ~ 00401378 区域恰好是我们我们注册信息不通过会被执行的指令。 回到 00401245： 如果我们使 00401241 处的 CMP 结果恒成立会怎么样？例如 CMP EAX, EAX。 那么此时，不管我们怎么输注册信息，都只会跳转 0040134D 执行注册成功的信息。 修改后的结果如下： 这个模拟程序对注册信息进行了两次验证，我们也进行了两次修改，从而屏蔽掉了有效性检查。 保存成果修改完成后在指令窗口的任意位置右键选择 Copy to executable 中的 All modifications： 选择 Copy all: 弹出了一个新的指令集页面： 该页面内任意位置右键选择 Save file： 重命名例如 CRACKME2.EXE，保存即可 关闭OD，验证破解成果。]]></content>
      <categories>
        <category>cracking</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[汇编 - 标志位和跳转]]></title>
    <url>%2Fcracking%2F20191130-ddcd.html</url>
    <content type="text"><![CDATA[跳转指令 大部分是基于 标志位 的值进行的，也有基于两个数的比较进行的。 标志位进位标志C进位/借位标志CF（Carry Flag）：当运算结果的最高有效位出现进位或者借位的时候，进位/借位标志被设为1，否则为0。 我们考虑BB+6A=125，我们说它出现了进位，我们这么认为是因为我们假设了操作数的有效位为2，而计算结果的有效位为3，即两个最高只有十位的数相加得到了最高位数为百位的结果，按照这种理解49+6D=B6没有发生进位。相应的，10-9=7发生了借位，因为操作数最高位为十位，计算结果最高位位各位。但是，在OD中，每个数用4个字节（1字节=8bits）表示，取值范围从 00 00 00 00 ~ FF FF FF FF，操作数最高位为第8位，在这种情况下所谓的发生进位的BB+6A完整的写法是00 00 00 BB + 00 00 00 6A = 00 00 01 25，此时最高位（第8位）并未发生进位。另一个例子,FF FF FF FF + 00 00 00 01就会在第9位产生1，即发生了进位，这已经超过了寄存器允许的最大值，去除溢出的部分，计算结果为00 00 00 00。借位类似，例如00 00 00 02 - 00 00 00 03 = FF FF FF FF。 奇偶标志P奇偶标志位PF（Parity Flag）：当运算结果最低字节的二进制数中1的个数为偶数时PF=1，为奇数时PF=0。 为什么是最低字节？没太搞懂，在OD中： ADD 8930, 0的结果显示P=1。8930H的二进制为1000 1001 0011 0000B，这个数中最低字节有两个1，总共五个1 ADD 8931, 0的结果显示P=0。尽管总共六个1，但是最低字节有3个1 零标志Z零标志ZF（Zero Flag）：当计算结果为0时Z=1，否为为0。零标志常用来判断两个数是否相等。 符号标志S符号标志SF（Sign Flag）：我们通常说的有符号数指的是负数。当计算结果为负数时S=1，否则S=0。 四字节数的取值范围是 00 00 00 00 ~ FF FF FF FF，但是实际的取值范围还取决于该四字节数表示的有符号数还是无符号数： 如果是无符号数：四字节数的表面值就等于实际值； 如果是有符号数：在有限的取值范围下，为了保证正数和负数数目相同，规定 00 00 00 00 ~ 7F FF FF FF 表示相当于十进制的 0 ~ 21,4748,3647，而 80 00 00 00 ~ FF FF FF FF 表示相当于十进制中的 -21,4748,3648 ~ -1。 溢出标志O溢出标志O（Overflow Flag）：只对有符号数的运算，在范围正中间附近（即7F FF FF FF ~ 80 00 00 00）时，计算结果的符号发生改变，则称之为溢出，此时O=1。计算结果的期望值与实际值不相等，是的计算结果发生错误。 例如 7F FF FF FF + 1 = 80 00 00 00 会形成溢出。 换句话说，在OD中，如果计算结果的值超出了[-80000000H, 7FFFFFFFH]的范围就属于溢出。 其它标志位: A T D跳转指令洒家总结了以下常见的跳转指令，大致可以分为4类： 检查标志位进行跳转 检查两个操作数进行跳转 比较两个无符号数进行跳转 比较两个有符号数进行跳转 基于标志位常见的标志为 C(进位/借位)，P(奇偶), Z(零), S(符号), O(溢出)，标志位的取值有0和1两种。 标志位值为1时使用J前缀进行跳转，值为0时使用JN前缀进行跳转。 Description Flag J- 跳转1 JN- 跳转0 低字节中1的数目为偶数？Parity？ -P JP JNP 计算结果为0？Zero？ -Z JZ JNZ 有符号？是负数？Zero？ -S JS JNS 溢出？Overflow？ -O JO JNO 进位/借位？Carry？ -C JC JNC 比较两个操作数比较两个普通操作数比较两个操作数使用CMP leftOp, rightOp，比较结果保存至标志位中，然后使用JE或者JNE进行跳转。 JE和JNE根据标志位Z的结果判定是否需要跳转。JE和JZ、JNE和JNE实际上是等效的，都是以标志位Z为依据进行跳转。它们的不同在于直观理解上： 后缀E（Equal）倾向于判断两个操作数是否相同 后缀Z（Zero）倾向于检查标志位Z的值 在不同的场景中选择不同的语法是个很好的习惯。 检查寄存器值是否为0检查寄存器的值是否为0，当然可以使用CMP，如CMP EAX, 0，但是更一般的，我们为寄存器设计了专用的指令。 指令结构为：J + [寄存器名称] + Z (表示Zero)，例如： JECXZ 表示当ECX=0时跳转 JCXZ 表示当CX=0时跳转 其它的寄存器检查类似。 比较两个无符号数跳转前缀只使用 J，跳转后缀使用 B/E/A 系统 。 当第一个操作数 小于 第二个操作数时，记为 B=1，意为 Below。当第一个操作数 等于 第二个操作数时，记为 E=1，意为 Equal；当第一个操作数 大于 第二个操作数时，记为 A=1，意为 Above。当然上述记法并不代表存在这样一个标志位B，E，A。 假设第一个操作数为left，第二个操作数为right，则 跳转条件 正面描述 指令 等价的反面描述 指令 （可以但没必要） $\text{left} &lt; \text{right}$ 小于 JB 不大于等于 JNAE $\text{left} \le \text{right}$ 小于等于 JBE 不大于 JNA $\text{left} = \text{right}$ 等于 JE 既不大于也不小于 JNAB （这个指令不存在） $\text{left} &gt; \text{right}$ 大于 JA 不小于等于 JNBE $\text{left} \ge \text{right}$ 大于等于 JAE 不小于 JNB 比较两个有符号数与无符号数比较类似，只是后缀采用 L/E/G 系统，分别代表 Less / Equal / Greater。 跳转条件 正面描述 指令 等价的反面描述 指令 （可以但没必要） $\text{left} &lt; \text{right}$ 小于 JL 不大于等于 JNGE $\text{left} \le \text{right}$ 小于等于 JLE 不大于 JNG $\text{left} = \text{right}$ 等于 JE 既不大于也不小于 JNGL （这个指令不存在） $\text{left} &gt; \text{right}$ 大于 JG 不小于等于 JNLE $\text{left} \ge \text{right}$ 大于等于 JGE 不小于 JNL]]></content>
      <categories>
        <category>cracking</category>
      </categories>
      <tags>
        <tag>汇编</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[请求库2 - requests]]></title>
    <url>%2Fspider%2F20191128-908f.html</url>
    <content type="text"><![CDATA[urllib是基本库，功能实现尚且比较复杂。requests库更加友好、高效。 requests能更加简单的实现Cookies、登录验证和代理设置等额外操作。 基本用法基本的GET请求实例1：抓取知乎专题的标题 文本内容通过response.text属性查询。 URL参数如果get访问的服务器需要参数需要在URL中加入参数，常见的方式有两种： 直接构建带参数的URL字符串，通过问号分隔，例如 http://xxx.com/?key=value 通过param参数传递URL参数，该参数接受一个字典，如 response = requests.get(..., param = {&#39;k&#39;: &#39;v&#39;}) 可以是简单的字典，如 {&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;}构造出的参数字符串为 ?key1=value1&amp;key2=value2 可以有多个值，如 {&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: [&quot;key21&quot;, &quot;key22&quot;]}构造出的参数字符串为?key1=value1&amp;key2=value21&amp;key2=value22 可以是空值，此时会忽略这个参数，如 {&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: None}构造出的参数字符串为 ?key1=value1 实例2：抓取json数据json数据本质上还是文本数据，我们抓取后需要转化为字典/json格式，为避免这一转化步骤，可直接通过response.json()方法获取json字典 实例3：抓取GitHub站点图标 二进制内容通过response.content获取，直接打印bytes类型的字符串是没有视觉意义的，因此需要保存到本地。注意文件打开格式为wb。 图片、视频、音频等二进制多媒体文件都用这种方式获取。 基本的POST请求POST请求需要提交表单（form），表单数据在python中以字典表示，通过requests.post()函数的data参数传入。 在响应结果中我们可以看到请求的数据存储在form字段中。 响应（response）我们通过requests.get()或者requests.post()向服务器发送一个请求（request），服务器给我们一个返回结果称之为响应（response）。 响应结果中包含了许多的信息，主要如下： text：str类型的请求内容 json() ：如果返回内容本质上是json数据，直接调用这个 content：bytes类型的请求内容 cookies：类型为requests.cookies.RequestsCookieJar headers：字典，requests.structures.CaseInsensitiveDict类型 ok：bool类型，请求是否正常返回结果 status_code：int类型，状态码，正常为200 encoding：编码方式，常见的如utf-8, gbk url：请求的url 状态码requests内置了一个状态码查询对象requests.codes 例如正常返回200对应的状态码对象为requests.codes.ok 下面是常见的状态码和对应的状态码对象： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 信息性状态码100: ('continue',),101: ('switching_protocols',),102: ('processing',),103: ('checkpoint',),122: ('uri_too_long', 'request_uri_too_long'),# 成功状态码200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),201: ('created',),202: ('accepted',),203: ('non_authoritative_info', 'non_authoritative_information'),204: ('no_content',),205: ('reset_content', 'reset'),206: ('partial_content', 'partial'),207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),208: ('already_reported',),226: ('im_used',),# 重定向状态码300: ('multiple_choices',),301: ('moved_permanently', 'moved', '\\o-'),302: ('found',),303: ('see_other', 'other'),304: ('not_modified',),305: ('use_proxy',),306: ('switch_proxy',),307: ('temporary_redirect', 'temporary_moved', 'temporary'),308: ('permanent_redirect', 'resume_incomplete', 'resume',), # These 2 to be removed in 3.0# 客户端错误状态码400: ('bad_request', 'bad'),401: ('unauthorized',),402: ('payment_required', 'payment'),403: ('forbidden',),404: ('not_found', '-o-'),405: ('method_not_allowed', 'not_allowed'),406: ('not_acceptable',),407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),408: ('request_timeout', 'timeout'),409: ('conflict',),410: ('gone',),411: ('length_required',),412: ('precondition_failed', 'precondition'),413: ('request_entity_too_large',),414: ('request_uri_too_large',),415: ('unsupported_media_type', 'unsupported_media', 'media_type'),416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),417: ('expectation_failed',),418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),421: ('misdirected_request',),422: ('unprocessable_entity', 'unprocessable'),423: ('locked',),424: ('failed_dependency', 'dependency'),425: ('unordered_collection', 'unordered'),426: ('upgrade_required', 'upgrade'),428: ('precondition_required', 'precondition'),429: ('too_many_requests', 'too_many'),431: ('header_fields_too_large', 'fields_too_large'),444: ('no_response', 'none'),449: ('retry_with', 'retry'),450: ('blocked_by_windows_parental_controls', 'parental_controls'),451: ('unavailable_for_legal_reasons', 'legal_reasons'),499: ('client_closed_request',),# 服务端错误状态码500: ('internal_server_error', 'server_error', '/o\\', '✗'),501: ('not_implemented',),502: ('bad_gateway',),503: ('service_unavailable', 'unavailable'),504: ('gateway_timeout',),505: ('http_version_not_supported', 'http_version'),506: ('variant_also_negotiates',),507: ('insufficient_storage',),509: ('bandwidth_limit_exceeded', 'bandwidth'),510: ('not_extended',),511: ('network_authentication_required', 'network_auth', 'network_authentication') 高级用法会话维持这里涉及到一个 Session 的概念。不管是调用get还是调用post或者其它请求方法，requests都会默认给我们建立一个Session，当请求完成自动·关闭Session。一个Session就相当于打开了一个浏览器，我们打开浏览器，点击某个页面，看到了信息，然后关闭浏览器。 如果我们连续两次发出请求，其实是相当于打开了两个浏览器窗口。两个窗口之间的信息是不互通的，例如我在一个浏览器中登录了，想在第二个浏览器中获取登陆后的个人信息是做不到的，除非我们手动在第二个浏览器的请求中添加第一次登录的Cookie信息。 如果我们是在同一个浏览器中打开了两个标签页，情况就不一样了。第一个标签页中登录，第二个标签页也处于登录状态。 Session方式通过requests.Session()初始化，返回一个requests.session.Session对象。 Session通常用来打开同一个站点的不同页面。 POST上传文件 requests.post()的参数files接受一个字典类型的数据，字典每个元素标识了相应的文件 注意传入的字典包含的是 文件对象，而不是文件名字符串。 Cookies获取Cookies信息存储在response.cookies中，数据类型为 requests.cookies.RequestsCookieJar。 response.cookies.items()返回一个 元组列表。 Cookies可以用来维持 登录状态: 123456headers = &#123; 'Cookie': '...', 'User-Agent': 'Chrome/53.0.2785.116'&#125;url = 'https://www.zhihu.com'response = requests.get(url, headers=headers) SSL证书验证原文所举的12306的例子好像已经被修复了 12response = requests.get('https://www.12306.cn', verify=True)response.status_code # 200 如果设置varify = True(默认值)，当我们请求一个HTTPS的站点需要验证SSL证书，如果证书无效（不存在或者不合法），程序抛出requests.exceptions.SSLError。 不验证SSL证书需要手动设置varify = False，可能会抛出 警告：建议设置证书。 忽略警告可以使用： requests.packages.urllib3.disable_warnings() import logging; logging.captureWarnings(True) 设置代理 对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的IP，导致一定时间段内无法访问。 通过设置 代理 可以解决上述问题，设置代理通过参数proxies实现。 HTTP代理简单的参数用于指定HTTP/HTTPS代理，例如 12proxies = &#123;"proxy_name": "http://host:port"&#125; # 模板地址response = requests.get(url, proxies = proxies) HTTP代理需要身份验证时 1proxies = &#123;"proxy_name": "http://user:passwd@host:port"&#125; SOCKS代理安装socks库：pip install requests[socks] 设置代理：proxies = {&#39;http&#39;: &#39;socks5://user:passwd@host:port&#39;} 超时 在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到timeout参数。 这个时间的计算是发出请求到服务器返回响应的时间。 请求时间分为两部分：连接（connect）和读取（read）。因此timeout参数的值可以是标量，也可以是元组。 身份认证在访问网站时我们可能需要提供用户名和密码才能继续访问。 HTTP认证requests自带身份认证模块requests.auth.HHTPBasicAuth。 带身份认证的请求通过参数auth实现。 12345from requests.auth import HTTPBasicAuth##1.完整形式response = requests.get(url, auth=HTTPBasicAuth('user', 'passwd'))##2.简写形式response = requests.get(url, auth=('user', 'passwd')) OAuth认证安装第三方包：pip install requests_oauthlib 123from requests_oauthlib import OAuth1auth = OAuth1('app_key', 'app_secret', 'user_oauth_token', 'user_oauth_token_secret')response = requests.get(url, auth = auth) 对请求进行抽象将 请求 抽象为 Requests类。 1from requests import Request, Session 初始化Request 1234url = '...'data = '&#123;...&#125;'headers = '&#123;...&#125;'request = Request('POST', url, data = data, headers = headers) 转换request对象 123session = Session()prepared_request = session.prepare_request(request)response = session.send(prepared_request) # not post or get 将 请求 独立成本地的对象，需要的时候才在 会话 中获取结果。 这在队列调度中十分有效。 使用chrome可以通过访问 chrome://version 查询到用户代理信息。 本文参考：https://cuiqingcai.com/5514.html]]></content>
      <categories>
        <category>spider</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[请求库1 - urllib]]></title>
    <url>%2Fspider%2F20191127-ae0b.html</url>
    <content type="text"><![CDATA[urllib库是Python内置的HTTP请求库，包含4个模块： request：用来模拟发送请求的基本HTTP请求模块 error：此模块定义了请求过程中可能出现的异常，高效的爬虫需要捕获并处理这些可能出现的异常以防止程序意外终止 parse：处理URL的工具 robotparser：不常用，识别网站的robots.txt文件，判定网站是否可爬 发送请求request.urlopenurllib.request模块可以模拟浏览器发送请求，提供了最基本的HTTP请求方法。 向服务器发送一个请求并获得响应（Response）：1response = urllib.request.urlopen("https://www.python.org") 取得的响应是一个http.client.HTTPResponse对象，它包含了与请求结果相关的各种属性和方法： 超文本解码：response.read().decode(&#39;utf-8&#39;) 请求状态码：response.status，200表示请求成功 头部信息：response.getheaders()，元组列表形式 某项头部信息：response.getheader(&#39;Server&#39;)，响应头中的Server值。例如如果Server值为”nginx”就代表服务器是由Nginx搭建的 urllib.request.urlopen()的API如下：1urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 除了给定基本的URL之外，还可以传递其它数据： data参数：可选，用于构造POST请求。data必须是bytes类型/bytes类型的可迭代对象/文件对象，不能是str类型。下面这种方式可以构造一个有效的data参数值： 12345678args = &#123;'word': 'hello'&#125; # dictargs = urllib.parse.urlencode(args) # str: Unicodedata = bytes(args, encoding='utf-8') # bytes: UTF-8response = urllib.request.urlopen('http://httpbin.org/post', data=data)# response.getheader('Content-Type') =&gt; application/json# response.read().decode('utf-8')是一个json字符串# 我们构造的data参数存储在"form"字段中# 表示我们提交的是表单请求 timeout参数：可选，超时时间，单位为秒。超时未响应的程序会抛出urllib.error.URLError错误（其实叫异常是不是更好~） context参数：可选，用来指定SSL设置，必须是ssl.SSLContext类型 cafile参数：可选，CA证书名称，HTTPS请求需要 capath参数：可选，CA证书路径，HTTPS请求需要 request.Requesturlopen只能实现最基本的请求发起，参数过于简单，不能加入headers等信息。构建完整的请求可以使用Request类。我们通过各种丰富的参数实例化Request类，然后再调用urlopen进行请求：123request = urllib.request.Request('https://python.org')response = urllib.request.urlopen(request)html = response.read().decode('utf-8') urllib.request.Request对象的构造方法如下：1class urllib.request.Request(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None) data：bytes类型，同urlopen中的data参数相似 headers：dict类型，请求头；或者通过.add_header()方法添加。常用的请求头为：{&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36&quot;} origin_req_host：请求方的host地址（ip地址） unverifiable：是否是无法验证的请求，默认为False，表示用胡可以获取请求的结果 method：请求的方法（GET/POST/PUT/…） 一个使用Request对象获取请求的例子：1234567891011url = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', 'Host': 'httpbin.org'&#125;data_dict = &#123;'name': 'Germey'&#125;data = bytes(urllib.parse.urlencode(data_dict), encoding = 'utf-8')request = urllib.request.Request(url, data = data, headers = headers, method = 'POST')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) request.Handlerurlopen只能进行最基础的请求，Request类实现了在请求中添加请求头，但它们都不能解决Cookies处理和代理设置等更加高级的问题，此时就轮到Handler工具登场了。 基于urllib.request.Handler基类继承实现的高级Handler主要有： HTTPDefaultErrorHandler：处理HTTP响应错误 HTTPRedirectHandler：处理重定向 HTTPCookieProcessor：处理Cookies ProxyHandler：设置代理 HTTPPasswordMgr：管理密码 HTTPBasicAuthHandler：管理认证 其它的Handler类 request.OpenerDirector在之前的例子中，urllib通过urlopen函数为我们提供了一个隐藏的OpenerDirector对象（或者叫Opener）。 Opener类相比于urlopen函数和Request类能实现更多的功能，它是更加底层的工具。 Opener对象通过Handler来构建。 opener.addheaders = [(&#39;user-Agent&#39;, &#39;...)]可设置请求头 [Python3网络爬虫开发实战] 3.1.1-发送请求 异常处理合理的异常处理使得程序更加稳健。 urllib.request产生的异常由urllib.error模块处理。 URLError (base)urllib.error.URLError是urllib.error模块中的基类，下面提到的所有异常都是继承自URLError。 所有的urllib.request产生的异常都可以通过URLError进行捕获，如果记不住其它更精细的异常类的话用这个准没错。 URLError异常有个很常用的属性叫做reason，帮助我们快速判断出错原因。 reason描述了错误的原因，但它不一定是字符串，也可能是个对象。例如超时请求会返回一个socket.timeout对象，因此使用异常的reason的值时最好进行类型检查。 HTTPError处理HTTP请求错误，常用的三个属性： code：状态码 reason：错误原因 headers：请求头 其它 [Python3网络爬虫开发实战] 3.1.2-处理异常 URL处理urllib.parse模块用来处理URL（部分），支持多种协议。 常用方法如下： urlparse: 将完整的URL字符串拆分为一个六元素元组。URL的标准格式为：scheme://netloc/path;parameters?query#fragment urlunparse: 将一个长度为6的可迭代对象（元组、列表等）转化为一个完整的URL字符串 urlsplit: 和urlparse()十分相似，只是把params部分合并到了path中 urlunsplit: 和urlunparse()十分相似，处理对象为5元素的可迭代对象 urljoin: 只能填补、合并 urlencode：将参数字典序列化为GET请求的参数字符串 parse_qs：功能与urlencode相反，将参数字符串转化为字典 parse_qsl：功能与urlencode相反，将参数字符串转化为元组列表 quote：对字符串进行URL编码，例如中文会被转化为编码 unquote：对编码的字符串进行URL解码，例如中文编码被解码回中文 其它 分析Robots协议Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。 它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。]]></content>
      <categories>
        <category>spider</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>urllib</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asyncio与异步协程爬虫]]></title>
    <url>%2Fspider%2F20191126-af0c.html</url>
    <content type="text"><![CDATA[关于异步协程异步协程往往对IO密集型任务非常有效，如网络IO、磁盘IO，典型的应用场景是提升爬虫效率。 不适用任何并行或者异步的爬虫往往因为服务器响应过慢而将大量的时间花费在IO等待上。 当需要获取多个url的内容时，怎么url内容获取的累计时间？这里主要有两个思路。一是多进程并行，这种方法的效率主要取决于机器CPU的核数，一般情况下CPU核数有限，而且单个CPU在等待响应的过程中仍然处于空闲状态。二是异步协程，其设计思想为：在执行某个子程序如果遇到阻塞就将该子程序挂起转而执行其它的子程序到其完成或者挂起为止，这样在多个子程序间进行切换，从而充分利用处理器。在爬虫设计中，多个url的内容获取实际上是相互独立的，当程序处理一个url遇到阻塞时可以将其挂起等待服务器响应转而处理其它的url，通过队列访问的方式不断循环处理任务。当被挂起的子程序处理完成后自动添加到任务队列末尾等待轮执；如果所有任务都处于挂起状态则程序进入阻塞状态直到新的可执行任务出现。 Python3.5以后添加了async/await关键字用以实现异步协程，使用它们需要导入包asyncio: async：定义一个异步函数（协程）或者异步生成器 await：当子程序阻塞时挂起子程序（任务） 一些基本概念： 阻塞：程序在等待某个操作完成而不能去干别的事情，即程序处于挂起状态，称该程序处于 阻塞状态。常见的阻塞形式有：等待网络I/O，等待磁盘I/O，等待用户输入，CPU切换上下文等； 非阻塞：非阻塞是相对于阻塞而言的：如果程序包含多个独立的子程序，当其中一个子程序完成或者阻塞时可以转而执行其它的子程序，从而避免因为毫无作为的等待而造成的效率低下； 同步：为了完成某个任务，不同程序间需要相互协商、相互影响，则这些程序是同步的；同步通常通过“锁”来实现 异步：如果在完成某个任务时，程序之间不需要相互通信而各自独立运行，则这些程序是异步的 异步协程中的基本概念： 事件循环（event loop）：我们需要向系统申请一个事件循环对象运行我们自己的协程对象，事件循环帮助我们自动执行、挂起协程子程序。 123loop = asyncio.get_event_loop()loop# &lt;_WindowsSelectorEventLoop running=False closed=False debug=False&gt; 协程（Coroutine）：协程对象类型，需要被注册到事件循环中才能被循环调用，通过async关键字定义，直接调用不会被立即执行，而是返回一个协程对象。 任务（Task）：任务是对协程对象的封装，包含了更多的信息。 future 异步协程爬虫下面以一个例子说明异步协程爬虫的基本语法。 模拟慢速服务器123456789101112from flask import Flaskimport time app = Flask(__name__) @app.route('/')def index(): time.sleep(3) return 'Hello!' if __name__ == '__main__': app.run(threaded=True) 这里我们定义了一个 Flask 服务，主入口是 index() 方法，方法里面先调用了 sleep() 方法休眠 3 秒，然后接着再返回结果，也就是说，每次请求这个接口至少要耗时 3 秒，这样我们就模拟了一个慢速的服务接口。 注意这里服务启动的时候，run() 方法加了一个参数 threaded，这表明 Flask 启动了多线程模式，不然默认是只有一个线程的。如果不开启多线程模式，同一时刻遇到多个请求的时候，只能顺次处理，这样即使我们使用协程异步请求了这个服务，也只能一个一个排队等待，瓶颈就会出现在服务端。所以，多线程模式是有必要打开的。 启动之后，Flask 应该默认会在 127.0.0.1:5000 上运行，运行之后控制台输出结果如下： 1Running on http:*//127.0.0.1:5000/ (Press CTRL+C to quit)* 定义协程123456789101112131415import asyncio # 异步I/Oimport aiohttp # 实现了异步的网络通信工具，替换requestsasync def get(url): session = aiohttp.ClientSession() response = await session.get(url) result = await response.text() session.close() return resultasync def request(): url = 'http://127.0.0.1:5000' print('Waiting for', url) result = await get(url) print('Get response from %s Result: %s'%(url, result)) asyncio中实现了定义和执行协程的关键字和方法，aiohttp中实现了支持异步操作的网络通信，requests不支持异步操作。 异步函数request()定义了一个协程，如果事件循环执行到本协程的await get(url)语句时遇到阻塞就会将此协程挂起转而执行任务队列中的其它任务。当本协程挂起时实际上是在等待get(url)的结果，get(url)本身也是一个协程，它返回一个可等待对象（Awaitable object）。 get(url)同样是一个协程，如果事件循环执行到await session.get(url)时遇到阻塞，该任务被挂起等待服务器响应，当下次轮执此协程时会再次遇到await response.text()，如果这个子程序没有遇到执行阻塞，就不会挂起继续向下执行。 总结一下： async定义协程，await当任务阻塞时挂起任务，await只能用于async定义的协程里面 await关键字修饰的必须是可等待对象。哪些是可等待对象？ 异步函数定义的原生协程对象 异步生成器 实现了__await__()方法的迭代器 任务包装并注册到事件循环将协程对象包装为任务对象 1tasks = [asyncio.ensure_future(request()) for _ in range(5)] 获取事件循环对象 1loop = asyncio.get_event_loop() 将任务注册到事件循环 12345## 单个任务loop.run_until_complete(task)## 多个任务loop.run_until_complete(asyncio.wait(tasks)) 获取运行结果 1results = [task.result() for task in tasks] 参考资料非原创声明：本笔记非原创，主要内容参考博客 https://cuiqingcai.com/6160.html]]></content>
      <categories>
        <category>spider</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树 - CART]]></title>
    <url>%2Fml%2F20191122-edcd.html</url>
    <content type="text"><![CDATA[Breiman等在1984年提出了 分类与回归树模型（CART, Classification and regression tree）。 CART能够输出随机变量$Y$的 条件概率分布。 CART的一个重要特点是假设决策树是二叉树，这意味着不论特征是连续型随机变量还是多个取值的离散型随机变量，都将被处理成二元变量，即需要确定一个 阈值。 CART生成生成回归树给定训练数据集$$D = { (x_1, y_1),(x_2,y_2), \cdots, (x_N,y_N) }$$如果选定第$j$个特征和该特征的某个取值$s$，我们就能将所有样本的第$j$个特征的取值与$s$分别进行比较，从而将数据集$D$划分为两部分。对于这两个划分的子数据集$D_1$和$D_2$，我们再次执行上面的操作进行进一步的划分…… 最终我们将数据集$D$划分成了$M$个子集，或者说将输入空间（特征空间）划分成了$M$个区域。对于划分的每一个子区域$R_m$（$m=1,2,\cdots,M$）我们可以通过某种方法求出这样一个值$c_m$，于是就可以定义 回归模型$$f(x) = \sum_{m=1}^M c_m I(x \in R_m)$$其中$I(x \in R_m)$称为 指示函数，样本$x$只会属于子区域中的某一个，而不会同时被划分到多个子区域，因为互斥完备。 这里产生了两个问题： 如何确定最优的$j$和$s$？ 如何定义$c_m$？ 先看第二个问题：因为是回归模型，我们用 平方误差 来表示回归树对训练数据的预测误差$$\begin{split}L(D) &amp;= \sum_{i=1}^N [y_i - f(x_i)]^2 \\&amp;= \sum_{m=1}^M \sum_{x_k \in R_m} [y_k - f(x_k)]^2 \\&amp;= \sum_{m=1}^M \sum_{x_k \in R_m} (y_k - c_m)^2\end{split}$$若使$L(D)$最小，则使$\sum_{x_k \in R_m} (y_k - c_m)^2$最小，一阶导等于0极小，解得$$C_m = \frac1{|D_m|} \sum_{k=1}^{|D_m|} y_k = avg(y_k|x_k \in R_m)$$其中数据集$D$位于子空间$R_m$中的子数据集为$D_m$，$|D_m|$为数据集$D_m$中的样本总数。 综上所述，子空间$R_m$上的$c_m$的最优估计值$\hat{c}_m$是$R_m$上所有输入样本对应的$y$的均值。 再看第一个问题：怎么确定最优的$j$和$s$，亦即怎么对样本空间进行划分。 这里我们将第$j$个变量称为 切分变量（splitting variable），将该变量的取值$s$称为 切分点（splitting point），并进行如下划分：$$\begin{split}R_1(j,s) &amp;= { x|x^{(j)} \le s } \\R_2(j,s) &amp;= { x|x^{(j)} \gt s}\end{split}$$启发式 方法寻找最优的切分变量$j$和切分点$s$：遍历所有输入输入特征，固定$j$扫描可能的切分点$s$，计算$$Loss(j,s) = \sum_{x_i \in R_1(j,s)}(y_i-\bar{y_i})^2 + \sum_{x_i \in R_2(j,s)}(y_i-\bar{y_i})^2$$哪组$(j,s)$的$Loss$小就选谁。 迭代划分得到$M$个子区域。 生成分类树基尼指数基尼指数 用来度量集合$D$的不确定性，基尼指数越大，集合$D$的不确定性越大，与熵类似。$$\text{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^K p_k^2$$二分类问题的基尼指数为 $2p(1-p)$ 样本集合$D$的基尼指数为 $1-\sum_{k=1}^K (\frac{|D_k|}{|D|})^2$ 依据某特征A是否取值为a可将数据集D分为两个子集$D_1$和$D_2$$$\begin{split}D &amp;= D_1 + D_2 \\D_1 &amp;= { (x,y) \in D | A(x) = a } \\D_2 &amp;= { (x,y) \in D | A(x) \ne a }\end{split}$$在特征A的条件下数据集D的基尼指数定义为$$\text{Gini}(D,A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)$$ CART生成遍历所有的可用的特征，遍历该特征所有可能的取值，分别计算基尼指数，取基尼指数最小的切分变量和切分点作为当前二分数据集的策略。 递归对数据集进行二分。 以下表为例说明基尼指数如何用于选择切分变量和切分点 $$\begin{split}\text{Gini}(D,A=1) &amp;= \frac{5}{15}(2 \times \frac25 \times (1-\frac25)) + \frac{10}{15}(2 \times \frac7{10} \times (1 - \frac7{10})) \\&amp;= 0.44 \\\text{Gini}(D,A=2) &amp;= 0.48 \\\text{Gini}(D,A=3) &amp;= 0.44\end{split}$$$\text{Gini}(D,A=1)$或者$\text{Gini}(D,A=3)$都可以作为A的最优切分点 同理求得其它特征的最优切分点$$\begin{split}\text{Gini}(D,B=1) &amp;= 0.32 \\\text{Gini}(D,C=1) &amp;= 0.27 \\\text{Gini}(D,E=3) &amp;= 0.32\end{split}$$上面的四个切分点中，$\text{Gini}(D,C=1)$最小，所以选择特征C（有自己的房子）作为最优分类特征，C=1为最优切分点，将数据集D分为两个子集，同时根节点也产生了两个子节点。由于C=1的样本标签全部为1，则此节点为叶子节点，类别标记为1；接下来对另一个子节点进行寻找最优分类特征和最优切分点。 CART剪枝CART剪枝算法是个很费解的东西，我尝试着通俗化地去描述它。 CART的剪枝同ID3/C4.5的剪枝算法有些许区别。ID3/C4.5的剪枝算法直接计算内部节点剪除前后决策树损失大小，CART剪枝算法多了一步：先从“完全生长”的整树$T_0$开始一个节点一个节点（内部节点）的剪除，直到剩下最后的根节点$T_n$，每剪一次就会得到一棵树，这样就得到了一个树的序列${T_0,T_1,\cdots,T_n}$，注意这里的$n$并不是指整树内部节点的多少，实际上由整树生成的子树非常多，这个$n$可能会很大，但是它始终是有限的；然后在 验证数据集 上分别进行测试，选择损失最小的树作为最终结果。 先来回顾一下决策树$T$的损失函数的定义，其中$C(T)$是对训练数据的预测误差，$|T|$是模型复杂度：$$C_\alpha(T) = C(T) + \alpha |T|,\ \alpha \ge 0$$在ID3/C4.5的剪枝算法中，$\alpha$作为超参需要预先设定，当我们预设$\alpha$的值后对整树进行剪枝，最后会得到一个整树的子树作为最终结果，这里$0&lt;\alpha&lt;+\infty$，特别地，当$\alpha=0$时，整树是最优解；当$\alpha=+\infty$时仅由根节点构成地单节点树是最优解，当然一般情况下$\alpha=0$或者$\alpha=+\infty$都不会是我们想要的结果。所以，当我们确定了$\alpha$的值时，我们就确定了整树的某一棵子树；换言之，一个$\alpha$值对应着一颗子树，而这棵子树就是这个$\alpha$条件下的最优解。 如果我更改$\alpha$的值就会得到不同的子树，那么哪一棵子树才是最好的呢？CART剪枝算法把$\alpha$视为一个参数而不是超参，在剪枝的过程中寻找最优的$\alpha$和这个$\alpha$对应的最优子树。$\alpha \in (0,+\infty)$表示$\alpha$有无穷多的取值，那这是不是意味着有无穷多的子树呢？显然不是的，因为整树的节点是有限的，所以整树的子树肯定是有限的。但是一个$\alpha$必定对应着一棵子树，这就表示很多个$\alpha$对应着相同的子树，换言之，一棵子树对应着很多个$\alpha$值，一棵子树对应了一个$\alpha$的区间。我们将$[0,+\infty]$区间分成$[\alpha_0,\alpha_1)$、$[\alpha_1,\alpha_2)$、…、$[\alpha_n,\alpha_{n+1})$，其中$\alpha_0=0$, $\alpha_{n+1}=+\infty$，每个区间$[\alpha_i,\alpha_{i+1})$的$\alpha$对应的最优子树都是$T_i$。于是乎，我们假设，如果我们可以求出每个子树$T_i$对应区间的左$\alpha$的值，我们就能将子树$T_i$与$\alpha$区间一一对应，通过求解最优子树来求解最优$\alpha$。 那么问题来了，怎么通过子树求区间左边界的$\alpha$呢？ 我们通过比较剪除节点$t$前后损失的变化来决定是否剪除节点$t$。我们观察预测误差$C_\alpha(T)= \sum_{k=1}^{|T|} N_k H_k(T)$：独立计算每个叶子节点$N_kH_k(T)$再求加和，这意味着剪除节点$t$整个决策树的损失变化就等价于上图中粉色区域子树剪除节点$t$损失的变化。 对于整树$T_0$的任意一个内部节点$t$，假设剪除此节点得到的子树为$T_k$，$k \in {1,\cdots,n}$。 以$t$为根节点的单节点树的损失函数为$$C_\alpha(t) = C(t) + \alpha |t| = C(t) + \alpha$$以$t$为根节点的多节点子树$T_t$的损失为$$C_\alpha(T_t) = C(T_t) + \alpha |T_t|$$上面两个式子中的$C(t)$和$C(T_t)$都是可求的。 当$\alpha=0$或者$\alpha$充分小时，预测误差占损失函数的大头，单节点树的损失肯定比多节点子树高，$C(t)&gt;C_\alpha(T_t)$；当$\alpha$充分大时，模型复杂度占损失函数的大头，单节点树的损失肯定比多节点树低，$C(t)&lt;C_\alpha(T_t)$。如果单节点损失较大，说明还是保留$t$的叶子节点比较好，因为损失比较小，此时不剪枝；如果单节点树的损失较小，说明应该剪除内部节点$t$。 那么应该存在这样一个不大不小的$\alpha$，使得单节点树的损失刚好等于多节点树的损失，即$C_\alpha(t)=C_\alpha(T_t)$，解得$$\alpha’ = \frac{C_t - C(T_t)}{|T_t|-1}$$如果$\alpha$的值比这个$\alpha’$小，单节点树损失更大，意味着不能剪$t$；当$\alpha$的值比这个$\alpha ‘$大，单节点树损失更小，意味着应该剪除$t$。根据前面的叙述，子树$T_k$对应的$\alpha$区间为$[\alpha_k, \alpha_{k+1})$，子树$T_k$存在的前体就是相应内部节点被无情的剪除了，所以$\alpha_k = \alpha’$。 所以，我们对整树$T_0$的所有内部节点求其$\frac{C_t - C(T_t)}{|T_t|-1}$，按$\alpha$从小到大依次剪除对应的节点得到相应的子树序列${T_0,T_1,\cdots,T_n}$。 至此，我们已经拿到了按照$\alpha$从小到大排列的子树序列，接下来在 验证数据集 上进行验证，取损失最小的子树即可。]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从生成器到协程]]></title>
    <url>%2Fspider%2F20191120-1aaa.html</url>
    <content type="text"><![CDATA[可迭代对象和迭代器可迭代对象（Iterable）指的是能够使用for循环进行遍历的对象，如字符串、数组、迭代器、生成器等。 可迭代对象实现了__iter__()方法。 迭代器（Iterator）是一种特殊的可迭代对象，实现了__next__()方法。 判断一个对象obj是否是可迭代对象/迭代器:1234from collections import Iterable, Iteratorisinstance(obj, Iterable)isinstance(obj, Iterator) iter()函数称之为迭代器工厂函数，它能将一个可迭代对象转化为迭代器，通过给非迭代器的可迭代对象添加__next__()方法，实现工厂式的加工和包装功能。123456x = [1, 2, 3, 4, 5]print(isinstance(x, Iterable)) # Trueprint(isinstance(x, Iterator)) # Falsex_iter = iter(x)print(isinstance(x_iter, Iterable)) # Trueprint(isinstance(x_iter, Iterator)) # True 生成器生成器（Generator）本质上还是迭代器，因此迭代器的所有特性都适合于生成器。他们的区别在于实现方式不同：实现一个迭代器往往需要定义一个类并且实现其__iter__()方法和__next__()方法；实现一个生成器只需要在函数中使用yield关键字。后者的实现方式更加简洁，性能与迭代器一样高效。 python中的生成器有两种基本实现方式，一种是通过yield关键字实现，一种叫做生成器表达式。生成器表达式与列表推导式十分相似，只需把[...]换成(...)即可。在大数据的迭代过程中，列表推导式将会耗费大量的时间和空间，此时选择生成器表达式性能将会得到显著的提升。只是简简单单地改变一个括号，程序运行的速度就能肉眼可见的变快！123456from collections import Generatorl = [x**3 for x in range(100)] # 列表推导式isinstance(l, list) # Trueg = (x**3 for x in range(100)) # 生成器表达式isinstance(g, Generator) # True 另外一种实现生成器的方法是在函数中使用yield关键字。当函数执行到yield时会返回yield指定的值，同时将函数挂起，这点同return不一样：return返回指定的值时意味着函数调用结束。当再次调用此函数时，不会从头执行，而是会接着上次挂起时的状态接着执行。 yield关键字不仅能返回数据，也能从外界接收数据，接收数据通过.send(...)方法实现。next()函数和.send()方法都能够激活生成器函数继续运行直到下一次遇到yield挂起，不同的是，.send()方法还能够向生成器函数传递值。看下面这个生硬的例子： 使用next(g)使生成器运行至yield处，函数等待外界传值给局部变量x并挂起，此时再使用g.send(3)将3传递给x，函数被激活继续向下执行，计算y等于8，此时遇到第二个yield，函数返回y的值并挂起，所以g.send(3)的返回结果为8。 如果再次执行g.send(4)会发现返回结果为空！这是因为：.send()方法激活可生成器函数并从上次挂起的地方继续执行（yield y），执行至x = yield等待外界传值，也就是说本次.send发送的消息，生成器函数内部没有变量接收，相当于一次无效的消息发送，故而没有返回值。如果再次执行g.send(4)，返回结果显示为16，这与上述g.send(3)的情景相似。 通过yield关键字和.send方法，用户可以随时中断一个函数执行转而执行另一个函数，相当于手动从一个子程序的执行切换到了另一个子程序的执行。在这种子程序的切换过程中没有涉及到线程的切换，我们将一个子程序和它被执行以及被挂起时的状态称之为一个协程（Coroutine）。]]></content>
      <categories>
        <category>spider</category>
      </categories>
      <tags>
        <tag>迭代器与生成器</tag>
        <tag>异步协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启搬瓦工VPS的IPv6服务]]></title>
    <url>%2Fit%2F20191119-a377.html</url>
    <content type="text"><![CDATA[搬瓦工上能买到带IPv6的服务器。此服务器位于LOS ANGELES，IPv4地址是xxx.xxx.xxx.xxx。如下 注册并登陆tunnelbroker，如下 点击Create Regular Tunnel，在2处输入服务器的IPv4地址，确定后自动检查该IP是否支持IPv6。 如果3处变为绿色并显示”IP is apotential tunnel endpoint”表示该IP支持IPv6服务。 在4处选择服务器所在的地区。 点击页面底部的确认。 点击Example Configurations，在1处选择服务器的操作系统（本服务器操作系统是Ubuntu16.04，选择”Linux-net-tools”），确认后在2处会显示相关命令。 打开服务器终端，依次执行上述命令。 输入ping6 google.com检查IPv6是否可用。]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[决策树 - ID3和C4.5]]></title>
    <url>%2Fml%2F20191117-62c1.html</url>
    <content type="text"><![CDATA[决策树学习通常包含三个步骤：特征选择、决策树的生成、决策树的修剪。 决策树学习的思想主要来源于： 由Quinlan提出的ID3算法和C4.5算法 由Breiman等人提出的CART算法 决策树模型与学习决策树模型决策树由节点（node）和有向边（directed edge）组成。 节点分为内部节点（表示特征/属性）和叶子节点（表示分类）。 &gt; 决策树与if-then规则 从根节点到叶子节点对应一条路径，亦即一条规则，不同规则间互斥，所有规则组成全体，即互斥完备性。 &gt; 决策树与条件概率分布 给定特征条件下类的条件概率分布。 决策树分类时强行将样本分到条件概率最大的那一类中去。 决策树学习给定训练数据集$$D = { (x_1, y_1),(x_2,y_2), \cdots, (x_N,y_N) }$$其中 $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入向量（特征向量），$n$为样本的特征维度 $y_i \in { 1,2,\cdots,K }$为每个样本的类标记，$K$为总类数 $i=1,2,\cdots,N$，$N$为样本容量 与训练数据集不矛盾的决策树可能有多个，也可能没有；我们需要求矛盾最小、对测试集泛化能力较高的那个决策树。 特征选择经典的特征选择准则有两个：信息增益，信息增益比。 信息增益熵&amp;经验熵在信息论和概率统计中，熵（Entropy）是随机变量不确定性的度量。 设$X$是有限取值的离散型随机变量，其概率分布为$$P(X=x_i)=p_i \text{, $i=1,2,\cdots,n$}$$随机变量$X$的熵为$$H(X)=-\sum_{i=1}^N p_i \log p_i \text{, 特别的 $0 \log 0 = 0$}$$由熵的定义可知，随机变量的熵（不确定性）与随机变量的取值无关。这其实不难理解：确定性变量是随机性变量的一种特例，对于确定性变量取值1还是1000它们的不确定性应该是一样的，都为0。 熵的单位：bit（当对数底为2时）或者nat（当对数底为$e$时）。 当随机变量$X$完全确定（$P(X=x_k)=1$且$P(X=x_{\text{else}})=0$）或者完全不确定（$P(X=x_{\text{any}})=1/n$）时，$H(X)$取得最小值（0）或者最大值（$\log n$）。 当随机变量只取0、1时$$H(p) = -p \log_2 p - (1-p) \log_2 (1-p)$$ 如果此随机变量的熵值通过极大似然估计得到，称之为经验熵。 条件熵&amp;经验条件熵条件熵（Conditional Entropy） 设随机变量$X$和$Y$的联合分布为$$P(X=x_i, Y=y_j) = p_{ij}, i=1,2,\cdots,n;j=1,2,\cdots,m$$条件熵$H(Y|X)$表示已知随机变量$X$的条件下随机变量$Y$的不确定性$$H(Y|X) = \sum_{i=1}^n [ P(X=x_i) \cdot H(Y|X=x_i) ]$$如果随机变量$X$和$Y$独立，则$H(Y|X=x_i)=H(Y)$，此时条件熵$$H(Y|X) = \sum_{i=1}^n [ P(X=x_i) \cdot H(Y) ] = H(Y)$$如果随机变量$Y$的条件熵通过极大似然估计得到，称之为经验条件熵。 经验之意，即从观察数据中进行推断。 我们将随机变量Y的熵与已知X条件下Y的条件熵的差称之为互信息（Mutual Information）。 信息增益信息增益（Information Gain）指，当得知特征$X$的信息时，类$Y$不确定性减少的程度。 特征A对训练数据集D的信息增益为$$g(D,A) = H(D) - H(D|A)$$决策树中信息增益的概念就相当于训练数据集中类与特征的互信息。 训练数据集中不同的特征具有不同的信息增益，信息增益较大的特征具有更强的分类能力。 计算信息增益举例： 类别的先验分布：$P(Y=是)=9/15=3/5$, $P(Y=否)=9/15=2/5$ 类别的熵：$H(Y)=-3/5\log3/5-2/5\log2/5 = 0.971$ 该训练数据集有4个特征：年龄、有工作、有自己的房子、信贷情况，分别用A、B、C、D表示。 以年龄为例计算其对训练数据集的信息增益： 随机变量取值数值化（有序分类）：青年 $\to$ 1, 中年 $\to$ 2，老年 $\to$ 3 年龄的先验分布: $P(A=1)=P(A=2)=P(A=3)=1/3$ 训练数据集D在年龄A已知时的条件概率分布：$$\begin{split}P(Y=0|A=1)=3/5, P(Y=1|A=1)=2/5 \\P(Y=0|A=2)=2/5, P(Y=1|A=2)=3/5 \\P(Y=0|A=3)=1/5, P(Y=1|A=3)=4/5\end{split}$$此时$$\begin{split}H(Y|A=1) &amp;= - 3/5 \log_2 3/5 - 2/5 \log_2 2/5 = 0.9709506 \\H(Y|A=2) &amp;= - 2/5 \log_2 2/5 - 3/5 \log_2 3/5 = 0.9709506 \\H(Y|A=3) &amp;= - 1/5 \log_2 1/5 - 4/5 \log_2 4/5 = 0.7219281\end{split}$$ 训练数据集D在年龄A已知时的条件熵：$$\begin{split}H(Y|A) &amp;= P(A=1)H(Y|A=1) + P(A=2)H(Y|A=2) + P(A=3)H(Y|A=3) \\&amp;= 1/3 \times 0.9709506 + 1/3 \times 0.9709506 + 1/3 \times 0.7219281 \\&amp;= 0.8879431 \\\end{split}$$ 信息增益 $$ \begin{split} g(Y,A) &amp;= H(Y) - H(Y|A) \\ &amp;= 0.971 - 0.8879431 \\ &amp;= 0.083 \end{split} $$ 同理得 $$ \begin{split} g(Y,B) &amp;= 0.324 \\ g(Y,C) &amp;= 0.420 \\ g(Y,D) &amp;= 0.363 \end{split} $$ 经过比较，特征C（有自己的房子）对训练数据集的信息增益最大，所以选择特征C作为最优的分类特征。 信息增益比信息增益的偏向性根据信息增益的定义$H(D,A)=H(D)-H(D|A)$，如果数据集D中某个特征A的采样值为恒定常数，那么$H(D|A)=0$，即$H(D,A)=H(D)$，此时特征A对数据集D的信息增益达到最大值——D的熵值。那么我们应该选择特征A作为最优划分特征吗？显然不。这是信息增益在描述采样值为常数的特征时的弊端。除此之外，信息增益在计算连续型随机变量时也会遇到不小的问题。 所以信息增益适合于计算离散特征，并且该随机变量取值的采样数不宜过少。 信息增益的相对性所谓的信息增益，字面意思就是确定性的增加量，这个确定性的增加是相对数据集D的熵的，这意味着，D的熵越大，信息增益就可能越大；反之越小。 信息增益比的定义于是，我们将信息增益除以数据集的熵，用相对的确定性增量来描述最优分类特征：$$g_R(D,A) = \frac{g(D,A)}{H(D)} = \frac{H(D)-H(D|A)}{H(D)} = 1 - \frac{H(D|A)}{H(D)}$$ 决策树的生成决策树学习的两个经典算法：ID3算法、C4.5算法。 生成算法只能用来生成树，十分容易过拟合。 生成的树需要进行剪枝，去拟合。 ID3算法算法核心是在决策树的各个节点应用信息增益的准则选择特征。 根据信息增益选择特征的方法还是属于极大似然。 &gt; ID3算法 123456789101112131415CONST 信息增益阈值DEF ID3算法(数据集D, 特征集合A): IF D中所有样本属于同一类C: T为单节点树，将C作为该节点的类标记，返回T ELSE IF A为空： T为单节点树，将D中样本数最多的那个类作为该节点的类标记，返回T ELSE: 计算特征集合A中各个特征对数据集D的信息增益，确定信息增益最大的特征Ag IF Ag &lt; 信息增益阈值: T为单节点树，将D中样本数最多的那个类作为该节点的类标记，返回T ELSE: FOR 采样值v IN 特征Ag中的所有采样值的集合: 从数据集D中划分出特征Ag值为v的子集Dv T的一个子节点 = 递归调用 ID3算法(数据集Dv, 特征集合&#123;A-Ag&#125;) 返回T C4.5算法将ID3算法中的 信息增益 替换为 信息增益比 即可。 为什么叫做C4.5算法而不叫ID4算法呢？据说作者发表完ID3算法后此领域过于火爆导致ID4、ID5等名字有人用了。 决策树的剪枝前面提到，决策树的生成算法倾向于拟合训练数据，极易出现过拟合。 剪枝（Pruning） 即考虑模型的复杂度，对生成的树进行简化。 剪枝 通过极小化 损失函数 实现。 &gt; 剪枝原理 假设：树$T$的叶子节点的个数为$|T|$，$t$是树$T$的一个叶子节点，该叶子节点有$N_t$个样本点，其中属于第$k$类的样本点有$N_{tk}$个。 则，叶子节点$t$的经验熵为$$H_t(T) = - \sum_{k=1}^K \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}$$决策树学习的损失函数为$$L_\alpha(T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T|$$每个叶子节点的样本数越少、经验熵越小，同时总叶子节点数越少，损失越小。 这里损失函数由两部分组成： 决策树模型对数据集的预测误差 $$L(T) = \sum_{t=1}^{|T|} N_t H_t(T) = - \sum_{t=1}^{|T|} \sum_{k=1}^K N_{tk} \log \frac{N_{tk}}{N_t}$$ 决策树模型的模型复杂度 $$C(T) = |T|$$ 剪枝的具体实现如下： 剪枝前的叶子节点集合为{A,B,C,D}，计算其损失函数$L_1$ 剪枝后的叶子节点集合为{E,C,D}，计算其损失函数$L_2$ 因为$L_2 &lt; L_1$，所以我们剪掉以E为根节点的子树，E的 类别 标记为样本数最多的那一个。 参考文献 CART算法见&lt; http://localhost:4002/ml/20191122-edcd.html &gt;]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R读取内容为空的文件报错怎么办？]]></title>
    <url>%2FR%2F20191112-e00c.html</url>
    <content type="text"><![CDATA[一般我们用R处理的多半都是表格文件，但是当文件内容为空时（准确的描述应该是没有有效的表格内容，这意味着可以有许多#注释的行），直接使用read.table函数读取文件会报错。 报错如下： 123&gt; read.table('kws_included.txt', sep = ',', stringsAsFactors = F)Error in read.table("kws_included.txt", sep = ",", stringsAsFactors = F) : 输入中没有多出的行 解决办法： 用try(...)函数捕获异常，只是R中的try(...)函数功能比较简单，但是用来判断一下语句执行是否报错还是没有什么问题的。用法如下： 123df = try(read.table('kws_included.txt', sep = ',', stringsAsFactors = F), silent = T)if('try-error' %in% class(df)) ... # 语句执行出错else ... # 语句执行1正常 其他解决办法： 未知~]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用parallel包的apply并行计算函数]]></title>
    <url>%2FR%2F20191112-d180.html</url>
    <content type="text"><![CDATA[apply系列的函数（例如常见的lapply, sapply）都是可以并行计算的，这在计算单位数较多且每个计算单位非常耗时时，能大大加快计算速度。这里简单记录内建的parallel包如何进行并行计算。 1. 加载R包 1library(parallel) 2. 查看当前节点可用CPU数，确定任务所需CPU数 1detectCores() 3. 指定任务所需CPU数，初始化资源调度器 12## 这里假设调度10核mkc = makeCluster(10) 4. 运行主程序 并行运算的apply家族的函数与原函数都是一一对应的，如 apply $\to$ parApply lapply $\to$ parLapply sapply $\to$ parSapply 代码示例如下：1xs = parLapply(mkc, xs, func()&#123;...&#125;) 注意，func(){...}中的所有第三方包都需要在函数内导入，因为parLapply会启动R的额外进程，在新的进程里面不会自动导入当前环境中已经导入的第三方包。 4. 关闭资源调度器 结束并行运算后一定要关闭资源调度器mkc 1stopCluster(mkc) 不然top之后你会发现自己刷屏了~]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从因变量类型对回归分类]]></title>
    <url>%2Fstat%2F20191017-ca7e.html</url>
    <content type="text"><![CDATA[假设自变量是$X$，因变量是$Y$。按照$Y$的数据类型将回归分为以下几种： (一) $Y$是数值资料 $Y$是连续数据：普通线性回归 $Y$是计数数据：计数回归 泊松回归 负二项回归 零膨胀泊松回归等 计数数据：对某些对象计数的数据，取值为自然数 $Y$是生存数据：生存回归 生存数据：生存数据表征了对象从”出生”到”死亡”的生存时间，本质上还是一个连续型数据。但是当我们进行分析时对象往往还未”死亡”，我们不能获得准确的生存时间，但是我们能获得该对象的最小生存时间（即到记录数据为止时的生存时间），记录数据时以”60d+”这种形式表示，意味着该对象的最小生存时间为60天。有趣的是，如果我们能获得所有样本的准确生存时间，生存回归本质上就是普通线性回归。 (二) $Y$是分类资料 $Y$是二值数据：0-1回归 Logistic Regression Probit Regression $Y$是定序数据：定序回归 定序数据：数值无意义（意味着不能进行代数运算），但是顺序很重要 注：本文只是简单总结记录方便查阅，欲览全文，查看此处，尊重原创！]]></content>
      <categories>
        <category>stat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[编写bash脚本时怎么方便的测试代码]]></title>
    <url>%2Fos%2F20190928-1314.html</url>
    <content type="text"><![CDATA[编写bash脚本时怎么方便的测试代码？ Esc Shift :/; wq 太麻烦了 当然前提是能频繁测试的代码 解决办法：开两个窗口，一个vim编辑并保存，一个通过watch -n 2 &quot;sh test.sh ...&quot;频繁刷新执行test.sh]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[screen-SSH终端离线持久化工具]]></title>
    <url>%2Fos%2F20190911-89a2.html</url>
    <content type="text"><![CDATA[有时候我们使用自己的电脑远程连接服务器（例如SSH）进行工作，某些任务我们希望放在前台运行但是其运行时间可能很长，如果程序运行期间我们需要断开连接，一般情况下这个前台任务也会随之中断。例如R交互式环境中的程序。screen工具就是为了解决这样一个问题。 screen工具不仅可以保证在断开远程连接的情况下继续运行当前任务，还可以实现单个实际窗口中操纵多个工作窗口。简单来说，新建一个screen会话会创建一个主进程，这个主进程对应着一个会话窗口。这个主进程是存储在服务器上的，它不受我们连接服务器的SSH进程的影响，因此当我们断开SSH连接时这个进程依旧存在，在下次重新连接服务器时依然可以恢复。而我们连接服务器时的工作环境中的任务实际上受SSH连接进程的影响，当我们断开连接时相关联的任务自然而然也就停止了。（PS: 下面提到的会话就是指一个虚拟窗口） screen的用法如下： 怎么查询当前服务器中建立了哪些会话？ screen -ls 即可 怎么建立一个会话？ screen -S &lt;SOCKNAME&gt; 可以建立一个虚拟会话，查看会话信息如下：由上可知，一个虚拟窗口的ID标识由进程号PID和会话名（SOCKNAME）组成，我们可以通过这两个信息恢复会话。除了 screen -s ... 之外，screen -R ... 也能建立一个新的虚拟窗口，与-S不同的是，-R是去尝试着恢复一个已有的会话，如果在已有会话中没有找到，他就会建立一个新的会话，跟“若目录不存在则创建目录”是一个意思。 怎么恢复一个会话？ screen -r ...可以恢复一个会话，当会话不存在时会报错；screen -R ...也能恢复一个会话，但是当会话不存在时会创建一个新的会话。注意：同一时间一个会话只能在一个实际窗口中打开（例如你可能会在不同的电脑上连接服务器或者在一台电脑上打开多个SSH会话）。当会话被挂起时（意味着此时没有窗口打开这个会话），会话信息中每个ID后面会标识出(Detached)，此时意味着你可以在当前窗口中打开这个会话继续工作；如果标识的是(Attached)，那么标识这个虚拟窗口已经在其它的地方被打开了，你将不能打开这个会话。如果你确定这属于异常情况，你可以使用screen -d ...强制挂起一个会话，此时状态会变成(Detached)，这表示你单方面终止了在某个未知地方打开的虚拟窗口。screen对你命令传入的会话名与会话ID进行匹配，例如你可以： 只指定进程号PID，例如screen -r 23288 在不引起歧义的情况下只指定会话名的首字母或前几个字母，例如screen -r n将恢复23288.net 当然你也可以传入完整的&lt;SOCKNAME&gt;或者&lt;PID&gt;.&lt;SOCKNAME&gt; 怎么退出并挂起当前会话？ 依次摁下Ctrl A D三个键即可退出并挂起当前会话 怎么挂起当前窗口中创建的指定会话？上面已经提到了，screen -d ...可强制挂起指定会话 怎么删除一个会话？请确认你的虚拟窗口完成使命后再删除它，不然追悔莫及。 直接kill &lt;PID&gt;可删除相应的会话 摁下Ctrl A K D可删除当前会话中的所有任务并退出当前会话 总结一下，screen常用的命令有： 12345678screen -lsscreen -rscreen -Rscreen -Sscreen -dCtrl + A + D 组合键Ctrl + A + K + D 组合键kill &lt;PID&gt; 下面是我设置的alias（使用极其频繁，根据奥卡姆剃刀原则，越频繁越简洁）： 12345alias sr="screen -r"alias sl="screen -ls"# 还可以设置几个快捷窗口alias srq='sr q'alias srr='sr r' 其它命令： 12## 重命名已经存在的会话名称screen -S &lt;OLDNAME&gt; -X sessionname &lt;NEWNAME&gt;]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>Linux实用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中与号和星号的再理解]]></title>
    <url>%2FCpp%2F20190713-1536.html</url>
    <content type="text"><![CDATA[大学初识指针和引用，一知半解。最近想重新学习C++，发现自己对指针和引用的了解透彻了许多，遂记之以温习。 &amp;和*实际上各有两种含义：出现在定义中时，作为运算符时 &amp;用于定义往往只在函数形参中使用，表示引用，如void func (int&amp; x, int&amp; y) ... &amp;作为运算符表示取地址操作，例如int* p = &amp;x *用于定义中时表示定义一个指针型变量，例如int* p或者int *p *作为运算符时表示间接取值，即通过地址间接而不是通过变量名直接获取值，如(*p)++ 作为运算符&amp;和*分别称之为取地址运算符和间接取值运算符。 &amp;是一个一元运算符，是一个运算符，它的作用是取出变量的地址，取出的是地址，地址的类型会带上* ，例如int型的变量x的地址（例如0x61fe44）的值的类型就是int*，这个值不能被保存到int类型中（因为int和int*是两种不同的数据类型），那么这个地址值怎么保存呢？这里定义了指针类型用于保存地址的值。上面例子中我们可以定义int指针类型用于保存int类型的变量的地址（通过&amp;获取），即int *p = &amp;x。其实int *p这种写法容易引起误解，int* p就明显一点，但是前者似乎更通用。上面的定义已经表明了：p 是一个int*类型（即int指针类型）,它保存的是变量x的内存地址，因此可以直接通过指针访问变量x存储的值，方法是*p，所以实际上*p和x都能访问到相应地址块上的值，这里*使得我们可以间接通过地址p访问值，所以叫间接寻址运算符。这里注意体会int*中的*和*p中的*意义是不完全相同的：前者表明这是变量（p）是一个指针型变量，后者是一种运算符，所谓运算，就从一个状态出发得到新的状态，这里就是指从地址出发计算得到了值 123456int x = 1000;int *p; // 写作 int* p可能更好理解点p = &amp;x; // 这两步可以直接写作 int *p = &amp;x，写成 int* p = &amp;x 可能更加容易理解cout &lt;&lt; x; // 1000cout &lt;&lt; p; // 类似于 0x61fe44 这样的表示地址的值，其类型为 int*cout &lt;&lt; *p; // 1000，对地址进行*运算得到值 用于参数定义*用于参数定义时表示定义一个指针型变量，这十分容易理解。 那么&amp;用于参数定义时表示什么意思呢？引用！ 下面以常见的三种函数参数传递方式为例说说引用的用法。 参数传递有三种常见方式：值传递、指针传递、引用传递 不得不说时隔这么多年，我总算理解了三种参数传递的方式的基本区别，真笨。 假设我们现在需要对一个整型变量x进行加1操作，三种参数传递的解决方案如下： 值传递：值传递即按照普通方式定义形参，此时会对变量进行复制，以保证函数内部不会修改实参的任何信息，这样的参数传递方式当然不能直接修改实参的值，它返回的实际上是一块新的内存区域 12345int f (int v) &#123;return ++v;&#125;int x = 1;int y = f(x);cout &lt;&lt; x; // 1cout &lt;&lt; y; // 2 指针传递：指针传递实际上传递的不是变量名称，而是变量指向的内存区域，这个区域怎么表示呢？当然用一个地址来表示啦。这个地址怎么获取呢？当然是用 取地址运算符&amp; 啦。对于指针传递来说，形参定义为一个指针型变量，而实参也是一个指针型变量（地址），所以指针传递将会直接修改原内存块的值，这导致函数执行后我们再通过相同的方式取访问变量（实际上是访问变量指向的内存块的值），它的值已经发生了改变 12345void addone (int* p) &#123;(*p)++;&#125;int x = 1;cout &lt;&lt; x; // 1addone(&amp;x);cout &lt;&lt; x; // 2 引用传递：指针传递实际上并不优雅，它直接让程序接触到了内存地址。与其如此，我们为什么不直接定义一个类似于值传递中的但是又不用对实参进行复制的形参呢？这个形参本质上还是一个非指针数据类型，但是它能直接修改实参的值，就相当于给实参定义了一个别名，不论通过实参本身修改值还是通过这个别名修改值都是等价的。这种方式称之为引用，通过&amp;实现，&amp;作为引用含义时常常出现在函数形参中（&amp;还可作为运算符使用），它表示形参实际上是实参的一个别名，对形参的任何修改都将等价于对实参的修改，例如形参int&amp; x定义的就是对某个整形实参的引用，此时传递的实参也不需要像指针传递那样需要先获取地址，程序本身规避了对地址进行直接操作 12345void addone (int&amp; v) &#123;++v;&#125;int x = 1;cout &lt;&lt; x; // 1addone(x);cout &lt;&lt; x; // 2 *和&amp;的其它高级用法以后遇到了再温习吧~]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[人类成脑单细胞转录组和表观组数据的整合分析]]></title>
    <url>%2Fbioinfo%2F20190531-c0e0.html</url>
    <content type="text"><![CDATA[原文【Nature Biotechnology】【PubMed】 Abstract对人脑细胞进行更加细致的分类需要设计大量的实验从细胞的各个角度研究细胞或者设计数据处理方法整合现有数据对细胞进行注释。本文即从后者出发思考怎么综合多组学数据。这里提出了两个改良的高通量测序方法： snDrop-seq: single-nucleus droplet-based sequencing scTHS-seq: single-cell transposome hypersensitive site sequencing 基于两种方法获得了&gt;60000个单细胞核（成人视觉皮质、额皮质、小脑）的转录数据和开放数据。 这两组数据内蕴与细胞类型差异相关的调控元件和转录因子信息，可用于 研究大脑复杂机制，本文研究了髓鞘再生（remyelination） 将疾病相关的风险变异关联到特定的细胞群，帮助认识大脑的病理 多组学方法综合分析能够对复杂组织器官的的单细胞进行更加细致的研究（~interrogation）。 Introduction转录组分析的不足@ 人脑是个极其复杂的结构 人脑是一个庞大、复杂的网络，它由大约一万亿个神经元组成，这些神经元在空间上有序组织，在功能上相互联系，它们镶嵌在更加庞大的神经胶质（glia）和非神经元细胞中。 @ 分析细胞核比分析完整细胞更好 建立完整的人脑细胞图谱，需要良好扩展性和无偏性的单细胞分析方法，这些方法不仅限于分析活体完整细胞，也能分析提取出来的细胞核信息。 如果能分析细胞核数据，将会好处多多$^{[1-4]}$： 细胞可以是活体采集的，也可以是保存性的，例如尸体样本、冷冻样本 可以获得大量的RNA数据，能得到更加准确的分析结果 减小组织解离的误差$^{[5]}$ @ snRNA测序技术及后续分析方法 单核转录组测序（SNS）能够以较大的测序深度（每个细胞大约八百万reads）分析多个人类大脑皮层区域上丰富的神经元亚型$^{[4]}$，但是它也有许多限制，例如 低通量：每张微流芯片理论上只能对64个细胞测序 高成本 样本误差：微流芯片对十分少量的非神经细胞的细胞核的捕获能力很差 本文提出了一个能够处理保存性样本、分析高通量snRNA-seq数据的方法。 表观组锦上添花转录图谱能够鉴定复杂组织器官中功能差异显著的细胞类型，额外的表观信息能够使我们对表达图谱的受调节方式有更加全面的认识 —— 如虎添翼。 全基因组水平的研究已经建立了调控位点到了染色质开放性区域（同下文的调节子，如启动子内、增强子内）的映射，因此顺式调控位点能够区分细胞类型和谱系$^{[6,7]}$。这种细胞类型特异性的调节子的鉴定有利于强化我们对基因程序（如细胞分化、细胞定型、功能化）的理解。 进一步地，与多样性和疾病相关的基因变异大多数都位于内含子内或者基因间区域$^{[8]}$。与组织特异性调控位点$^{[6,7]}$的富集分析结合绘制的细胞类型特异性调控子图谱能够提供额外的信息帮助我们了解发病机理。 与转录组实验相比，表观组实验的一个障碍就是难以获得大量的单细胞样本。 最近的一些工作提高了实验灵敏度，将细胞需求减少到了几百个$^{[9]}$甚至是一个细胞$^{[10-13]}$；然而，这类单细胞精度的分析方法是否适用于异质性的、保存性的人类组织（例如大脑）仍需验证。 解决了什么问题构建&lt;包括表观遗传信息在内的&gt;人脑细胞类型的综合图谱，需要&lt;用于分析核转录数据和保存性样本数据的&gt;更加高效的方法。 首先单细胞基因组研究中核分离技术是可信的（~amenable）$^{[14,15]}$。因此本文提出了两个优化的测序方法，在单细胞水平上帮助量化核转录本和DNA的开放性。因此我们可以在同一个保存性样本中同时对基因的表达和调节进行综合性分析。 本文前面部分解决了下面三个问题： 描述人类大脑皮质和小脑脑区的细胞多样性 鉴定具有脑区特异性的神经元/非神经细胞类型 大规模地鉴定转录因子的活性和目标基因的表达谱 最后，还将疾病风险变异（risk variants）映射到了细胞类型特异性的调控区域，从概念上验证（proof-of-concept）出了一些可能与大脑疾病相关的病变细胞类型。 Results人类大脑皮质和小脑脑区的单细胞分析介绍了两种改进的测序方法snDrop-seqsingle-nucleus Droplet-bases sequencing 基于微滴实验技术$^{[16-18]}$的snDrop-seq，实现了高通量的单细胞转录组并行测序。将此技术应用于六个成年人类遗体的大脑分析（包括视觉皮质、额皮质、侧小脑半球）。=&gt; 这是一种转录组测序方法！！！ scTHS-seqsingle-cell Transposome Hypersensitive Site sequence 为了同时研究表观遗传，本文设计了一种定量单细胞DNA开放性的实验方法。=&gt; 表观遗传 表型遗传 DNA/染色质开放性 ATAC scTHS-seq利用了体外转录组线性扩增和人为设计Tn5转座酶突变$^{[15]}$的优势，实现比ATAC-seq$^{[20]}$灵敏度更高的测序方式，能较高覆盖远端具有细胞类型特异性的增强子$^{[21]}$。 联合这两个测序方法可以从同一个脑区样本中同时得到表达和调节特征，从而对细胞多样性进行独立无偏的分析。 基因表达分析对snDrop-seq的结果进行基因的表达分析，质控后一共36,166个核，其中35,289个核（=视觉皮质19,368+额皮质10,319+小脑半球5,602）被成功分型。 跨物种混合分析的证明了很小比例（2~11%）的双峰（doublets），这与全细胞测量结果相似。 （图） 测序结果平均到每个细胞核有6,200个reads，这些reads大部分都落入了内含子区域，并且主要位于转录本的3’端，这是因为转录本和前体RNA都是依靠poly(A)捕获的。（图） 与全细胞转录组方法相比较，仅含细胞核的Drop-seq和完整细胞的Drop-seq的覆盖度更低，但是却能产生相似的结果（例如检测到的基因比例）。尽管细胞核和完整细胞结果具有一致性，核测序结果还是存在一定的系统误差，特别是对于较长的基因，这可能是可变剪切导致的。 总的来说，每核测到了928个不重复的转录本和719个基因（中值）。随着样本数量的增加，这一深度可以有效解决细胞类型多样性和基因表达动态变化的问题。 实际分型结果为35个明显的聚类簇，包括 神经细胞 皮质兴奋性神经元（Ex） 皮质抑制性神经元（In） 小脑颗粒细胞（Gran） 浦肯野神经元（Purk） 非神经细胞 内皮细胞（End） 平滑肌细胞或者周皮细胞 星形胶质细胞（Ast） 少突胶质细胞（Oli）以及他们的前体细胞（OPCs） 小星形胶质细胞（Mic） 还研究了这些簇在脑区分布上的异质性，例如 小脑特异性的Ast（Ast_Cer）、OPCs 在视觉皮质和额皮质检测到了不同的兴奋性神经元（图） 基因调控分析为了识别相应的调控信号，基于scTHS-seq进行了基因的调控分析。 一共测定了32,869个细胞核。对每个脑区单独聚类，其中27,906个核（=视觉皮质13,232+额皮质4,753+小脑9,921）能成功分型。（这里没有给出聚类图，只给出结果表） 27906个核中，15,786个核映射到了snDrop-seq的聚类结果上。 鉴定出了覆盖了大约144百万碱基对共287,381个与染色质开放性相关的peak。 每个细胞覆盖染色质开放性区域的reads个数（median）为10,168。 人鼠数据混合分析的低比例双峰特性说明可以使用combinatorial indexing protocols。 为了鉴定出scTHS-seq数据中表观遗传差异显著的亚型，本文采用了一种无偏聚类策略：将每个细胞每个基因组位点上观察到read的概率建模为censored poisson process。这个方法解释了：即使是开放性最强位点的信号也会在匹配到少量reads时饱和。【Q】 The scTHS-seq signal from even the most accessible site will saturate after only a few reads. 总结：解决了什么问题从表观组对细胞分型比从转录组对细胞分型更加困难，因为许多调控元件的功能仍然是未知的。但是借助于邻近开放性位点的基因的信息，就能够区分皮质和小脑脑区的许多细胞类型。.具体来说，调控分析主要解决了下面两个问题 与额皮质、视觉皮质之间主要细胞类型相关的表观特征 尚未深入研究的小脑Gran神经细胞的神经元特征 转录组 =&gt; 细胞类型 &amp; 脑区异质性转录组数据来源于snDrop-seq marker表达一致性1. 基于snDrop-seq的marker表达情况 不同的脑区都使用转录本数据对细胞的归属和身份进行鉴定，检测到了细胞类型/细胞亚型特异性的marker基因的预期表达，表达谱与从人、鼠脑区的混池数据推出的结论高度一致。$$\text{Figure 2 - A \&amp; B}$$A图是marker基因-细胞类型基因表达的小提琴图，从图中明显可以看到每个marker都在某一类细胞中大量表达。A图右侧的三列数据分别表示：细胞核数目、转录本数目、不同脑区采样的相对比例。B图是marker基因-细胞亚型的表达图谱。 2. snDrop-seq的一致性验证 比较小鼠视觉皮层和人颞叶数据，基于核的数据与基于全细胞的数据具有一致性。$$\text{Supplementary Figure 7 - A \&amp; B}$$ 神经元的过表达是以牺牲非神经细胞例如Ast和End为代价的。【Q】 However, we observed over-representation of neurons at the expense of non-neuronal types such as Ast and End cells. 从snDrop-seq数据计算细胞组成时仍然存在技术上的误差，可能是因为细胞转录本数量太少$$\text{Figure 2 - A}$$ Ex和In按照（先前对6个皮质区域进行单细胞核测序鉴定出的）细胞亚型的相关性进行注释$$\text{Supplementary Figure 7 - C}$$从上图还可以看出，除了高度的一致性，snDrop-seq还能对细胞亚型的分型进行优化。例如，视觉皮质层Ex的细胞亚型Ex3细化为了Exa-d四种亚型。相比于作者之前的一项研究$^{[4]}$，snDrop-seq通过复杂组织的少量采样解决神经细胞亚型多样性问题灵敏度更高。（神奇般的引用了自己） 细胞亚型的空间特异性（皮质层分布）Ex和Gran将Ex和Gran（通过SLC17A7和GRM4筛选）进一步分解为14种细胞亚型（13Ex+1Gran），如图Figure2A。 【好长的一句话，看图比较容易】除了对皮质层细胞进行更加细致的亚型分型（此处的皮质层包括第5层中不同的亚型$\text{HS3ST5}^-\text{PCP4}^+\text{/ Ex5a}$、$\text{HS3ST5}^+\text{PCP4}^-\text{/ Ex5b}$ 和 $\text{HTR2C}^+\text{PCP4}^+\text{TLE4}^+\text{/ Ex6a}$，Ex6a与第6层中的$\text{HTR2C}^-\text{TLE4}^+\text{/ Ex6b}$相邻），随着可视的皮质层特异性的细胞亚型的数量的增加，我们还可以解决巨大的脑区异质性问题，包括亚型$\text{RORB}^+\text{PCP4}^+\text{/ Ex3b}$、$\text{RORB}^+\text{NEFM}^{hi}\text{/ Ex3c}$、$\text{RORB}^+\text{PHACTR2}^+\text{EYA4}^+\text{/ Ex3d}$。如图Figure2C$$\text{Figure 2 - C}$$ 从上图，Ex3d神经元的marker是EYA4，它在第4皮质层的视觉皮质中特异性表达，在额皮质中没有表达。 In和Purk我们将In和Purk（利用GAD1筛选，如图Figure2A）按照权威的中间神经元marker（例如VIP, RELN, PVALB, SST）和在单一亚型中表达的基因（例如THSD7B, CA8, GLCE）分解为13种细胞亚型（11In+2Purk），如下图。$$\text{Figure 2 - B - Bottom}$$ 进一步研究了In神经元亚型的空间分布差异，如下图$$\text{Supplementary Figure 8 - E}$$ 小脑细胞类型分析前面做的都是大脑皮质，现在开始研究小脑 小脑结构相比于大脑皮质，小脑具有更加明显的细胞结构$$\text{Figure 2 - E}$$ marker的表达及空间特异性本文解决了多个主要的细胞群，包括Gran和Purk神经元以及它们的支持类型$$\text{Figure 2 - F}$$$$\text{Figure 2 - G}$$ Purk神经元亚型发现了两个明显的Purk神经元亚型Purk1和Purk2：它们都表达了抑制性的marker基因GAD1/GAD2（如图Figure 2F），但是可以通过marker基因SORCS3进行区分（SORCS3在Purk2中不表达）。 Ast神经元亚型鉴定出了已被证明存在的Ast的亚群（如图Figure 2F）： Velate Astrocytes, VAs：表现出与皮质星形胶质细胞相似的转录特征，对Gran神经元起支持作用 Bergmann Glia, Ast_Cer：一种特殊的星形胶质细胞，在小脑层状结构的发育过程中起着重要作用，支持和调节Purk神经元的突出活动。通过marker基因ALDH1A1筛选出来的Ast_Cer细胞丰富表达了AMPA受体编码基因GRIA1和SLC1A3（或者叫GLAST）。 OPCs神经元亚型进一步分解出了两个明显的OPCs细胞亚群： $\text{LUZP2}^+\text{CASK}^+$：表现出与皮质OPC相似的转录特点 $\text{ORAOV1}^+\text{LRP6}^+\text{RCN2}^+$：只在小脑中发现，未介绍功能 Oli神经元大部分小脑Oli起源于小脑外部，少数起源于local祖细胞。【很莫名的一句话】 This is consistent with the majority of the cerebellar Oli originating from outside the cerebellum, and only a minority being derived from local progenitors$^{[30]}$. 形态学区分法的限制也有人提出了基于形态学区分小脑半球的不同类型，但是其局限性很大： 这些易于区分的细胞数量有限 小脑组织中Gran神经元极其丰富，形态学并不能帮助我我们完美地区分亚型。 转录表观联合模型思想为了建立不同亚型转录状态和表观状态之间更加准确的对应关系，进行如下操作： 从ATAC数据中寻找（与通过转录组数据求出的细胞亚群相对应的）细胞 从转录组数据中寻找（与通过ATAC数据求出的细胞亚群相对应的）细胞 为了实现上述操作，进行了下面两个工作【Q】： 以差异表达模式为基础，训练一个GBM预测差异开放的基因组位点 以差异开放性为基础，训练另外一个GBM预测差异表达 量化依据： 一个位点到一个基因的距离 某个位点或基因差异表达或者差异开放的程度 模型示意图如下$$\text{Figure 3 - a}$$ 可行性尽管预测单个基因或者位点的差异表达或者差异开放的程度有点困难，联合考虑多个基因或者位点来对细胞分类的可信度还是挺高的。Supplementary Figure 10 步骤考虑到转录组数据的高精度特性，我们试图通过识别观察到的转录组亚群的染色质开放性特征，将该模型应用于进一步地划分染色质开放性聚类簇【Q】 通过积累表达信号对已识别的细胞类型进行层次聚类，建立一个细胞类型关系的系统树图 在系统树图上进行迭代二分，识别出两个分支上差异表达的基因 使用GBM分类器预测差异开放的基因组位点 考虑所有预测的差异开放的位点将scTHS-seq的细胞分类到其中一个分支，其染色质开放模式与该分支的染色质开放模式一致 从起始的分类结果我们可以定义一个refined差异开放信号，综合二者判断最终所属分支，同时通过交叉验证评估分支注释的稳定性【Q】 应用通过这种方法，我们从转录数据中识别出了（新的？）非神经元细胞和神经元细胞类型之间的差异表达基因。 通过预测的差异开放性位点，细化（染色质开放性数据中的）非神经元和神经元细胞类型置信度更高。 解决了神经元细胞类型的问题，我们可以重复这个过程来区分兴奋性神经元和抑制性神经元。然后再次重复这个过程来区分抑制神经元中不同的细胞亚型（Figure 3b &amp; Figure 3c）。依此类推。 通过这种方法，我们能够识别与抑制性神经元亚型（InA, InB）相关的表观遗传（开放性）差异 InA和InB的差异：发育来源不同（Figure 3b） InA起源于内侧神经节的皮质下区域 InB起源于外侧/尾侧神经节的皮质下区域 然而，因为差异开放位点的数量不够，尝试从InA和InB中分解出更多的细胞亚型时结果不稳定。（Figure 3b &amp; Figure 3c） 仅仅通过差异开放信息区分ExL4和ExL5&amp;ExL6的差异是很难的，此时结合来自更高精度的转录本数据的差异表达信息能够帮助我们识别（更多的？）相关差异开放性位点，从而将基于染色质开放性的聚类簇分解成更加细致的亚型。 鉴定出主要细胞类型的置信度较高，如 Oli, OPC, Ast, End, In, and Ex cells as expected from the visual cortex（Figure 3c） Ast, Oli, In, and Ex cells in the frontal cortex and Gran, Oli, and End cells in the cerebellum（Supplementary Figure 6） 我们进一步证实了，分解出来的细胞类型/亚型在marker的启动子位置出现了开放性富集，这表明了：表达和调控的一致性。（Figure 3g） 总结相比于转录组数据，表观组数据的细胞分型精度较低。但是整合两类数据能够重构出较为详细的大脑细胞类型开放性图谱，简单研究每个细胞类型调控过程的活性。 髓鞘再生 中枢神经系统的髓鞘是由少突胶质细胞缠绕神经轴突形成 找TFs前面建立了可以根据表观组数据区分细胞类型/亚型的模型，现在寻找与每一种细胞类型/亚型特异性相关的TFs。 我们要找的是这样一些TFs：它们的预测结合位点在具有细胞类型差异性的开放性区域内过度表达。（Figure 4a） 扫描379个TFs（来自于JSPAR数据库，带有PWM），然后找到了与至少一种细胞类型统计显著相关的TFs。（Figure 4b） 深入研究了不同空间位置的Ex和In特异性的TF的活性。 使用snDrop-seq数据进行交叉验证，确保细胞类型特异性的TF也会在相应的细胞类型中高表达。 成年人脑中OPCs向Oli的转变过程一个转录因子引发的惨案：新的中间细胞、人鼠转变差异 【机翻】为了证明前面提到的综合性分析方法具有一定的生物学实用价值，深入研究了成年人脑中OPCs向Oli的转变过程。髓鞘再生是通过OPCs的神经元活化和分化成有髓少突胶质细胞而发生的，这些细胞重新包裹神经元轴突，以恢复跳跃传导和正常功能。这一过程的失调可导致严重的神经系统疾病，例如多发性硬化症（MS）。值得注意的是，我们发现了区分OPC和Oli细胞群的特异转录因子（F4b）。为了确定这些【Q】是否能揭示成人髓鞘再形成的关键调控过程，我们评估了视觉皮质中这些谱系的分化状态和相关基因表达特征。利用Destiny的扩散图谱，我们可以将OPC和Oli细胞沿着发育轨迹定位（F4C、SF12），并评估细胞在开始、中间和结束时的差异表达。在此过程中，我们发现了独立于实验批次的中间细胞（未成熟的Oli，称之为iOli），它们具有独特的表达特征（图4C-D，图S12，表S6），可以深入了解人类成年Oli成熟的早期机制。与小鼠实验结果一致，我们的人类OPC细胞群表达了与小鼠OPCs相关的marker（PDGFRA, CSPG4, SOX6, VCAN），但他们也表达了更忠诚的小鼠祖细胞的marker(ITPR2, NEU4)，表明我们的人类数据无法区分这些微妙的状态（图4D，表S6）。此外，成熟的Oli（mOli）细胞群表达了与髓鞘形成相关的marker（PLP1, MBP, MOG）（图4D，表S6），并没有像小鼠那样分解成进一步的亚型，这可能是成年人脑中缺乏幼年状态的原因。 渐进表达特征 =&gt; OPC谷氨酸激活反应结合其他研究 然而，在OPCs、iOli和mOli中发现的渐进表达特征（表达量？），是跨脑区域保守且独立于排序方法的，可以进一步细化为OPC谷氨酸激活反应的各个阶段（图S13）。最近的研究表明，AMPA和红藻氨酸受体介导了一种对谷氨酸的初始轴突-OPC突触反应，该反应将OPCs导向暴露的轴突位置，NMDA受体的激活将在轴突位置上引导髓鞘再生。与这一发现一致，我们的数据显示AMPA和红藻氨酸受体编码基因（GRIN/GRIK）在OPCs和iOli中富集，而NMDA受体编码基因（GRIN2A, GRIN2B）仅在iOli亚群中富集（图4D，表S6）。OPC成熟过程中所鉴定的基因集的功能个体发育经历了六个阶段，这些结果为神经细胞在髓鞘再生中的活动机制提供了独立的支持：(图S13) 神经发生（祖细胞marker表达） 谷氨酸受体活性 突触传递 离子通道活性 膜组装 轴突鞘形成。 OPC和Oli亚群的差异上调基因的开放性差异开放、开放互斥、保守调控 为了了解这些基因表达动态的调控机制，我们基于scTHS-seq数据联合评估了视觉皮质中OPC和Oli亚群的差异上调基因的开放性。 与我们的表达数据一致，OPC、iOli和mOli基因的调控位点显示出差异开放性(图4E)。此外，OPC和iOli基因的开放性显示出几乎完全的互斥性，表明可能存在维持这两种状态的活跃调控机制。OPC开放性区域内最显著的转录因子活动与SOX9有关(图4F，表S7)（已知SOX9与小鼠OPC的特化、生存和迁移有关）。此外，我们发现iOli特异的开放性位点表现出了重要的TCF4转录因子的结合富集（图4 g、表S7）（TCF4在调节Wnt/β-catenin以促进小鼠髓鞘再生中扮演着很重要的角色）。因此，我们的转录因子分析揭示了维持成年少突胶质细胞祖细胞并协调其成熟以进行再髓鞘化的保守调控机制。 将病变风险映射到大脑特定的细胞类型细胞类型特异性表观基因组信息对于许多常见遗传疾病的致病细胞类型和特异性调控机制的识别具有重要价值，但对脑疾病的认识仍不充分。 为了解决这一问题，我们通过美国国立卫生研究院（NIH）的GRASP搜索工具（Genome-Wide Repository of Associations Between SNPs and Phenotypes）获取了$p&lt;10^{-6}$的SNPs，这些SNPs与10个大脑疾病相关，同时还选取了7个非大脑疾病相关SNPs作为对照。 由于致病突变常位于与GWAS SNPs连锁不平衡的不同位置，我们以某一特定疾病的所有GWAS SNPs为中心，在100 kb窗口中寻找富集的dna开放性区域，并通过随机排列评估其意义。 该分析证明了（在多种细胞类型和亚型中）很强的疾病特异性富集，与另一种可能（即一致性）形成对比。 值得注意的是，我们发现阿尔茨海默病(AD)风险变异在Mic中高度富集，这与在晚发性AD皮质和AD风险中被发现激活的显著Mic信号一致。 与ATAC-seq data批量数据的比较表明，我们的单细胞数据对于预测Mic调控位点及其相关的疾病特异性风险变异富集具有敏感性。 我们没有发现任何与大脑无关的疾病变异的显著富集神经元，这进一步支持了我们对疾病致病性的预测细胞类型。 事实上，大多数与大脑无关的富集都是与这些疾病密切相关的细胞类型，如自身免疫性疾病中的Mic和End细胞。 因此，我们的单细胞调控图与大量研究高度一致，可能与细胞类型特异性疾病风险相关。 尽管进一步地验证需要更多的样本和其它疾病的数据，发病机理尚未研究，我们的染色质图谱提供了一个细胞类型或细胞亚型特异性数据集，通过这些数据集可以从新的角度了解大脑疾病。 总结： 在线搜索疾病相关的SNPs，观察它们是否有细胞类型特异性：验证出了AD在Mic中富集，与大脑无关的疾病不会富集。 Discussion细胞类型组成的重构有利于加深我们对人脑正常功能和功能失调以及发病机理的理解。 本文使用了两个扩展性较强的方法（snDrop-seq和scTHS-seq）在单细胞水平上综合分析了成年遗体大脑的转录组和表观组数据。 使用核分离技术克服活体采样或者保存性样本预处理的阻碍，我们鉴定出了大脑皮质和小脑脑区神经细胞/非神经细胞一共35个细胞亚群。 我们的结果强调了在复杂组织中进行单细胞稀疏采样的价值：只要单细胞数据内含用于聚类和排序的所有信息，它们就能被整合起来，起到与混池测序相同的效果。 这一结论也能应用于染色质开放性数据，用来解释基于视觉皮质数据分解出了大量的细胞亚型。 然而，尽管可以增加scTHS的测序深度，染色质开放性数据本身用于比较精细的细胞类型分解比较困难，其灵敏度需要提高。 尽管snDrop-seq能够用于更加广泛的组织建谱，但是我们在小鼠的皮质Ast和Oli的细胞亚型的寻找过程中没有成功！！！然后分析了可能的原因。 还是解决了一些问题：细胞分型、髓鞘再生 para2 提出了一种整合转录组和表观组数据的策略，进行更加精细的细胞分型。 Glossary提供……的视角 provide insights into …综合分析 integrative analysis最终 ultimately使……成为必需 necessitate [vt]总的来说 overall与……相关 underlying解释，诠释，解调 interrogation被进一步的分解为 be further resolved into起源于 stem from / derive fromuneven/even 不均匀的，奇数/偶数notably 特别地，尤其地，引人注目地be in line with 与……一致 corresponding with 基因内区域 intronic region基因间区域 intergenic region表观遗传构型 epigenetic configuration髓鞘再生 remyelination cerebellum 英 [,serɪ’beləm] 美 [‘sɛrə’bɛləm] n. [解剖] 小脑 [ 复数 cerebellus或cerebella ]temporal lobe [解剖] 颞叶]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux三个常见的压缩打包命令]]></title>
    <url>%2Fos%2F20190515-feae.html</url>
    <content type="text"><![CDATA[常见压缩文件扩展名 扩展名 解释 *.gz gzip压缩文件 *.bz2 bzip2压缩文件 *.tar tar打包的文件，没有压缩 *.tar.gz tar打包文件。经过gzip压缩 *.tar.bz2 tar打包文件，经过bzip2压缩 gzip压缩并删除本地文件：gzip -v SOURCE 压缩但保留本地文件：gzip -c SOURCE &gt; TARGET 解压缩：gzip -d SOURCE 不解压缩但查看文件内容：zcat SOURCE bzip2取代gzip，压缩比例高于gzip 压缩并删除本地文件：bzip2 -zv SOURCE 压缩但保留本地文件：bzip2 -kv SOURCE 解压缩：bzip2 -d SOURCE 不解压缩但查看文件内容：bzcat SOURCE tar将（多个）文件打包（压缩）成一个文件 压缩 -c： 创建bzip2压缩文件：tar -jcvf TARGET.tar.bz2 SOURCE 创建gzip压缩文件：tar -zcvf TARGET.tar.gz SOURCE 不解压缩查看文件列表 -t： tar -jtvf SOUTCE.tar.bz2 tar -ztvf SOURCE.tar.gz 解压缩 -x： tar -jxvf SOURCE.tar.bz2 tar -zxvf SOURCE.tar.gz]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>Linux实用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown奇淫技巧]]></title>
    <url>%2Fcargo%2F20190514-71e.html</url>
    <content type="text"><![CDATA[原生表格中插入包含 | 的代码由于markdown原生表格使用”|”分隔列，在表格中直接输入”|”会使表格结构解析错误。 下面这种写法可以使表格正常解析： 样式 代码 A &#124; B &lt;code&gt;A &amp;#124; B&lt;/code&gt;]]></content>
      <categories>
        <category>cargo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R模型公式]]></title>
    <url>%2FR%2F20190514-95ad.html</url>
    <content type="text"><![CDATA[在进行 方差分析（ANOVA）或者 回归分析 时我们常常会遇到 ~ 操作符，这对R新手来说实在是难以理解，遂查查文档整理整理。 12345aov(formula, data = NULL, projections = FALSE, qr = TRUE, contrasts = NULL, ...)lm(formula, data, subset, weights, na.action, method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...)glm(formula, family = gaussian, data, weights, subset, na.action, start = NULL, etastart, mustart, offset, control = list(...), model = TRUE, method = "glm.fit", x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, ...) R函数例如 aov(), lm(), glm() 都提供了 formula 参数供用户定义将要进行的分析中涉及到的变量（反应变量、解释变量）。这个 formula 参数直接决定了R构建和测试的模型结构。formula 参数的基本格式为： 1response_variable[反应变量] ~ explanatory_variables[解释变量] 上式中的波浪号（tilde）可以理解为“通过……建模”或者“是……的函数”。 上式的技巧多在于，如何书写解释变量。 最基础的回归分析的 formula 参数格式如下： 1y ~ x 上式中的”x” 称之为 解释变量（Explanatory Variable）或者 自变量（Independent Variable, IV）,”y”称之为 反应变量（Response Variable）或者 因变量（Dependent Variable, DV）。 如果还有其它的解释变量添加到表达式后面即可。下式表示构建两变量的多回归模型： 1y ~ x + z 类似于上面的式子我们称之为 模型公式（Model Formula），它将传递给 formula 参数。 如何书写正确优雅的模型公式是一件很有意思的事情…… 尤其注意，我们在其它地方使用的数学运算符（例如四则运算符）在模型公式里都有新的含义，也就是说我们不能像读一般数学公式那样去阅读模型公式。 下面这张表列出了模型公式中常用符号的意义，我们可以直观地感受到它们与一般意义地显著差别： 符号 例子 含义 + + x 包含了解释变量x - - x 删除解释变量x（没太理解） : x : z 包含了解释变量x和z间的互作 * x * z 包含了解释变量x和z，以及它们之间的互作 / x / z 嵌套：包含了嵌套在x中的z &#124; x &#124; z 条件：包含了给定z时的x ^ (u + v + w + z) ^ 3 包含了四个变量，以及它们之间最多三个变量间的互作 poly poly(x, 3) 多项式回归：正交多项式 Error Error(a/b) 指定误差项 I I(x*z) I() 中的表达式保留一般的数学意义，表示包含了x乘以z这个新变量 1 (数字) - 1 截距：删除截距，即通过原点回归 同一个模型可以通过不同的公式表达： 123456789# model 1y ~ u + v + w + u:v + u:w + v:w + u:v:wy ~ u * v * wy ~ (u + v + w)^3# model 2y ~ u + v + w + u:v + v:w + v:wy ~ u * v * w - u:v:wy ~ (u + v + w)^2 解释变量的属性（例如二值变量、分类变量、数值变量……）决定了模型的特性，例如对公式 y ~ x + z： 如果x和y是两个分类变量，该公式表示方差分析[?] 如果x和y是两个数值变量，该公式表示多回归[?] 如果一个是数值变量一个是分类变量，则表示相关性分析[?] 关于 Error() 的一点见解 略]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Basset：CNN学习新的染色体开放位点]]></title>
    <url>%2Fbioinfo%2F20190423-1702.html</url>
    <content type="text"><![CDATA[尝试着将神经网络的元件与生物学意义联系起来。大胆假设，小心求证！ PMC | Genome Res. | GitHub 数据下载从ENCODE Project Consortium下载125种细胞类型的数据。从Roadmap Epigenomics Consortium下载39种细胞类型的数据。数据形式为DNase-seq的peak信息，保存在BED格式的文件中。使用未去重叠（overlap）的peak数据。 预处理 以1%的FDR使用模拟方法修改原始数据集——robustness 归并重叠的peaks共 $2,071,886$ 个峰，比对到hg19参考基因组 标准输入为每个位点600bp的DNA序列长度 标准标签为一个164维的二值向量，该向量表示这个峰（位点）在164种细胞类型中的开放情况（1为开放，0为不开放） 将数据集切分为训练集、测试集和验证集 training data：训练模型参数 testing data：计算序列特异性参数 validation data：用于early stopping 应模型后续分析需要，使用GENOME v18 reference catalog将位点分为promoter（转录起始位点周围2kb区域）、intragenic（与基因区域发生重叠）、intergenic（位于基因间区域内）三类 模型训练模型和测试模型的用途有细微差别。 训练训练模型是一个3CNN+2FC的简单神经网络，此项目的亮点不在神经网络的设计，而在于将神经网络的元件与生物学意义结合起来，这也是以往项目中我所想不透的地方。训练网络如下： 模型的输入是一个one-hot编码的 $4 \times 600$ 的序列（与处理中已经将所有位点的序列长度截取到了600bp）。 the first CNN是本模型的重点，它包括了一个卷积层、一个激活层和一个池化层。 卷积层使用的是300个 $4 \times M$ 的一维卷积核（filter，滤波器），其中 $M$ 长度跟motif长度相当（这里取19b）。作者在这里给每个filter赋予了生命力，认为它们不仅是一种网络元件。因为每个卷积核是基于所有序列优化得来的，所以我们认为卷积核代表了所有序列共有的一种信息，即模式。这是符合思考逻辑的。 简而言之，每个卷积核可能代表了一种motif，这个motif具体的生物学意义未知。此项目试图从蛋白质结合motif出发去验证卷积核中是否存在相应的结构与之对应。 这是一种不错的假设-验证思想。 模型的最后一层也就是第二个全连接层的输出是164个节点，分别代表了该600bp的序列在164种类型细胞种的开放情况。参考标准标签计算binary loss进行参数优化。 测试测试阶段作者做了很有意思的事情。 为了计算卷积核（motif）对（预测的）开放性打分结果的影响程度，将第一CNN的输出结果（每个卷积核会将 $4 \times 600$ 的序列转化为（600-19+1）维的向量）全部用其平均值替换，这将消除卷积核的特异性影响。用替换结果进行后续计算，将计算结果与未经替换的预测结果进行比较。计算差异的平方和，这个值作为该卷积核的influence值。 IC（Information Content）值得原文计算方法如下：$$\mathrm{IC} = \sum_{i,j}{m_{ij}\log_2 m_{ij}} - \sum_{i,j}{ b_j \log_{2}b_{j}}$$写成这样似乎更清楚点：$$\mathrm{IC} = \sum_{i=1}^{19} \sum_{j=1}^4 m_i^{(j)} \log_2 m_i^{(j)} - 19\sum_{j=1}^4 b^{(j)} \log_2 b^{(j)}$$ 其中向量 $b$ 是四个碱基分布的背景值，向量 $m\;(4 \times 19)$ 是motif各个位置的碱基概率分布，等效于一个卷积核。 有趣的是，除了对卷积核进行上述操作计算IC值influence值，作者还直接从数据库中（CIS-BP数据库）下载已知的蛋白质motif代替卷积核进行计算，最后一同进行比较分析。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>基因组开放位点</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样5 - M-H采样解决接受率α过小的问题]]></title>
    <url>%2Fstat%2F20190422-551c.html</url>
    <content type="text"><![CDATA[M-H采样 是 Metropolis-Hastings采样 的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。 M-H采样解决了原始MCMC采样中接受率过小的问题。 对于细致平稳条件$$\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji}$$既然 $\alpha_{ij}​$ 太小，我们就把它扩大到1，多么简单粗暴：$$\alpha(i, j)=\min \left \lbrace \frac{\pi(j) Q(j, i)}{\pi(i) Q(i, j)},\; 1 \right \rbrace$$M-H采样过程如下： 已知平稳分布 $\pi(x)​$，预设状态转移次数阈值 $n​$ 和采样样本数 $m​$，选定任意的某个马氏链对应的状态转移矩阵 $Q(i,\;j)​$ 从任意简单概率采样得 $x_0$ 从 $t=1$ 到 $t=n+m-1$ 循环采样： 从 $q(x|x_{t-1})$ 中采样得到状态 $\ast$ 的采样值 $x^\ast$ 从均匀分布中采样得到 $u \sim uniform(0,\;1)$ 如果 $u \lt \alpha(t-1,\;\ast)=\min \left \lbrace \frac{\pi(\ast) Q(\ast, \;t-1)}{\pi(t-1) Q(t-1,\;\ast)},\; 1 \right \rbrace$ ，则接受采样值 $x^\ast$，这意味着我们接受了状态 $t-1$ 到状态 $\ast$ 的转移，即 $x_t=x^\ast$；否则拒绝转移，即 $x_t=x_{t-1}$ $\left \lbrace x_n,\; x_{n+1},\; \cdots,\; x_{n+m-1} \right \rbrace$ 即为平稳分布对应的采样集 一般强化先验知识有助于简化计算。如果我们预设的 $Q​$ 是对称矩阵，即 $Q(i,\;j)=Q(j,\;i)​$，那么此时 $\alpha​$ 可以简化为$$\alpha(i,\;j) = \min \left \lbrace \frac{\pi_j}{\pi_i},\; 1 \right \rbrace$$尽管M-H采样解决了[原始MCMMC采样][]过小的问题，它还是有很多缺陷，特别是在特征维度很大时： 接受概率 $\alpha$ 难以计算，或进行了许多的无用计算（拒绝采样）； 联合分布难以计算，相比之下条件概率分布计算更容易 Gibbs采样解决了上面的问题。]]></content>
      <categories>
        <category>stat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[采样4 - 细致平稳条件和MCMC采样]]></title>
    <url>%2Fstat%2F20190417-ea8c.html</url>
    <content type="text"><![CDATA[细致平稳条件&gt; 细致平稳条件（Detailed Banlance Condition） 当一个系统微观上达到平衡时满足 细致平稳条件：$\pi_i​$ 表示系统处于状态 $i​$ 的概率，$P_{i{\to}j}​$ 表示系统从状态 $i​$ 转移到状态 $j​$ 的概率，一个反应（转移）应该与它的逆反应（逆转移）达到平衡，即$$\pi_i P_{i \to j} = \pi_j P_{j \to i} \tag{1}$$细致平稳条件 是 Ludwig Boltzmann 于1872年提出来的。 &gt; 马尔可夫链具有平稳分布的充分条件 假设马尔可夫链的状态分布为 $\pi$，状态转移矩阵为 $P$，如果存在某一个时间节点使得对于任意的状态 $i$ 和 $j$ 有$$\pi_i P_{ij} = \pi_j P_{ji} \tag{2}$$上述细致平稳条件成立，则该马尔可夫链具有平稳分布 $\pi$（Stationary Distribution）。 细致平稳条件是马氏链收敛的充分条件，而不是必要条件。 充分性证明： 因为 $\sum_i \pi_i P_{ij} = \sum_i \pi_j P_{ji} = \pi_j \sum_i P_{ji} = \pi_j$ 所以 $ \sum_i \pi_i P_{ij} = \pi P = \pi $ 仅在二状态系统下，细致平稳条件也是马氏链收敛的必要条件，即充要条件。 &gt; 从目标平稳分布 $\pi(x)$ 出发寻找转移矩阵 $P$ 假设目标平稳分布 $\pi(x)$ 已知，根据细致平稳条件，我们只需要确定一个矩阵 $P$ 作为相应的状态转移概率矩阵，就能确定这样一个马尔可夫链：转移矩阵为 $P$，平稳分布为 $\pi(x)$。 但是，矩阵 $P$ 的确定并不是一件很容易的事情，MCMC采样算法很巧妙的解决了这个问题。 MCMC采样&gt; 马尔可夫蒙特卡洛采样（Markov Chain Monte Carlo，MCMC）的基本思想 我们随机确定一个马尔可夫链的状态转移矩阵 $Q$，它并不满足细致平衡条件，即$$\pi_i Q_{ij} \ne \pi_j Q_{ji} \tag{3}$$为了使等式成立，引入非零 $\alpha_{ij}$ 项，即$$\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji} \tag{4}$$取$$\begin{cases}\alpha_{ij} &amp;= \pi_j Q_{ji} \\alpha_{ji} &amp;= \pi_i Q_{ij}\end{cases} \tag{5}$$此时目标平稳分布 $\pi(x)$ 对应的马氏链的状态转移概率矩阵 $P$ 可表示为$$P_{ij} = Q_{ij} \alpha_{ij} \tag{6}$$由(5)式可知，$0 \le \alpha_{ij}^{(k)} \le 1$，对于特定的 $Q_{ij}$，$\alpha_{ij}$ 越小，则相应的 $P_{ij}$ 越小：这可以解释为 $\alpha_{ij}$ 是对 $Q_{ij}$ 的接受率，目标状态转移矩阵 $P$ 可以通过任意一个马尔可夫链的状态转移矩阵 $Q$ 以一定概率 $\alpha$ 接受获得。 &gt; MCMC采样算法过程 初始化状态分布 $\pi_0$ 并采样 $x_0$ 循环采样直到获得 $m$ 个样本： $t$ 时刻马氏链的状态分布为 $\pi_t$（未知，我们也不关系纯向量的值），从 $Q(x|x_t)$ 采样得样本 $x^*$ 从均匀分布中采样 $u \sim uniform(0,\;1)​$ 如果 $u &lt; \alpha_{x_t \to x^} = \pi_(x^)Q(x^,\;x_t)$ 则接受转移 $x_t \to x^$，即 $x_{t+1}=x^*$；否则不接受转移，即 $x_{t+1}=x_t$ 舍弃第 $0$ 到第 $n-1$ 共 $n$ 个样本，保留第 $n$ 到第 $n+m-1$ 共 $m$ 个样本作为采样样本 上面的 $\alpha_{ij}$ 实际上很小，这导致大部分采样都会被拒绝，这意味着我们的 $n$ 要预设的非常大。 基于这个困境，M-H采样 对 MCMC采样 进行了优化。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>采样</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样3 - 离散马尔可夫链采样]]></title>
    <url>%2Fstat%2F20190416-1f84.html</url>
    <content type="text"><![CDATA[如果我们的目标分布是简单、离散的分布，这个目标分布可以当作某个马尔可夫链的平稳分布，这个平稳分布对应着一个状态转移矩阵。如果这个状态转移矩阵已知，我们就可以十分容易的得到目标分布的采样集。 离散马尔可夫链采样只是理论基础，实用价值不大。 采样下面直接以例子说明采样过程： 假设一个质量分布不均匀的六面骰子掷出六个点数的概率服从一个简单、离散的概率分布，记为 $\pi(x)$，$x=\lbrace 1\;2\;3\;4\;5\;6 \rbrace$。 $\pi(x)$ 可以通过大量独立重复实验用频率进行估计，如果知道了状态转移矩阵 $P$ 就可以很容易的进行采样。 先假设转移概率矩阵 $P$ 已知，这里模拟一个状态转移矩阵作为示例： 12p = np.random.uniform(0, 10, size=36).reshape([6, 6])p = p/p.sum(axis=1).reshape([-1, 1]) $$p = \left[\begin{matrix}0.18206229 &amp; 0.1764297 &amp; 0.23015706 &amp; 0.13247823 &amp; 0.23143627 &amp;0.04743645 \\0.03751516 &amp; 0.33564335 &amp; 0.39400589 &amp; 0.12847041 &amp; 0.07526389 &amp;0.0291013 \\0.12921795 &amp; 0.25957537 &amp; 0.01825006 &amp; 0.32088546 &amp; 0.26899888 &amp;0.00307228 \\0.22883623 &amp; 0.07873173 &amp; 0.23422034 &amp; 0.23744235 &amp; 0.08045652 &amp;0.14031283 \\0.14770889 &amp; 0.20742323 &amp; 0.19634175 &amp; 0.07048638 &amp; 0.16482596 &amp;0.21321379 \\0.07080479 &amp; 0.24460837 &amp; 0.22450135 &amp; 0.21595409 &amp; 0.11542076 &amp;0.12871064\end{matrix}\right]$$&gt; 1. 指定初始分布 因为马尔可夫链的平稳分布只与状态转移矩阵相关，而与初始状态分布无关。 所以我们可以从一个从任意初始化状态分布出发进行采样。 12pi = np.random.uniform(0, 10, size=6)pi = pi / pi.sum() $$\pi_0 = [0.22081522,0.11290264,0.23593329,0.0524546,0.1917839,0.18610888]$$ &gt; 2. 指定记录采样值时的状态转移次数 $n$ 和需要记录的采样数 $m$ 整个采样过程中需要进行的状态转移次数为$$N= n+m-1$$将要记录下的采样值集合为$$X = \lbrace \underbrace{x_n,\;x_{n+1},\;\cdots,\;x_{n+m-1}}_{\text{共 $$m 项}} \rbrace$$采样集合 $X$ 即为平稳分布 $\pi(x)$ 对应的样本集。 实际应用中，$n$ 凭经验确定。 &gt; 3. 从初始分布采样 从初始分布采样的意思是：从简单分布（例如均匀分布）采样，以初始分布的确定采样值（常见的方法是以初始分布的累积分布函数确定采样值）。 计算累积分布函数（python中调用 np.cumsum(arr)）：$$F_0 = [0.22081522,0.33371785,0.56965314,0.62210774,0.81389112,1]$$从[0,1]均匀分布采样一次（np.random.uniform(0, 1)）得 $p_0 = 0.27364499172190815​$ 因为 $F_0^{(1)}&lt;p_0&lt;F_0^{(2)}​$，我们认为本次采样值 $x_1=2​$，即本次骰子之出来的点数是2。 &gt; 4. 基于 $P(x|x_1)​$ 进行第二次采样 $P(x|x_1=2)$ 即从状态转移矩阵 $P$ 的第二行采样$$P(x|x_1) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]$$从[0,1]均匀分布采样一次得 $p_1 = 0.23181961317728317$，$x_2=2$。 &gt; 5. 基于 $P(x|x_2)​$ 进行第三次采样 $P(x|x_2=2)$ 即从状态转移矩阵 $P$ 的第二行采样$$P(x|x_2) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]$$从[0,1]均匀分布采样一次得 $p_2= 0.5880393198555685​$，$x_3=3​$。 &gt; 基于 $P(x|x_{n-1})$ 进行第 $n$ 次采样 $P(x|x_{n-1})$ 即从状态转移矩阵 $P$ 的第 $x_{n-1}$ 行采样，得到第 $n$ 个采样值 $x_n$，这个采样值将作为我们第一个目标样本。也就是说前面的 $n-1$ 个采样会被舍弃掉。 继续采样，直到获取第 $n+m-1$ 个采样值。 挠头Q1 由上知是从矩阵 $P$ 而不是每个状态分布 $\pi$ 中采样，因为不论状态转移多少次，矩阵 $P$ 总是不变的，为什么不直接从矩阵 $P$ 中采样，而是从初始分布采样舍弃 $n-1$ 个样本？ 这个问题看似复杂，其实很简单：因为总是选取矩阵 $P$ 的某一行采样，至于要选取哪一行取决于上一次的采样结果。那么，第一次使用矩阵 $P$ 进行采样（区别于第一次采样）应该选取哪一行呢？这是不确定的。因此第一次采样我们从指定的初始分布采样，第二次采样（即第一次使用矩阵 $P$ 进行采样）基于第一次采样结果进行。 Q2 为什么要舍弃前面的 $n-1$ 个采样值？ 因为初始分布是我们自己定义的，而采样值服从的分布应该是平稳分布而不是初始分布或者达到平稳分布前的任何一个分布。 $n$ 一般是人为设定的阈值，代表第 $n$ 次转移时已经达到平稳分布，显然这不是一个十分精确的阈值，但是在实际应用过程中影响不大。 状态的分布伴随着采样过程而改变，这是一个隐藏的过程。当状态分布收敛时的采样值集为平稳分布的采样值。（这句话很玄学，暂时没太理解！） 总结上面的马尔可夫链采样的基础是状态概率矩阵 $P$ 已知，但是实际上 $P$ 是很难求出来的，即我们知道的只有平稳分布 $\pi(x)$。这似乎是一个死循环~ MCMC采样巧妙地解决了这个问题。 M-H采样和Gibbs采样是MCMC采样地改进版。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>采样</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样2 - 离散马尔可夫链的几个性质]]></title>
    <url>%2Fstat%2F20190416-ea6c.html</url>
    <content type="text"><![CDATA[互通、可约、周期、常返、遍历 状态间的互通性 互通性的三个性质： 自返律（假设状态有自环）：$\mathbf{i} \leftrightarrow \mathbf{i}​$ 对称律：$\mathbf{i} \leftrightarrow \mathbf{j}​$当且仅当$\mathbf{j} \leftrightarrow \mathbf{i}​$ 传递律：如果$\mathbf{i} \leftrightarrow \mathbf{k}$且$\mathbf{k} \leftrightarrow \mathbf{j}$，那么$\mathbf{i} \leftrightarrow \mathbf{j}$ 链的可约性不可约性：如果一个马氏链的任意两个状态都互通，则这个马氏链不可约；否则可约。 不可约的马氏链： 可约的马氏链： 状态的周期性状态的周期性有个挠头的定义（$from$ 张波《应用随机过程》）： 记$d_i$为数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$的最大公约数，则称它为状态$i$的周期。 若对一切$n{\ge}1$有$p_{i i}^{(n)}=0$，则约定$d_{i}=\infty$。 当$d_i&gt;1​$时，称$i​$是有周期的状态；当$d_i=1​$时，称$i​$是非周期的状态。 数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$指的是从状态$i$出发再次回到状态$i​$的步数的集合。 上图状态1再次回到状态1的方式有： $1{\to}1$：步数为1 $1\to2\to1$：步数为2 重复方式1，或者重复方式2，或者方式1和方式2的组合，例如$1\to1\to1$，$1\to2\to1\to1$ 因此状态1回到状态1的步数集合是{1, 2, 3, 4, 5, …}，$d_i=1$，状态1是非周期的； 状态2回到状态2的基本方式是{$2\to1\to2$：2步，$2\to1\to1\to2$：3步}，步数集合是{2，3，4，5，…}，最大公约数$d_i=1$，因此状态2是非周期的。 再看下图： 状态1自返的基本方式是： $1\to2\to3\to4\to1$，步数为4 $1\to5\to6\to7\to8\to9\to1$，步数为6 状态1自返的步数集合是{4，6，8，10，…}，最大公约数$d_i=2$，所以状态1是周期的。 状态2，3，4的步数集合是{4，8，10，12，14，…}，都是周期的。 状态5，6，7，8，9的步数集合是{6，10，12，18，…}，都是周期的。 如何简单判断一个状态是周期/非周期的？ 带有自环的状态一定是非周期的（因为$d_i=1$），但不是所有非周期的状态都有自环，如下条 与非周期状态互通的状态一定是非周期的 状态的常返性常返性即：马氏链由一个状态出发之后能否再次回归到本状态的特性。 常返性分为三种： 正常返（必定会返回，平均返回时间为有限值） 零常返（必定会返回，平均返回时间为 $\infty​$ ） 非常返（可能不再返回） 定义略 不可约马氏链的状态一致性定理：不可约马氏链的状态集全为正常返，或者全为零常返，或者全为非常返，并且每个状态的周期相同。 上图不可约马氏链中： 如果 $p&lt;q$，那么全为正常返； 如果 $p=q$，那么全为零常返； 如果 $p&gt;q$，那么全为非常返。 状态/链的遍历性如果齐次马氏链中的某个状态是非周期、正常返状态，称这个状态是可遍历的。 如果马氏链所有状态全互通（不可约）、可遍历（非周期、正常返），称这个马氏链为遍历链。 遍历链存在一个平稳分布： 平稳分布与初始状态无关 平稳分布是唯一的 平稳分布全部大于0]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>采样</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样1 - 逆变换采样和拒绝采样]]></title>
    <url>%2Fstat%2F20190415-a232.html</url>
    <content type="text"><![CDATA[蒙特卡洛方法（Monte Carlo Method）尝试利用计算机模拟随机数（伪随机数）解决一类问题，这类问题通常是：1. 所求解的问题本身具有内在随机性，例如中子与原子核的相互作用受量子力学规律的制约；2. 所求解问题可以转化为某种随机分布的特征数，例如通过撒豆子的方式计算不规则图形的面积。蒙特卡洛法是一种以概率统计理论为指导的数值计算方法。 抽样（采样）指从总体中抽取一部分作为样本。计算机模拟中，抽样意味着从一个概率分布中生成一个观察值，这涉及到一个随机的过程。一般认为计算机只能进行均匀分布的采样，对于复杂的概率分布，需要进行采样方法设计。 逆变换采样从图像上理解连续型随机变量的采样是个什么玩意儿十分形象： 左图是正态分布的概率密度函数（Probability Density Function，PDF）记为$f(x)​$，右图是正态分布的累积分布函数（Cumulative Distribute Function，CDF）记为$F(x)​$。随机变量的采样就是在$[0,1]​$均匀分布采样的基础上，选取尽可能分散的点，使这些点尽可能地拟合CDF曲线。 拒绝采样逆变换采样虽然简单有效，但是其应用场景十分有限：当累积分布函数或者反函数难求时，而实际情况往往是这样。 下图中的$f(x)$是我们采样的目标PDF，当其CDF或者CDF的反函数不容易求的时就不能直接对$f(x)$进行采样。拒绝采样（Rejection Sampling）的基本思想是借助这样一个参考概率密度函数$f_r(x)$即下图中的$Mg(x)$： $f_r(x)$十分容易进行采样，例如取均匀分布意味着参考PDF可以直接进行逆变换采样 $f_r(x)$位于$f(x)$上方，即对任意$x$有$f_r(x){\ge}f(x)$ $Mg(x)$表示将均匀分布$g(x)$向上移动，此时以$f(x)$的极大值确定$M$的值效果比较好 从图上来看，参考PDF“罩住”了目标PDF： 拒绝采样的过程如下： 从$f_r(x)$进行一次采样$x_i$ 计算$x_i$的接受概率$\alpha$（Acceptance Probability）：$$\alpha=\frac{f\left(x_{i}\right)}{f_r\left(x_{i}\right)}$$ 从$(0,1)$均匀分布中进行一次采样$u$ 如果$\alpha{\ge}u$，接受$x_i$作为一个来自$f(x)$的采样；否则，重复第1步 显然对于特定的目标PDF，参考PDF不止一个，不同PDF的$\max(\alpha)$不同。以均匀分布采样为例，当参考PDF从上面越靠近目标PDF采样效率越高，相应的寻找这样的参考PDF的难度就越大。采样效率高意味着对于那些概率密度较小的区域有更大的几率能够采样到。 为了平衡采样效率和参考PDF的确定难度，提出了自适应拒绝采样。 自适应拒绝采样当参考PDF不能很好的“罩住”目标PDF时，那些未罩住区域内的采样点被拒绝的概率就会很大，采样效率低。所以如果能够找到一个跟目标PDF非常接近的参考PDF，即参考PDF计划能够完全从上面贴合目标PDF，此时能够达到较好的采样效率。 当目标PDF是log-concave函数时可以采用自适应拒绝采样（Adaptive Rejection Sampling，ARS）。 log-concave函数：当概率密度函数$f(x)$是凹函数（concave）且$\log{f(x)}$仍然是凹函数时，$f(x)$称之为log-concave函数：$$f(\theta x+(1-\theta) y) \geq \theta f(x)+(1-\theta) f(y) \\log f(\theta x+(1-\theta) y) \geq \theta \log f(x)+(1-\theta) \log f(y)$$ 在log-concave函数上随机选取一些点做切线： 将log-concave函数变换回原来的PDF，此时上图的切线将变成曲线（取指数），它！们！弯！了！ 将这组弯了的“切线”组成成一个分段函数，这个分段函数将会很好的贴合目标PDF。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>采样</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git疑难杂症]]></title>
    <url>%2Fbug%2F20190415-bb05.html</url>
    <content type="text"><![CDATA[报错关键词：Updates were rejected because a pushed branch tip is behind its remote counterpart. 原因分析：直接在远程master分支进行修改或者有其他人修改后已经提交到了远程master，而本地使用test（非本地master分支）分支进行再次修改后直接push到远程master分支，此时本地master分支的版本还是远程master分支修改之前的版本，即本地master的版本落后于远程master的版本，因此导致push失败。 解决办法：先 git checkout master 到本地master分支，再 git pull 远程仓库 master 拉取最新版本，再 git checkout test 回到本地工作分支，再 git push 远程仓库 master 推送最新版本。]]></content>
      <categories>
        <category>bug</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python3中的字符串与编码问题]]></title>
    <url>%2Fpython%2F20190411-d7e5.html</url>
    <content type="text"><![CDATA[python3的编码方式比python2已经简单很多了，不过还是让我等菜鸟头疼。廖老师的教程写的着实通俗易懂，恍然大悟。 ASCII、Unicode和UTF-8所有的字符串都会拆分成单个字符进行传输和存储。对字符串编码不仅要考虑字符集的完整性、不同语言字符集的兼容性，还要考虑传输存储空间大小。 ASCII ASCII全称是American Standard Code for Information Interchange (美国信息交换标准代码)。 计算机是美国人发明的，因此早期的ASCII编码只考虑到26个英文字母和一些简单的字符。由于数量较少，我们只需要用一个字节就可以完成编码，所以ASCII编码对应的整数值从0到255。 Unicode ASCII编码不能用来编码中文字符，当然也不能用来编码日文字符、韩文字符和阿拉伯文字符等等等等。如果每个国家都为了是计算机能识别自己特有的字符而使用自己的编码方式，当处理多国语言文本时就会出现许多问题。所以大家统一规定了Unicode编码。换句话说，Unicode编码兼容地球上所有国家的语言符号！真是一个令人振奋的消息~ 一般情况下，Unicode使用两个字节对字符进行编码（对于某些十分偏僻的字符可能要用到更多的字节）。对于原来的ASCII编码的字符，在它们的ASCII编码（二进制编码）前加上一个字节的0（即8个0）即可完美转化为Unicode编码。 字符 ASCII编码 Unicode编码 UTF-8编码 A 01000001 00000000 01000001 01000001 中 不能编码 01001110 00101101 11100100 10111000 10101101 好了，现在通过Unicode能够兼容所有语言的字符集了。但是又出现了新的问题：当我的文本（几乎）是全英文是，如果直接使用Unicode编码文件，其传输的字节数量和占用的磁盘空间都是ASCII编码的两倍，显然十分多余。为了解决这个问题，又出现了UTF-8编码。 UTF-8 UTF-8编码是不定长的编码方式：那些可以用ASCII编码的字符转化成UTF-8编码时仍然只有一个字节，UTF-8编码的汉字一般是三个字节，少数特殊字符所用的字节数更多。所以，UTF-8是完美兼容ASCII编码的，只支持ASCII编码的应用仍然可以在UTF-8编码上运行。 总结 总结以下： ASCII编码只适用于英文字符，固定单字节； Unicode编码是内存中的编码方式，固定多字节； UTF-8编码优化了Unicode编码，不定字节； 计算机编码工作方式字符串所在的场景不外乎两种： 当我们在python程序中处理字符串时，它们存储在内存中，此时使用Unicode编码字符串； 当我们存储字符串时往往使用UTF-8等编码方式（建议只适用UTF-8编码存储文件）。 场景1：记事本编辑文件 文本文件通常以UTF-8编码存储在磁盘上，记事本应用读取文件到内存中，此时字符串改用Unicode编码，当我们编辑完成之后需要使用UTF-8编码文件保存到磁盘。 场景2：网页浏览 当我们浏览某些网页的时候，服务器会自动生成信息，这些字符串是Unicode编码；服务器使用UTF-8编码这些字符串传输至浏览器并显示。 python3中的字符串python中字符串类型是str，它们以Unicode方式编码。 字符通过Unicode编码可以与数字（二进制、十进制、十六进制…）进行转换： 1234567891011## 获取字符的Unicode编码的十进制整数值ord('A') # 65ord('中') # 20013## 将十进制整数通过Unicode编码方式转化为字符chr(65) # 'A'chr(20013) # '中'## 注意：# ord()只能转换单个字符，传递字符串会报错# 传入chr()接收的整数如果超过Unicode编码的范围也会报错 我们在程序中处理的字符串都是Unicode编码的，一个字符占据了多个字节。 当进行网络传输或者本地存储时，str类型的字符串需要被转换为字节数据： 1234567891011# 单单一个'中'是Unicode编码的，直观上就没有反映出字节信息&gt;&gt;&gt; '中'.encode('utf-8')b'\xe4\xb8\xad'# 从encode结果我们可以看到'中'包含了三个字节&gt;&gt;&gt; '中'.encode('gbk')b'\xd6\xd0'# 下面两种方式军会报错LookupError&gt;&gt;&gt; '中'.encode('unicode') # 因为'中'本来就是Unicode编码了&gt;&gt;&gt; '中'.encode('ascii') # ASCII不能编码汉字字符 python中 str.encode(CODING_TYPE)方法可以将Unicode字符串转换成CODING_TYPE编码的字符串； bytes.decode(ORIGIN_TYPE)方法将ORIGIN_TYPE编码的字节串解码成Unicode字符串。 encode的参数是目标编码方式，decode的参数是源字节串的编码方式。 12345678910111213141516&gt;&gt;&gt; b'\xe4\xb8\xad'.decode('utf-8')'中'&gt;&gt;&gt; b'\xd6\xd0'.decode('gbk')'中'## .decode(..., errors='ignore')可忽略掉部分不能正确解码的字节# \xe4\xb8\xad 被忽略&gt;&gt;&gt; b'\xe4\xb8\xad\xd6\xd0'.decode('utf-8', errors='ignore')# \xd6\xd0 被忽略&gt;&gt;&gt; b'\xe4\xb8\xad\xd6\xd0'.decode('gbk', errors='ignore')## 下面两种是错误的&gt;&gt;&gt; b'\xe4\xb8\xad'.decode('gbk')&gt;&gt;&gt; b'\xd6\xd0'.decode('utf-8') len(...)函数接收字符串时计算的是字符串的字符数目，接收字节串时计算的是字节串的字节数。 123len('中国') #=&gt; 2len('中国'.encode('utf-8')) #=&gt; 6，等价于 len(b'\xe4\xb8\xad\xe5\x9b\xbd')len('中国'.encode('gbk')) #=&gt; 4，等价于 len(b'\xd6\xd0\xb9\xfa') .py源文件也是文本，我们在保存是应该保存为UTF-8编码。为了使python解释器能正确读取源文件，我们需要指定解释器读取源文件的编码方式： 12#!/usr/bin/env python3# -*- coding: utf-8 -*- notepad++应该选择 Encoding in UTF-8 without BOM 编码文件！ 中文文件的读写1234567891011# encoding参数指定原文件的编码格式，可设置errors='ignore'with open(INPUT_FP, encoding='utf-8') as reader: OUTPUT_STRING = ... # encoding指定新文件的编码格式with open(OUPUT_FP, 'w', encoding='utf-8') as writer: writer.write(OUTPUT_STRING) # 不要将这里的encoding参数和上面的.encode/.decode混淆：# |- encoding参数指定输入文件/输出文件的编码类型，一般在读文件（本地/网络）的时候才用到# |- .encode/.decode方法指定unicode编码字符串与其它编码字符串间的转换方式 参考自廖雪峰的python教程-python基础-字符串和编码]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[灵感源自macaron让人十分舒适的十种颜色]]></title>
    <url>%2Fcargo%2F20190410-3bf3.html</url>
    <content type="text"><![CDATA[Macaron是一种用蛋白、杏仁粉、白砂糖和糖霜制作，并夹有水果酱或奶油的法式甜点。口感丰富，外脆内柔，外观五彩缤纷，精致小巧。Macaron 10 色因为它们的视觉舒适而广为流传。 英文名 代码 视觉改释 bewitched tree #19CAAD 或 rgb(19,202,173) mystical green #8CC7B5 或 rgb(140,199,181) light heart blue #A0EEE1 或 rgb(160,238,225) glass gall #BEE7E9 或 rgb(190,231,233) silly fizz #BEEDC7 或 rgb(190,237,199) brain sand #D6D5B7 或 rgb(214,213,183) mustard addicted #D1BA74 或 rgb(209,186,116) magic powder #E6CEAC 或 rgb(230,206,172) true blush #ECAD9E 或 rgb(236,173,158) merry cranesbill #F4606C 或 rgb(244,96,108)]]></content>
      <categories>
        <category>cargo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[降维04 - TSNE引领时尚]]></title>
    <url>%2Fml%2F20190328-acd8.html</url>
    <content type="text"><![CDATA[t-SNE (t-distributed Stochastic Neighbor Embedding) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。 相比于SNE，t-SNE的主要优化有：联合概率替代条件概率、低维空间下使用t分布代替高斯分布。[1] 背景可视化早期的可视化 (Visualization) 工具不负责解释数据，这就限制了这些工具在真实世界数据上的应用，因为我们要想解释数据，我们还是只能靠人眼看。相比于能解释数据的监督学习而言，可视化只需要展示训练数据，而不需要训练模型使它能够拟合到测试数据集。可视化的任务简单许多。[2] 线性降维 将数据从高位空间映射到低维空间的过程我们称之为 map，相应的，低维空间中的映射点被称之为 map points。降维算法已经注意到，要将高维空间中的数据结构问题尽可能的保留到低维空间。 但是传统的线性降维 (Linear dimentionality reduction) 算法，例如PCA、MDS，更加侧重在低维空间中保持高维空间中的差异性，即尽可能地分开数据。同时它们更加关注数据地全局特征，这点与非线性降维算法显著不同。 非线性降维大部分非线性降维 (non-linear dimentionality reduction) 算法关注的是 在低维空间中保持高维空间地局部特征。这就意味着，它们不能同时关注数据的全局特征和局部特征。全局特征就是基于所有数据进行的解释，比如聚类结果就是基于所有数据进行的，理想情况下每个数据点都能找到它自己所属的类；局部特征只是基于部分数据点进行的推导，比如在SNE算法中，总是计算离中心点欧式距离小的部分点进行下降，它关注的是以中心点为圆心，以有限长度为半径的（超）球体内的点。 下面7个常见的非线性降维算法，它们在局部特征提取上都是很优秀的：Sammon mapping [3], CCA [4], SNE [5], Isomap [6], MVU [7], LLE [8], Laplacian Eigenmaps [9]。 t-SNEt-SNE继承自SNE算法，同样是非线性降维，它的优势在于：能够保持大部分局部特征到低维空间，同时不丢失全局特征（例如聚类）。 与SNE一样，t-SNE的思想还是计算两个点间的相似度 (similarity)。 方法论SNE尽管能得到比较好的可视化结果，但是它的损失函数难以优化，并且还存在 crowding problem (拥挤问题) 。相比之下，t-SNE能缓和上面提到的所有问题（优化问题和拥挤问题），与SNE相比，t-SNE主要在两个方面进行改进：1.使用对称的损失函数，新的损失函数求导会更加容易。[10]2.计算低维空间中两点的相似度使用t分布而不是高斯分布，t分布是一种重尾分布 (heavy-tailed distribution)，它能够有效缓解拥挤问题和优化问题，后面将会详细介绍。 优化SNE成对称结构联合概率替换条件概率 在SNE中我们通过条件概率分别计算高维空间和低维空间中点对间的相似度：$$\begin{cases}&amp; p_{ij}=p(x_j|x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{k{\ne}i}{\exp(-||x_i-x_k||^2)}} \\&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}}\end{cases}$$ 然后在t-SNE中我们将条件概率换成联合概率：$$\begin{cases}&amp; p_{ij}=p(x_j,x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{m{\ne}n}{\exp(-||x_m-x_n||^2)}} \\&amp; q_{ij}=q(y_j,y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}\end{cases}$$ 注意上面两种表述方式的分母的差异： 条件概率的分母是中心点 $x_i$ 与其它所有点的相似度之和; 联合概率的分母没有中心点一说，计算的是所有点对（n个数据点有 $C_n^2$ 个点对）的相似度之和。 条件概率中 $p_{ij}{\ne}p_{ji}$，而联合概率中 $p_{ij}=p_{ji}$（q同理），这正好也与分母的这种差异吻合。 注意到联合概率算法会产生一个条件概率算法不会遇到的问题：离群点。 观察上面的联合概率公式，对于离群点 $x_i$，所有与它配对计算出来的 $p_{ij}$ 或者 $p_{ji}$ 的 $||x_i-x_j||^2$ 将会特别大，这导致 $p_{ij}$ 或者 $p_{ji}$ 总是特别的小，即与 $x_i$ 相关的 $p_{ij}$ 或者 $p_{ji}$ 在对损失函数的贡献总是特别小。这相当于自动减小了那些低密度区域的点在损失函数中的权重，使得通过相似性确定离群点在低维空间中的位置更加困难。 所以呢，必须想办法消除这种效应，增大离群点在损失函数中的比重，文中用用条件概率公式代替上述 $p_{ij}$ 的定义，即：$$\begin{cases}&amp; p_{ij}=\frac{p(x_j|x_i)+p(x_i|x_j)}{2n} \\&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}\end{cases}$$ 这样每个点 $x_i$ 对损失函数的贡献度 $p(x_i)=\sum_jp_{ij}&gt;\frac1{2n}$，这就保证了离群点的贡献不会太少。 KL散度作为损失函数 t-SNE仍然使用KL散度作为损失函数，所不同的是，这里求的是两个联合概率分布之间的散度：$$C=KL(P||Q)=\sum_i\sum_jp_{ij}\log{\frac{p_{ij}}{q_{ij}}}$$ 此时KL损失函数求导的结果更加简洁：$$\frac{\partial{C}}{\partial{y_i}}=4\sum_k(p_{ik}-q_{ik})(y_i-y_k)$$ SNE求导结果为：$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$ 解决SNE的拥挤问题什么是拥挤问题流形的直观理解manifold的Wiki解释： In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. 而中文概念“流形”是由北大已故数学教授江泽涵先生提出来。江老的堂姐夫是胡适？… … 不过“流形：这个词真的很艺术，我初次见到时就感叹其形象而不能自已。流形的Wiki中文解释： 是局部具有欧几里得空间性质的空间，是欧几里得空间中的曲线、曲面等概念的推广。欧几里得空间就是最简单的流形的实例。地球表面这样的球面则是一个稍微复杂的例子。一般的流形可以通过把许多平直的片折弯并粘连而成。 为什么说二维流形面上的点距容易建模 (model)？ 这个问题直观上理解最是简单。首先对于欧几里得空间，我们普通人类最多能直观感受到三维。换算成黎曼空间，就意味着我们只能在三维空间中直观感不超过二维流形曲面的存在，二维流形曲面上的距离就是曲面内连接它们的最短曲线长度。经典的二维流形曲面如下（Swiss Roll 流形, Swiss Roll dataset）： 为什么存在拥挤为了便于可视化，我们会将高维流形上的点映射到二维空间，同时最大程度的保留它们的相对位置（这种每个点相对于整体数据点的定位就是一种全局特征）。然而这种映射是很难完美实现的，举个例子，十维空间（欧几里得空间或者黎曼空间）中可以很容易找到11个相互等距的点（就好比二维空间中能轻易找到三个相互等距的点一样），然而映射到二维空间是不可能找到11个相互等距的点的，势必会有一些点会相互靠近挤在一起，如下图所示：以二维相互等距的三个点映射到一维空间为例，无论怎么努力，三点都不可能再等距。归根揭底，不同维度空间内的距离分布是不同的，降维映射难免尽如人意。 再以球内区域为例解释crowding现象：以数据点 $x_i$ 为中心的球的体积与 $r^m$ 直接相关（ $r$ 是半径，$m$ 是球所在空间的维度）。如果在十维流形曲面上数据点均匀分布在这个球中，我们试图在二维流形曲面上以 $y_i$ 为中心对与 $x_i$ 相关的两两距离进行建模。此时我们就会遇到传说中的拥挤问题：与容纳中心点附近数据点的区域相比，容纳适中距离数据点的区域显得不够用。 在均匀分布的条件下，等距点的数量与半径相关，距离越大数量越多，这意味着映射到低维空间就会越“挤”。因此如果我们想要较为准确的在二维流形曲面中对以 $x_i$ 为中心的两两距离建模，我们就必须把距离 $x_i$ 适中位置的那些点往更远的地方推置（因为太挤了）。 不只是SNE，其它局部特征提取算法例如Sammon mapping等也都面临着拥挤难题。 怎么解决拥挤问题一种叫做UNI-SNE的改良算法[10]提出了一种解决办法：给每一个两两相似度添加一个背景值，背景值采样自均匀分布并以一定的比例 $\rho$ 进行混合。由于每个点对之间都引进了背景值，因此不管低维空间中两个映射点离的多么远，$q_{ij}$ 永远不会小于 $\frac{\rho}{n(n-1)/2}$（n个数据点可组合成 $C_n^2$ 个点对）。 引进背景值导致对于高维空间中相距很远的两个数据点总有$q_{ij}&gt;p_{ij}$（$p_{ij}{\rightarrow}0$ 时 $q_{ij}{\rightarrow}\frac{\rho}{n(n-1)/2}$），这表示低维空间中点对并没有完全拟合高维空间的点对相似度，映射后相似度变小。 尽管UNI-SNE的效果比SNE好，但是其损失函数却很难优化。目前较好的优化UNI-SNE的方法是：开始的时候将背景值混合比例设为0，这实际上等效于运行SNE；当SNE开始使用模拟退火策略时增大背景值的混合比例，促进自然分类间的gaps形成。 直接优化UNI-SNE并不可行，因为低维空间中两个相距很远的映射点的 $q_{ij}$ 几乎全部来自于背景值，即高维空间中相应两点间（即使他们的 $p_{ij}$ 很大）的距离对 $q_{ij}$ 的影响很小，这使得映射后的两点间的 $q_{ij}$ 没有什么实际意义。这表示，如果一个自然类的两部分在优化早期就分开了，就再也不会再聚合在一起了。 低维空间采用柯西分布表达联合概率UNI-SNE通过添加背景值使低维空间中相距甚远的 $q_{ij}$ 不至于趋近于0。 本文提出了一种新的解决办法，采用与高斯分布性质极其相似的重尾分布计算联合概率。右重尾分布使得当随机变量取值很大时其对应的概率值高斯分布要大，典型的重尾分布是t分布，如下图所示：t分布实际上是不同方差的高斯分布的混合分布，它的性质与高斯分布十分接近，而且更加容易计算：因为高斯分布涉及到指数运算，而t分布只需要求倒数。 这里采用的是自由度 $\nu=1$ 的t、分布，又叫做柯西分布，其概率密度函数如下：$$f(x;x_0,\gamma)=\frac1{\pi\gamma[1+(\frac{x-x_o}{\gamma})]^2}$$ 取 $x_0=0, \gamma=1$ 得标准柯西分布：$$f(x;0,1)=\frac1{\pi[1+(x-x_0)]^2}$$ 用标准柯西分布表示联合概率：$$q_{ij}=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum_{m{\ne}n}{(1+||y_m-y_n||^2)^{-1}}}=\frac{\sum_{m{\ne}n}(1+||y_m-y_n||^2)}{1+||y_i-y_j||^2}$$ 求导结果如下：$$\frac{\partial{C}}{\partial{y_i}}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}}$$ Pseudo code下面是精简版t-SNE算法伪代码，非常简洁： 优化方法添加微小的动量项可以减少到达最优解的迭代次数 精简版的t-SNE算法采用适应性学习率加速训练：在梯度较稳定的方向上增大学习率。[11] 尽管精简版算法已经可以吊打其它非参数降维技术了，还是可以继续优化，文中提出了两个技巧： 1. early compression：优化起始的时候将所有的映射点初始化在原点附近，有利于映射点移动、形成自然类。early compression通过给损失函数加上一个L2惩罚项实现。2. early exaggeration：在优化的初始阶段将所有的 $p_{ij}$ 扩大指定倍数加快收敛速度。 总结一下模型优化的参数配置： 起始的50次迭代中将所有的 $p_{ij}$ 乘以4（这个步骤在精简版算法的伪代码中没有写出来）； 梯度下降的迭代轮数T设为1000； 动量项 $\alpha^{(t)}$ 当 $t&lt;250$ 时设为0.5，当 $t{\ge}250$ 时设为0.8； 学习率初始值设为100，每次迭代都将进行适应性更新 [11]。 算法的Matlab实现 不足作者分析了三个不足之处。 1. 不能用于低维空间超过三维的情况 因为t-SNE算法在映射空间利用了柯西分布的重尾特性解决拥挤问题，柯西分布是自由度为1的t分布，这种特性在二维空间中表现十分优异。但如果需要降到三维以上的映射空间，1自由度的t分布不能很好的保留局部特征，我们可能需要使用更多自由度的t分布。 2. 本征维度诅咒 t-SNE虽然能够保留全局特征，但是总体上还是基于局部特征进行的降维，这表示t-SNE对原始数据的 本征维度 (intrinsic dimentionality) 十分敏感，因为本征维度过高，我们就不能再把流形曲面的局部区域当欧几里得空间处理了，数据点间的局部特征更加复杂 [12]。不仅t-SNE，其它主流的基于局部特征提取的降维算法（如Isomap，LLe）都面临着这个诅咒。 作者提出了一种可行的办法：先用 自编码器 (autoencoder)[13] 对数据进行压缩，这类模型可以大大降低原始数据的维度，同时最大保留高维数据的特征。经过编码的数据再进行t-SNE降维。 3. 损失函数不凸~ 不幸的是当前主流降维算法使用的损失函数都是凸函数，而t-SNE优化的超参更多，这使得其损失函数是非凸的。这意味着，不同的超参取值、不同的初始化都可能收敛到不同的（局部最优）解。但是作者表示，如果固定这些超参，t-SNE就可以应用于不同的可视化任务，优化结果不会随着不同批次而发生变化。 t-SNE降维结果中点间的距离是没有实际意义的。原始的t-SNE训练很慢，后面有许多改进，比如 multiple maps of t-SNE parametric t-SNE … … 引用[1] Maaten, L. V. D., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.[2] M.C. Ferreira de Oliveira and H. Levkowitz. From visual data exploration to visual data mining: A survey. IEEE Transactions on Visualization and Computer Graphics, 9(3):378–394, 2003.[3] J.W. Sammon. A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, 18(5):401–409, 1969.[4] P. Demartines and J. Herault. ´ Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets. IEEE Transactions on Neural Networks, 8(1):148–154, 1997[5] G.E. Hinton and S.T. Roweis. Stochastic Neighbor Embedding. In Advances in Neural Information Processing Systems, volume 15, pages 833–840, Cambridge, MA, USA, 2002. The MIT Press.[6] J.B. Tenenbaum, V. de Silva, and J.C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.[7] K.Q. Weinberger, F. Sha, and L.K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the 21st International Confernence on Machine Learning, 2004.[8] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by Locally Linear Embedding. Science, 290(5500):2323–2326, 2000.[9] M. Belkin and P. Niyogi. Laplacian Eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, volume 14, pages 585–591, Cambridge, MA, USA, 2002. The MIT Press.[10] J.A. Cook, I. Sutskever, A. Mnih, and G.E. Hinton. Visualizing similarity data with a mixture of maps. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, volume 2, pages 67–74, 2007.[11] R.A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1: 295–307, 1988.[12] Y. Bengio. Learning deep architectures for AI. Technical Report 1312, Universite´ de Montreal, ´ 2007.[13] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>降维</tag>
        <tag>t-SNE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维03 - SNE原理]]></title>
    <url>%2Fml%2F20190325-80ae.html</url>
    <content type="text"><![CDATA[Hinton, G. E., &amp; Roweis, S. T. (2003). Stochastic neighbor embedding. In Advances in neural information processing systems (pp. 857-864). 随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE) 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 流形先看一张图 上图描述了一个曲面，颜色相同的点聚为一类。 曲面上有三个点$A$, $B$, $C$，我们将该曲面置于三维空间中，曲面上每个点都可以用一个三维坐标$(x,y,z)$表示。 考虑三维空间中的欧式距离，有$\overline{AC}&lt;\overline{AB}$，这表示相对于$B$点而言，$C$点离$A$点更近。 但是很明显$A$点更倾向于与$B$点聚为一类。 我们用曲面上两点间最短线段的长度来表示两点间的距离，如图$\widehat{AB}$和$\widehat{AC}$，有$\widehat{AB} &lt; \widehat{AC}$，这表示在曲面空间上$B$点离$A$点更近。 我们在三维空间中去描述这个曲面的特征，我们使用的是 欧式空间（欧几里得空间）； 我们直接在曲面上去描述点与点的远近，我们使用的是 黎曼空间（流行空间）。 流行学习 的观点认为，我们观察到的数据实际上是 低维流形 映射到 高维空间 中的形式，因为维度间的冗余，高维数据都能用较低的维度去描述。 流形学习的基本思想 定义条件概率即使是不能直观感受的高维空间中，如果两个点无限靠近，那么它们的 欧式距离 也趋近于0。我们可以用 欧式距离 来描述高维空间中两个点的相似性。 对于 $R^n$ 空间中某个点 $x_i$, 我们基于 欧式距离 定义任意点 $x_j \in R^n$ 与 $x_i$ 的 相似度：$$d^2(x_i,x_j)=\frac{||x_i-x_j||^2}{2\sigma_i^2}$$上式中 $\sigma_i^2$ 是样本空间中除 $x_i$ 外其它所有点到 $x_i$ 距离的 方差。 $d^2(x_i,x_j)$ 是 标准化 后的结果，它表示了点 $x_i$ 与点 $x_j$ 的相似程度。 SNE将 $d^2(x_i,x_j)$ 转化为 条件概率 来描述点 $x_i$ 选择点 $x_j$ 作为邻居的概率，记为 $p(x_j|x_i)$：$$P_{ij}=p(x_j|x_i)=\frac{\exp(-d_{ij}^2)}{\sum_{k{\ne}i}{\exp(-d_{ik}^2)}},\ j \ne i$$ 当 $j \ne i$ 时，$p (x_j|x_i)$ 即点 $x_i$ 选择点 $x_j$ 作为邻居的概率，介于 $0 \sim 1$ 之间； 当 $j=i$ 时，点 $x_i$ 永远不可能成为自己的邻居， $p(x_i|x_i)=0$。 SNE用 条件概率 来代替欧式距离度量两个高维数据点间的相似性。 确定方差相似度中用来标准化的 $\sigma_i^2$ 是样本空间中除 $x_i$ 外其它所有点到 $x_i$ 距离的 方差。 如果 $x_i$ 周围的样本点比较密集，方差较小；$x_i$ 周围样本点比较稀疏，方差较大。 距离 $x_i$ 非常远的点，相对于整个样本总体，$x_i$ 选择它作为自己邻居的概率非常小。 我们考虑样本点 $x_i$ 与哪些样本点相似性比较高，即 $x_i$ 选择哪些样本点作为邻居的概率较高，我们将与 $x_i$ 更可能成为邻居的样本点称为 有效邻居。有效邻居实际上是一个定性概念，我们并不清楚 $x_i$ 的有效邻居有多少，因此我们需要规定一个有效邻居数。原文中定义了一个名叫 困惑度（Perplexity）的东西， $x_i$ 的困惑度定义为$$K_i = 2^{H(P_i)}$$其中 $H(P_i)$ 是香农熵：$$H(P_i) = -\sum_{j{\ne}i}{p_{ij}\log_2{p_{ij}}}=\log_2 K_i$$ 此外，对于那些非常遥远得点，中心点 $x$ 与它们得距离非常大，$x$ 选择它们作为邻居得概率也非常小，在空间维度很大、采样点很多的情况下，忽略它们不失为一种加快计算的有效办法。 这里为每个点 $x_i$ 定义了一个 困惑度（Perplexity）$K_i$，它与以 $x_i$ 为中心、以 $\sigma_i^2$ 为方差的高斯分布的（经验）熵有关：$$K_i = 2^{H(P_i)}$$其中 $H(P_i)$ 是香农熵：$$H(P_i) = -\sum_{j{\ne}i}{p_{ij}\log_2{p_{ij}}}=\log_2 K_i$$结合上述式子即可求出方差 $\sigma_i^2$：方差越大，则以点 $x_i$ 为中心的“密集区域”（总体）越大，被选中的有效邻居数就越多，相应的该分布的熵也越大，困惑度越大。这意味着，困惑度$K_i$可以用来描述点$x_i$选择的有效邻居数目。我们通过设置超参$K= K_i$来求方差，从而标准化每个分布。困惑度的取值范围一般为 $5 \sim 50$。 熵 $H(P_i)$ 的理解 不确定性：熵本身就意味着不确定性，当区域点密集时，中心点位置的不确定性就小； 能量：不确定性大意味着能量大，拉不住中心点，它要到处跑； 有效邻居数：点密集时中心点的有效邻居就多。 映射到低维空间在低维空间（二维或者三维）确定一点 $y_i$，它与高维空间的点 $x_i$ 对应，我们手动设置点 $y_i$ 的条件概率分布，即固定以 $y_i$ 为中心点的高斯分布对应的方差为 $\frac12$，当 $j{\ne}i$ 时：$$ q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}} $$当j=i时 $q(y_j|y_i)=0$ 。 此时，如果低维点 $y_i$ 能够正确表示高维点 $x_i$，意味着 $q(y_j|y_i)=p(x_j|x_i)$。为了使两个概率（近似）相等，我们可以最小化KL散度。损失函数如下：$$C=\sum_iKL(P_i|Q_i)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$ $P_i$ 表示：给定点 $x_i$，其它所有点的条件概率分布； $Q_i$ 表示：给定低维空间映射点 $y_i$，其它所有低维映射点的条件概率分布。 KL散度又叫相对熵，用来度量两个分布间的距离。假设P是真实分布，Q是模型分布，$KL(P|Q)$ 表示用Q表示P分布的数据所需的额外信息。 KL散度是不对称的 KL散度中包含 $\log\frac{p_{ij}}{q_{ij}}$ 意味着这个映射不是对称的，即： 使用距离较小的低维点表示距离较大的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于小于0，则损失C较小； 使用距离较大的低维点表示距离较小的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于大于0，则损失C较大。 这里就存在一个问题：当两个高维点距离很远，而我构造两个距离很近的低维点能够使损失函数更小，却与实际的目的不相符！所以，SNE算法使得高维空间中距离近的点在低维空间中距离仍然很近，但是远的点就嘿嘿嘿了。 最小化损失函数从 $q_{ij}$ 的定义式的分母部分可知，低维空间中点 $y_i$ 选择点 $y_j$ 的概率 $q_{ij}$ 与低维空间中的每一个映射点都有关系（分母起到了normalization的作用），但是求导结果却十分简洁：$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$ 想沿着所有点以最陡梯度下降是不现实的，不仅低效，还可能陷入糟糕费解的局部最优。上面的梯度公式右边理论上是针对所有低维映射点进行迭代，但是实际上，相距较远的一堆点间的影响十分小（抽象），在计算时往往可以忽略不计，也就是说：仅仅计算与中心点相距较近的一部分点，即邻居 ，这也是为什么算法中含有单词neighbour的原因了吧。 选择中心点的部分较近的邻居参与计算表示，我们只保留了中心点附近区域的特性，而忽略了整体局势，所以说SNE是保留局部特征而非全局特征的算法。这个局部特性主要反应为 $\sigma_i^2$：局部点密集方差小，局部点稀疏方法大。方差确定的方法前面已经陈述了~。 带动量项的梯度更新为了加速优化过程、避免很一般的局部解，可以在每次下降时添加动量项。动量 (momentum) 的作用就是在下降到局部最优时，小球仍然具有沿切线方向的分量，这使得小球将继续朝前运动，这会有两种结果： 小球越过障碍，继续前行； 小球回退，返回局部最优解；具体的，更新公式如下：$$\gamma^{(t)}=\gamma^{(t-1)}-{\eta}\frac{\partial{C}}{\partial{\gamma}}+\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$$ 式中 $\alpha(t)$ 即动量，动量项 ($\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$) 还与上一次运动幅度有关，直观的看，上一次运动越剧烈，下一次就越刹不住车。 随机抖动随机抖动 (random jitter) 是一种初始化技巧，即将低维空间中的所有数据点初始化在离坐标原点极近的地方。在迭代的过程中，它们将抖抖抖抖抖动直至收敛。尽管还是很慢，不过在节约时间和选择更优局部解时还是有明显提升的。 模拟退火在优化早期给每一步迭代加上高斯噪音，这可以帮助避免不好的局部最优解。随着迭代次数变多，噪音方差将逐渐减小。当方差变化非常慢时，这表明开始形成全局结构。（玄学） 然而，高斯噪音的数量和衰减速率是十分难以确定的，它们不仅与动量相关，也受学习率的影响。怎么办？多算几次！666]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>降维</tag>
        <tag>非线性降维</tag>
        <tag>SNE</tag>
        <tag>t-SNE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维02 - 主成分分析 (PCA)]]></title>
    <url>%2Fml%2F20190325-c024.html</url>
    <content type="text"><![CDATA[PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了线性代数之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。 PCA是最易于理解的特征提取过程：通过对原始特征的线性组合构造新的“特征”，这些特征不同于原始特征，但是又能与原始特征一样表达原始数据的信息。 PCA (Primary Component Analysis, 主成分分析) 为什么叫做主成分分析呢？因为PCA构造出的新特征地位并不是等同的，即这些新特征的重要程度存在差异： 第一主成分 (the first component) 是新特征中最重要的特征，它在所有新特征中方差最大，这意味着它对数据变异的贡献是最大的； 第二主成分 (the second component) 在保证不影响第一主成分的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异）； 第三主成分 (the third component) 在不保证第一和第二主成分呢的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异 - 第二主成分引起的变异）; 依次类推…… 线代原理预备知识：线性代数（矩阵运算、特征值&amp;特征向量、特征值分解） 可对角化如果一个n阶方阵A相似于对角矩阵，即存在可逆矩阵$P$使得$P^{-1}AP$是对角矩阵，则称方阵A是可对角化的。 n阶方阵A可对角化的充要条件是A每个特征值的几何重数与代数重数相等：代数重数指特征多项式中该特征值的幂次，几何重数指特征值对应的线性无关的特征向量的个数。 n阶方阵A可对角化的充要条件是A有n个线性无关的特征向量：几何重数与代数重数相等意味着n个线性无关的特征向量。 即使方阵A可逆也不能保证每个特征值的代数重数与几何重数相等，因此A可逆不是A可对角化的充要条件！ 特征值分解如果矩阵A是一个可对角化的方阵，它就可以进行特征值分解，即A可表示为：$$A=Q{\Lambda}Q^{-1}$$其中 $Q$ 是n阶方阵，它的n个列向量是方阵A的n个特征向量 $\Lambda$ 是对角方阵，对角线元素是方阵A的特征值，其位置与 $Q$ 中的特征向量位置相对应 特征值分解的应用？求逆。如果方阵A是非奇异矩阵（即可以进行特征值分解且特征值不含0），则 $A^{-1}=Q{\Lambda}^{-1}Q^{-1}$，其中 $[{\Lambda}^{-1}]_{ii}=\frac1{\lambda_i}$。 奇异值分解特征值分解对A的要求格外严格：可逆、特征值不含0、方阵……放松特征值分解的限制，将A扩展到任意 $m{\times}n$ 的矩阵即得到 奇异值分解 (Singular Value Decomposition, SVD) 。 假设M是定义在实数域或者复数域上的 $m{\times}n$ 阶的矩阵：$$M=U{\Sigma}V^\ast$$其中 U是 $m{\times}m$ 阶酉矩阵：U的m个列向量实际上是 $M^{\ast}M$ 的特征向量，称为M的左奇异向量。 $\Sigma$ 是 $m{\times}n$ 阶非负实数对角矩阵：对角线元素称为M的奇异值，一般情况下奇异值按从大到小的顺序排列！ $V^\ast$ 是 $V$ 的共轭转置，是 $n{\times}n$ 阶酉矩阵：V的n个列向量实际上是 $MM^\ast$ 的特征向量，称为M的右奇异向量。 共轭转置：共轭转置与转置是两个概念，当矩阵定义在实数域上时二者结果相同，矩阵A的共轭转置记为 $A^\ast$，定义如下：$$A^\ast=(\overline{A})^T=\overline{A^T}$$其中，$\overline{A}$ 表示对A的元素复共轭，当A定义在实数域时 $\overline{A}=A$。 当矩阵M定义在实数域时有：$$M=U{\Sigma}V^T$$我们在应用SVD时一般都是定义在实数域上的哟~ 主成分分析上面提到了对于任意 $m{\times}n$ 阶矩阵M的SVD分解：$$M_{m{\times}n}=U_{m{\times}m}{\Sigma_{m{\times}n}}V_{n{\times}n}^T$$直观图如下（这里假设样本数量m多于特征数量n，这意味着M有n个奇异值）：其中 $\Sigma$ 矩阵很有意思，当m&gt;n时，矩阵 $\Sigma_{m{\times}n}$ 中只有子矩阵 $\Sigma_{n{\times}n}$ 的对角线上的值不为0，如下图所示： 以scRNA测序为例：假设在表达谱矩阵中，一行表示一个细胞中不同基因的表达量，一列表示一个基因在不同细胞中的表达量。这与我们的习惯（一列表示一个细胞，一行表示一个基因）有所不同！ 对应到上述SVD分解式我们发现，n表示细胞数量，m表示基因数量。我们降维的结果肯定是要保证细胞总数m不变，而将基因数目从n减小到k。 具体的，取 $\Sigma$ 中最大的k个奇异值，即取 $\Sigma_{k{\times}k}$ 子矩阵，相应的取U的前k列和V的前k列（即$V^\ast$的前k行），即：此时$$M_{m{\times}n}=U_{m{\times}k}{\Sigma_{k{\times}k}}V_{k{\times}n}^T$$上式中的 $U_{m{\times}k}$ 就是 $M_{m{\times}n}$ 从n维特征空间降到k维特征空间的结果。注意矩阵 $U_{m{\times}k}$ 的k个列向量并不在矩阵 $M_{m{\times}n}$ 中，而是M中的n个列向量线性组合的结果。 工具12# in pythonfrom sklearn.decomposition import PCA]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维01 - 特征选择和特征提取]]></title>
    <url>%2Fml%2F20190325-d2ce.html</url>
    <content type="text"><![CDATA[大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的降维浓缩技术应运而生。 降维的好处有哪些？ 减少数据维度，存储数据需要的空间也会减少（盘霸可忽略~）； 低维数据可以减少计算量，缩短模型训练时间； 很多算法在高维数据上的表现远远没有在低维数据上好； 去掉冗余特征（强相关特征），提高数据的质量； 有助于可视化，我们只能形象观察三维及以下的数据！ 分类降维总是围绕着减少特征数进行的，根据对特征的操作可分为： 特征选择：保留原始特征集的子集，即选取部分原始特征； 特征提取：构造不同于原始特征的新特征，新特征往往是原始特征的组合，替代原始特征表达原始数据想表达的信息。特征提取是降维算法研究的核心内容。 特征选择特征选择只是对每个特征进行评估，去掉不重要的或者选出重要的： 缺失值比率：按缺失值比率删除特征； 低方差滤波：删除方差小的特征； 高相关滤波：只保留高相关特征中的一个； 随机森林：计算每个特征的重要性； 前向特征选择：依次增加特征数检验模型性能； 反向特征消除：依次减少特征数检验模型性能。 特征提取特征提取才是降维思想的核心内容，降维算法家族枝繁叶茂，先做一个总体分类：]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批次效应校正与RUV算法]]></title>
    <url>%2Fbioinfo%2F20190322-afa7.html</url>
    <content type="text"><![CDATA[方法 Gagnon-Bartsch et al.提出了RUV-2用来标准化连续的微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化离散的RNA测序数据。 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建泛化线性模型 (Generalized Linear Model, GLM):$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$参数意义如下： $Y$ 是 $n{\times}J$ 的表达矩阵； $W$ 是 $n{\times}k$ 与不需要的变异相关的多余变异相关矩阵（k是不需要的变异相关的变量的个数），$\alpha$ 是 $k{\times}J$ 的多余变异相关矩阵的系数（参数）； $X$ 是 $n{\times}p$ 与感兴趣的变异相关的期望变异相关矩阵（p是感兴趣的变异相关的变量的个数），$\beta$ 是 $p{\times}J$ 的期望变异相关矩阵的系数（参数）； $O$ 是一个 $n{\times}J$ 的矩阵，它可以置零，也可以包含其它的标准化过程（如UQ标准化）。 矩阵 $X$ 是一个随机变量，是我们实验的测量值，是已知的（先验）。 矩阵 $W$ 是未观测的随机变量；$\alpha$、$\beta$、$k$ 都是未知参数。 不同于先前的标准化方法，RUV可以使用GLM标准化技术同时标准化reads计数（$W\alpha$）和推断差异表达（$X\beta$）。标准化的计数也可以通过由原始计数对不需要的因子进行回归分析后求残差得到，但是直接从原始计数中移除不需要的因子（$W\alpha$）可能会损失掉 $X$ 的一部分。[reference] 同时估计 $W$, $\alpha$, $\beta$ 和 $k$ 是很难的。对于一个给定的 $k$ 值，我们尝试着用下面三种方法对$W$ 进行估计： 1. 基于阴性对照基因的RUVg 假设我们鉴定出了一个阴性对照基因 (negative control genes) 的集合（大小为 $J_c$），例如不差异表达的基因，对这个基因集合来说 $\beta_c=0$ 即 $\log{E[Y_c|W,X,O]}=W\alpha_c+O_c$，公式中的下标c将矩阵限制在了大小为 $J_c$ 的基因集合里。 定义 $Z=\log{Y}-O$，$Z^\ast$ 是 $Z$ 列向量中心化（$Z$ 的各个列向量均值都为0）的结果。 对 $Z_c^\ast$ 进行奇异值分解 (singular value decomposition, SVD) 即 $Z_c^\ast=U{\Lambda}V^T$。矩阵 $U$ 是 $n{\times}n$ 列正交矩阵，它的列向量是 $Z^\ast$ 的左奇异向量集；矩阵 $V$ 是 $J_c{\times}J_c$ 的列正交矩阵，它的列向量是 $Z^\ast$ 的右奇异向量集；$\Lambda$ 矩阵是由 $Z^\ast$ 的奇异值组成的非方形对角矩阵，大小为 $n{\times}J_c$。$Z^\ast$ 最少有 $\min{(n,J_c)}$ 个奇异值。对于一个给定的 $k$，通过 $\widehat{W\alpha_c}=U\Lambda_kV^T$ 估计 $W\alpha_c$，通过 $\hat{W}=U\Lambda_k$ 估计 $W$。$|lambda_k$ 是由 $\Lambda$ 导出的大小为 $n{\times}J_c$ 的非方形对角矩阵，保留 $\Lambda$ 中最大的 $k$ 个奇异值，将其它的奇异值置为0。 将 $\hat{W}$ 带入上面基于 $J$ 个基因构建的公式中，通过GLM回归估计 $\alpha$ 和 $\beta$。 （可选）将标准化的读段计数定义为 $Z$ 对 $\hat{W}$ 的普通最小二乘回归 (ordinary least squares, OLS) 的残差。 这是最基础的RUV-2的离散版本。其中的关键假设是我们能够找到这个阴性对照基因集合。然而，RUV-2已被证实对对照基因的选择十分敏感。我们因此考虑下面的两种方法：RUVr不需要阴性对照基因，RUVs对阴性对照基因选择的鲁棒性更强。 2. 基于残差的RUVr 计算残差矩阵 $E(n{\times}J)$: 计数矩阵 $Y(n{\times}J)$ 关于感兴趣的协变量矩阵 $X(n{\times}J)$ 的初步GLM回归，例如异常值残差。这里用于回归计算的计数矩阵可以是未标准化的原始数据，也可以是经过其它标准化工具（例如UQ）处理过的数据。 对残差进行奇异值分解，即 $E=U{\Lambda}V^T$，通过 $\hat{W}=U\Lambda_k$ 估计 $w$。接下来的步骤与 RUVg 的第4、5步相同。 3. 基于重复/阴性对照样本的RUVs 假设在多个复制样本中具有生物学特征的（我们感兴趣的）某些协变量的表达量可看作恒定的，它们的计数差异与RUVg中的阴性对照基因一样，对我们后续的研究没有影响。现在假设有 $R$ 个复制组，$r(i){\in}{1,…,R}$ 表示样本 $i$ 所属的复制组；如果样本 $i$ 不属于任何一个复制组，则 $r(i)=0$。例如，对于SEQC数据集，样本A和样本B各自的64个复制本（$=4[\text{libraries}]{\times}2[\text{flow-cell}]{\times}8[\text{lanes}]$）分别组成了一个复制组。 对每一个复制本对应的计数矩阵进行列中心化处理，即矩阵各个列向量的均值都为0。去掉不属于预期复制组的样本，即筛选出 $n_d=\sum_i{I(r(i)\ne0)}$ 个样本对应的列中心化后的计数子矩阵 $Y_d(n_d{\times}J)$。 此时 $\log{E[Y_d|W,X,O]}=W_d\alpha+O_d$，对应的矩阵大小是 $(n_d{\times}J){\leftarrow}({n_d\times}k)({k\times}J)+(n_d{\times}J)$。 定义 $Z_d=\log{Y_d}-O_d$，$Z_d^\ast$ 是 $Z_d$ 列中心化的结果，$Z_d^\ast=U{\Lambda}V^T$。通过 $\hat{\alpha}=\Lambda_kV^T$（保留最大的 $k$ 个奇异值，$k{\le}\min{(n_d,J)}$）来估计 $\alpha$。 在所有 $n$ 个原始数据和 $J_c$ 个阴性对照基因上对 $Z_c$ 进行最小二乘回归（OLS）。估计讨厌因子 $W$：$\hat{W}=Z_c\hat\alpha_c^T(\hat\alpha_c\hat\alpha_c^T)^{-1}$。接下来的步骤与 RUVg的第4、5步相同。 要点两个数据集 SEQC data set: The third phase of the MicroArray Quality Control (MAQC) project, also known as the Sequencing Quality Control17 (SEQC) project, aims to assess the technical performance of high-throughput sequencing platforms by generating benchmarking data sets. Zebrafish (斑马鱼) data set: All procedures were conducted in compliance with US federal guidelines in an AAALAC-accredited facility and were approved by the UC Berkeley Office of Animal Care and Use. 两种讨厌因子本文分析了两种讨厌因子：library preparation &amp; flow-cell effects。 flowcell：流动室，别称鞘流池、流动池，是流式细胞技术的基础关键部件。大概长这个样子： 作者用正交的主成分图展示了这两种讨厌因子：Scatterplot matrix of first three principal components (PC) for unnormalized counts (log scale, centered). The principal components are orthogonal linear combinations of the original 21,559-dimensional gene expression profiles, with successively maximal variance across the 128 samples, that is, the first principal component is the weighted average of the 21,559 gene expression measures that provides the most separation between the 128 samples. Each point corresponds to one of the 128 samples. The four sample A and the four sample B libraries are represented by different shades of blue and red, respectively (16 replicates per library). Circles and triangles represent samples sequenced in the first and second flow-cells, respectively. As expected for the SEQC data set, the first principal component is driven by the extreme biological difference between sample A and sample B. The second and third principal components clearly show library preparation effects (the samples cluster by shade) and, to a lesser extent, flow-cell effects reflecting differences in sequencing depths (within each shade, the samples cluster by shape). 算法横向对比上分位数标准化 (Upper-quartile normalization, UQ)，UQ只能消除流细胞效应而对文库效应束手无策，RUV算法解决的就是如何消除不同文库的影响。 局部加权回归散点平滑法 (Locally Weighted Scatterplot Smoothing, LOWESS/LOESS)不能消除文库效应。 补充ERCC spike-in controlsERCC 即 External RNA Controls Consortium，是斯坦福大学为了定制一套spike-in RNA而成立的专门性组织，主要的工作是设计了好用的spike-in RNA，方便microarray以及RNA-Seq进行内参定量。[官方首页] RNA spike-in是一种数量和序列都已知的RNA转录本，用于校准RNA杂交实验（例如DNA微阵列实验、RT-qPCR、RNA测序等）的测量值。RNA spike-in作为对照组（控制组）探针，被设计成能与具有相应匹配序列的DNA分子结合，这个特异性结合的过程我们称之为杂交。在制备的过程中，已知数量的spike-in将与实验样本进行混合。spike-ins的杂交程度可以用来标准化样本RNA的测量值。[wiki] [reference]]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批次效应（batch effect）]]></title>
    <url>%2Fbioinfo%2F20190319-cca5.html</url>
    <content type="text"><![CDATA[一、定义下面是大佬给出来的关于批次效应(batch effect)的定义： Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. Leek et. al (2010) 批次效应是测量结果中的一部分，它们因为实验条件的不同而具有不同的表现形式，并且与我们研究的变量没有半毛钱关系。一般批次效应可能在下述情形中出现： 一个实验的不同部分在不同时间完成； 一个实验的不同部分由不同的人完成； 试剂用量不同、芯片不同、实验仪器不同； 将自己测的数据与从网上下载的数据混合使用； …… 二、检测批次效应相关协变量已知时，直接聚类观察结果是否和相应协变量相关。混合数据因为实验条件迥异，一般批次效应都很大。 以R为例，通过聚类检验是否存在批次效应。请先查看下面的示例数据集。12345678# t() 转置函数# dist() 距离函数：按照指定规则求行向量间的距离，因此要转置&gt; dist_mat &lt;- dist(t(edata))&gt; clustering &lt;- hclust(dist_mat) # hclust 的输入结构与 dist 相同！# 按照批次信息聚类&gt; plot(clustering, labels = pheno$batch)# 按照是否是正常细胞聚类&gt; plot(clustering, labels = pheno$cancer) 聚类结果如下：左边的红色框框是正常细胞中混入的癌细胞，右边蓝色框框中是癌细胞中混入的正常细胞。 还有许多检验批次效应的的方法，这篇文章给出了多种检验方式： 图分析：双柱状图、QQ图、箱线图、块图、… 定量分析：F检验、双样本t检验、… 三、处理实验条件允许的条件下，应该优化实验设计，将引起批次效应的协变量采样分散开来。例如，对于时间批次效应，实验的不同部分应该在各个时间内均匀采样。这叫“治病就治本”。 但是大多数情况下实验条件不允许，如果够幸运的话批次效应相关的协变量已经被记录下来了，此时对批次效应进行验证，然后使用统计模型过滤；如果十分不幸，批次效应相关的协变量没有被记录或者不明显，我们就需要借助相关工具猜一下哪个变量可能造成了批次效应，然后使用统计模型过滤。前者叫参数化方法，后者叫非参数化方法。 1.导入示例数据集bladderbatch包bladderbatch包包含了一项膀胱癌研究中相关的57个样本的基因表达数据，这些数据已经使用RMA标准化，并且已经按照相关协议进行了预处理。 另外阅读R文档我们发现： eSet是一个包含高通量实验元数据的一个类，它不能直接被实例化。 pData方法在类eSet中被定义，它的作用是访问数据的元数据（注释信息）。 ExpressionSet继承自eSet，同样是一个高通量测序数据的容器，由&gt; * biobase包引入，封装了表达矩阵和样本分组信息。表达矩阵存储在exprs中。 bladderbatch数据集是（类似）ExpressionSet类型，我们可以使用pData()加载元数据，使用exprs()加载表达谱数据。bladderbatch数据集用来演示如何校正批次效应。 下载和加载数据集123456789101112## 1.安装并加载数据集&gt; BiocInstaller::biocLite("bladderbatch")&gt; library(bladderbatch) # 或者 library("bladderbatch", character.only=TRUE)## 2.查看当前可用数据集&gt; data()## 3.检查是否有如下信息Data sets in package ‘bladderbatch’:bladderEset (bladderdata) Bladder Gene Expression Data Illustrating Batch Effects## 加载数据集&gt; data(bladderdata) # 实际加载进来的数据集名字叫做 bladderEset !&gt; pheno &lt;- pData(bladderEset) # 使用 pData 加载元数据/注释信息&gt; edata &lt;- exprs(bladderEset) # 使用 exprs 加载数据 pheno如下所示：样本的批次信息存储作为元数据存储在pheno$batch中（R中使用$访问对象的属性）。 edata如下所示：一列表示一个样本（细胞），后面求距离需要转置。 2.R中的sva包sva用于移除高通量测序数据中的批次效应以及其它无关变量的影响。 sva包含用于标识和构建高维数据集（例如基因表达、RNA测序/甲基化/脑成像数据等可以直接进行后续分析的数据）代理变量的函数。代理变量是直接从高维数据构建的协变量，可以在后续分析中用于调整未知的、未建模的或潜在的噪音源。 代理变量（surrogate/proxy variable）: A variable that can be measured (or is easy to measure) that is used in place of one that cannot be measured (or is difficult to measure). For example, whereas it may be difficult to assess the wealth of a household, it is relatively easy to assess the value of a house. See also proxy variable. (from Oxford Reference)代理变量分析（Surrogate Variable Analysis）：Click here sva从三个方面消除人为设计造成的影响： 为未知变异源构造代理变量；(Leek and Storey 2007 PLoS Genetics, 2011 Pharm Stat.) 使用ComBat直接移除已知的批次效应；(Johnson et al. 2007 Biostatistics) 使用已知的控制探针(known control probes)移除批次效应；(Leek 2014 biorXiv)移除批次效应和使用代理变量可以减少依赖性，稳定错误率估计值，提高重现性。 查看sva在线文档。 &gt; 已记录批次信息当批次协变量已知时（即每个样本分属于哪一个批次记录在数据集的元数据中），可以使用sva的ComBat校正批次效应。ComBat使用参数（parametric）或者非参数（non-parametric）的经验贝叶斯框架（Empirical Bayes Frameworks）进行批次效应的校正。 先看ComBat的用法：摘自官方文档12345&gt; ComBat(dat, batch, mod=NULL, par.prior = TRUE, prior.plots = FALSE)# dat: 基因组测量矩阵（探针维度 X 样本数），探针维度例如marker数、基因数.....，例如表达谱矩阵# batch: 批次协变量，只能传入一个批次协变量！# mod: 这是一个模式矩阵，里面包含了我们感兴趣的变量！# par.prior: 基于参数/非参数，默认为基于参数 有了背景知识我们就可以进行膀胱癌数据的批次校正：123456&gt; pheno$hasCancer &lt;- pheno$cancer == "Cancer"# 或者 &gt; pheno$hasCancer &lt;- as.numeric(pheno$cancer == "Cancer")&gt; model &lt;- model.matrix(~hasCancer, data=pheno)&gt; combat_edata &lt;- ComBat(dat = edata, batch = pheno$batch, mod = model)# 这里的 mod 参数就比较有意思了，它记录的是我们感兴趣的变量。因为初次接触R只能肤浅理解一下。# 它应该是一个我们期望样本能被正确聚类所依据的协变量，它总是数值型变量 model.matrix(...)的详细解释见这里。 画图：1234&gt; dist_mat_combat &lt;- dist(t(combat_edata))&gt; clustering_combat &lt;- hclust(dist_mat_combat, method = "complete")&gt; plot(clustering, labels = pheno$batch)&gt; plot(clustering, labels = pheno$cancer)) 我们发现批次效应被移除了： &gt; 没有记录批次信息看这里 3.R中的ber包ber的全称就是batch effects removal，使用&gt; install.packages(&quot;ber&quot;)安装ber包，查看用户手册。 这个包里有6个函数，它们的作用就是校正微阵列标准数据中的批次效应。标准数据指的是：输入矩阵每一行代表独立的样本，每一列代表基因；批次信息作为已知的分类变量；期望变量可以大大提高批次效应校正的效率。 berr(Y, b, covariates = NULL) using a two-stage regression approach (M. Giordan. February 2013) ber_bg(Y, b, covariates = NULL,partial=TRUE,nSim=150) using a two-stage regression approach and bagging (M. Giordan. February 2013) combat_p(Y, b, covariates = NULL, prior.plots=T) using a parametric empirical Bayes approach (n Johnson et al. 2007) combat_np(Y, b, covariates = NULL) using a non-parametric empirical Bayes approach (n Johnson et al. 2007) mean_centering(Y, b) using the means of the batches standardization(Y, b) using the means and the standard deviations of the batches 上表中的： Y是输入矩阵（样本数 $n{\times}g$ 探针数） b是 $n$ 维分类1向量，每个分量对应着每个样本的批次信息 covariates是一个 $n$ 行的data.frame实例 上面的6个函数都需要指定b，所以它们都是用来处理批次信息被记录的情形的，对于启发性的校正貌似没提出解决方案。 4.R中的RUVSeq包RUVSeq means Remove Unwanted Variation from RNA-Seq Data, which shows us how to conduct a differential expression (DE) analysis that controls for “unwanted variation”, e.g., batch, library preparation, and other nuisance effects, using the between-sample normalization methods proposed in Risso et al. (2014). RUV算法基本原理参考这里，原文在这里。 5. R中的BatchQC包BatchQC工具 四、FAQ 标准化（normalization）可以消除批次效应吗？ 只能缓解，不能消除。 五、其它资料Stanford大学MOOC公开课讲义：PH525x series - Biomedical Data Science TCGA Batch Effects Viewer From BioMedSearch: Removing batch effects in analysis of expression microarray data: an evaluation of six batch adjustment methods.]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[model.matrix(...)]]></title>
    <url>%2FR%2F20190319-1548.html</url>
    <content type="text"><![CDATA[R中的模型矩阵函数。 分类变量分类变量（Factors）：R中用来存储分类数据的类别信息。12345678910&gt; f = factor(c('a','b','a','c'))# 检查变量是否是分类变量（因子）&gt; class(f)[1] "factor"# 查看分类变量中有哪些类别&gt; levels(f)[1] "a" "b" "c"# 查看分类变量中有几类&gt; nlevels(f)[1] 3 哑变量虚拟变量/哑变量（dummy variable）：量化非数值类型的变量，通常取0/1。例如，衡量一个人的性别：男 -&gt; 1，女 -&gt; 0。 解释变量解释变量（explanatory variable）：单纯从数理角度来看，解释变量等同于控制变量/自变量，与之相对的是被解释变量（反应变量/因变量）。REF 设计矩阵设计矩阵（design matrix）：又叫模型矩阵（model matrix）或者回归矩阵（regressor matrix）。由解释变量值组成的矩阵：一行代表一个独立的观测对象（样本），一列代表对应的变量（特征值、元数据），通常记为$X$。简单理解，就是我们所说的输入矩阵，可以是元数据的，也可以是数据的。 model.matrix(…)定义：1234# S3 method for default model.matrix(object, data = environment(object), contrasts.arg = NULL, xlev = NULL, …)# 函数依据 object 创建设计矩阵，矩阵的创建必须借助于数据集 data# data 必须能提供与 object 相同名字的变量！ 以膀胱癌去批次效应为例，元数据形式如下 下面是部分列处理后的结果：123456789101112131415161718192021222324252627&gt; model &lt;- model.matrix(~batch, data = pheno) (Intercept)batchGSM71019.CEL 1 3GSM71020.CEL 1 2# pheno$batch 是数值型变量，相当于提取列# 此时新的变量名仍然是 batch#---------------------------------------------------------------------&gt;model &lt;- model.matrix(~cancer, data = pheno) (Intercept) cancerCancer cancerNormalGSM71019.CEL 1 0 1GSM71020.CEL 1 0 1# pheno$cancer 被处理成分类变量，每一类将单独作为列（哑变量），取值为0/1# 此时新的变量名为 cancerCancer 和 cancerNormal#---------------------------------------------------------------------&gt; model &lt;- model.matrix(~cancer=="Cancer", data = pheno) (Intercept) cancer == "Cancer"TRUEGSM71019.CEL 1 0GSM71020.CEL 1 0# cancer=="Cancer" 是一个 logical 类型# 这种写法极不优雅！我们应该先定好名字&gt; pheno$hasCancer &lt;- pheno$cancer == "Cancer"&gt; model &lt;- model.matrix(~hasCancer, data=pheno) (Intercept) hasCancerGSM71019.CEL 1 0GSM71020.CEL 1 0]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-SCENIC]]></title>
    <url>%2Fbioinfo%2F20190315-94a8.html</url>
    <content type="text"><![CDATA[SCENIC: Single-cell regulatory network inference and clustering 基于regulon(TF-&gt;targets)构建GRNs，基于GRNs可以进行细胞聚类。 与其说是算法，不如说SCENIC是组合算法的一个流程。如下图所示： 1. 基因数据过滤共表达分析之前需要对基因数据进行过滤，本文使用如下两个方法： 1.根据每个基因的reads数目移除可信度低或者将产生较大噪声的基因。reads数目的阈值与数据集的大小有关，以下是原文表述：3 UMI counts (slightly over the median of the nonzero values) multiplied by 1% of the number of cells in the dataset (e.g. in mouse brain: 3 UMI counts x 30 (1% of cells) = minimum 90 counts per gene). 2.根据可检测到某个基因的细胞数目过滤掉只在一个细胞或者极少量细胞中表达的基因，细胞数目的阈值设定参考原文：set a percentage lower than the smallest population of cells to be detected. For example, since microglia cells represent approximately 3% of the total cells in the dataset, we used a detection threshold of at least 1% of the cells. 2. 共表达分析仅仅借助共表达分析(co-expression analysis)(GENIE3或者GRNBoost)得到regulons的表达情况（GRNs）。一个regulon由一个TF和它调控的靶基因组成。当数据集非常大时GENIE3的运行速度将会变得非常慢，此时使用GRNBoost替换GENIE3能大大加快计算速度。 但是，共表达分析存在许多假阳性结果，我们需要找到这些实际上不存在的TF-靶基因配对，因此需要下一步的基序富集分析。 GENIE3GENIE3的核心算法是随机森林回归模型。随机森林能够处理非线性共表达关系。针对不同的TF训练不同的模型，这些模型用于计算相应TF的权重，这些权重可以用来衡量它与靶基因共表达的强度。 输入：一个基因表达矩阵，矩阵的每一列代表一个细胞的不同基因，每一行代表一个基因在不同细胞里的表达量。矩阵的元素可以是UMI计数，也可以是其它指标，例如TPM (Transcripts Per Million)、FPKM/RPKM等等。输入矩阵应避免进行标准化或归一化处理，这样会人为的引入多余的协方差。 输出：一张三列表，分别代表TF、靶基因、权重（TF靶向目标基因的可信度） GRNBoost Spark RDD：Risilient Distributed Datasets 弹性分布式数据库广播变量(Broadcast Variable)：将只读变量广播到各个节点以供读取，避免变量在任务间进行传递。变量被广播之后应避免被修改。 GRNBoost作为GENIE3在大数据集下的替代方案，它仍然接受GENIE3的基本思想：仅从基因表达数据中推断GRNs。 算法方面，GRNBoost使用了XGBoost库中的GBM (Gradient Boosting Machines)。GBM是一种结合多种弱学习器、以提升学习作为基本策略的集成学习方法。相对于随机森林，GBM使用了bagging自助聚合进行模型的平均以提高回归准确度。 然而，GRNBoost的主要贡献是基于Spark实现了多回归并行计算。软件输入是基因表达向量（一系列基因和一个转录因子表达量组成的向量？）。GRNBoost首先将基因表达向量分发给集群的各个节点，然后构建一个基于表达数据全集的预测矩阵。然后使用广播变量（Broadcast Variable）将这个预测矩阵广播到各个节点，接下来进行Map/Reduce分布式计算。 Map阶段：基于基因表达向量使用预测器训练XGBoost回归模型。基于训练的模型，TF和靶基因的靶向强度将以网络的边的形式呈现出来。 Reduce阶段：整合所有的边形成最终的GRNs。 3. 基序富集分析基序富集分析(motif enrichment analysis)使用的工具是RcisTarget，它能找到共表达分析的假阳性结果。删除这些假阳性结果就能得到正确的GRNs (Gene Regulatory Networks)。 RcisTarget 是 i-cisTarget 和 iRegulon 基序富集框架的 R/Bioconductor 实现。 主要分为两个步骤： 选择在基因的转录起始位点（TSS）附近明显高表达的DNA基序This is achieved by applying a recovery-based method on a database that contains genome-wide cross-species rankings for each motif. 实现方法：在基序全基因组跨物种排名数据库上使用recovery方法保留那些可以注释到TF并且标准富集分数(Normalized Enrichment Score, NES)大于3.0的基序2.对于每个基序和基因集，RcisTarget预测候选靶基因（如在基因集中排列在前缘以上的基因）方法详情见引用[32]，此方法在i-cisTarget&amp;iRegulon中均有实现，因此使用RcisTarget得到的结果与i-cisTarget&amp;iRegulon的结果一致 为了构建最终的调控子，我们将每个有基序富集的TF模块预测的靶基因进行归并。上面针对的是正调控，对于抑制，仍然可以对负相关的TF模块做相同的处理；但是我们的分析中，这类模块较少。基于上述事实，本实验之研究正相关，不研究负相关本文使用的数据集：the “18k motif collection” from iRegulon (gene-based motif rankings) for human and mouseTSS搜索空间：10kb around the TSS or 500bp upstream the TSS 4. AUCell打分The relative scores of each regulon across the cells allow identifying which cells have a significantly high sub-network activity细胞的调节子打分容许我们识别哪些细胞具有明显的高子网络活性？？？结果是一个二进制的活性矩阵，可用于下游分析——对此矩阵的聚类可用于细胞类型或者细胞状态的识别，基于调控子网络的活性共享。对抗dropouts增强鲁棒性：对调节子整体进行打分，而不是针对特定的转录因子或者单个基因。 基于单细胞测序数据，从活化的GRNs中鉴定细胞输入是一个基因集，输出为每个细胞中基因集的活性（AUC指标）在SCENIC中，这些基因集表现为regulons，每个调控子由一个TF和它对应的靶基因组成AUCell calculates the enrichment of the regulon as an area under the recovery curve (AUC) across the ranking of all genes in a particular cell, whereby genes are ranked by their expression value.将AUC区域面积作为regulon的富集量，该区域包含了特定细胞中所有基因的排序。This method is therefore independent of the gene expression units and the normalization procedure.因为是在单个细胞上进行检验，因此很容易可以应用到大数据集 ——AUCell用来估计每个细胞中每个regulon的活性，通过计算恢复曲线下的面积，整合了每个regulon里所有排列的基因的信息——AUC打分（上面计算出来的）通过设定阈值构建Regulon Activity Matrix，用来判定哪些细胞里的regulon处于on状态——左图横坐标是一个regulon的靶基因的排列，纵坐标是从输入数据集中数出来的基因数目然后，AUCell使用“曲线下面积”(AUC)计算输入基因集中的一个关键子集是否在每个细胞的排名顶部富集。AUC表示表达基因在特征中的比例以及相对于细胞内其他基因的相对表达值这一部的输出是一个矩阵：每一个细胞的每一个基因集的AUC分数使用细胞中的一系列regulon的AUC值进行细胞的聚类，或者使用处理过的二值矩阵——二值矩阵：自动 or 手动——下图是AUC分布的几个例子 5. 基于GRNs的细胞聚类AUC activity matrix：每个细胞中每个regulon的AUC值，连续值regulon activity matrix：上面矩阵二值化的结果，01矩阵可视化：主要用t-SNE、层次聚类的热图探究结果的其他可选项—— t-SNE的高密度区域 =&gt; 最可能的稳定状态—— 鉴定key regulators—— 基于数据库注释了解细胞属性—— GO terms (regulon内的基因富集分析) 操纵子（operon）：包含了操纵基因的核苷酸序列，被某个启动子控制，对应一组受操纵基因调控的基因调节子（regulon）：对应受某个起调节作用的蛋白质调节的一组基因刺激子（stimulon）：对应某类起调节作用的细胞调节的一组基因 引用Nat Methods. 2017 Nov;14(11):1083-1086. doi: 10.1038/nmeth.4463. Epub 2017 Oct 9.SCENIC: Single-cell regulatory network inference and clusteringAibar S1,2, González-Blas CB1,2, Moerman T3,4, Huynh-Thu VA5, Imrichova H1,2, Hulselmans G1,2, Rambow F6,7, Marine JC6,7, Geurts P5, Aerts J3,4, van den Oord J8, Atak ZK1,2, Wouters J1,2,8, Aerts S1,2. &gt;&gt;&gt; SCENIC]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-RaceID2+StemID]]></title>
    <url>%2Fbioinfo%2F20190315-98e6.html</url>
    <content type="text"><![CDATA[背景数据分析时要处理的噪声不仅来自实验误差和测量误差，基因表达和少量mRNA的扩增都会产生一定的噪声。其中来自生物过程的噪声可以通过大量测序来缓和。 如果根据某一时刻的转录表达情况推断细胞系谱结构一直是一个严峻的挑战。作者肯定了Wanderlust算法的设计思想，但是同时对其设计的拓扑结构的合理性表示怀疑，并提出了自己的解决方案。 RaceID2算法旨在：基于单细胞转录组数据，通过聚类鉴定出干细胞 StemID算法旨在：利用RaceID2的聚类结果，构建系谱树 算法算法在小肠的Lgr5+细胞、骨髓造血干细胞上学习，在人类胰腺多能干细胞上测试。Lgr5+小鼠肠道干细胞群的分化过程研究已经发表，作为算法学习的基础数据集。 RaceID此处嘀向作者2015年发表的论文 RaceID2作者对自己已经发表的RaceID算法进行优化：RaceID使用K-means进行聚类，因此求所有样本平均值的做法使得RaceID对异常值十分敏感，同时RaceID使用间隔统计量（Gap Statistics, GS）确定分类个数；但是作者认为这种方法并不理想，因此在RaceID2中改用K-medoids方法聚类，并且依据类内散布饱和临界值为依据确定分类个数。K-medoids聚类方法使用类似于中位数的方法确定聚类中心，与K-means不同，它的聚类中心始终产生在样本点上。 RaceID2是一种改进的聚类算法，能够将大量细胞进行聚类，从而确定不同细胞群/亚群的分界线。 StemIDStemID是一种系谱图推导方法，StemID的系谱图推导基于RaceID2的聚类结果。 下面是从原文摘录的算法流程图： 图A是RaceID2的聚类结果，其聚类中心都在样本点上。 这里先介绍作者给出的一个感性假设：每个节点 $k$ (一个细胞)除了属于自身的第 $i$ 类外，它还将连接到一个其它的某个类 $j$ ，这个类 $j$ 实际意义等同于另一个的细胞群/亚群，细胞 $k$ 将倾向于朝细胞群 $j$ 分化。 如图A所示，Cluster 1的聚类中心是 $m_i$，这里 $i=1$，将 $m_i$ 与其它所有聚类中进行连接（如图A黑色矢量）。第 $i$ 类中的节点 $k$（蓝色矢量箭头处）与类中心 $m_i$ 组成了一个向量（如图A蓝色矢量）。蓝色矢量将在所有黑色矢量上产生一个 投影。我们取与 最大投影长度 相对应的那个外类作为 感性假设 中陈述的潜在分化方向。 如图B2所示，将所有节点转换成到之相对应的投影位置，此时所有节点都将落到由聚类中心组成的网络上。 此网络就是StenID算法所构建的系谱树框架。 给网络的边打分：映射后不同边上点的分布是不同的，对于某条边 $L$ 上的任意两点 $r_i$ 和 $r_j$，定义打分公式： $$score=1-\max_{i,j{\in}L}{||r_i-r_j||}$$ 当 $score{\to}0$ 时说明该边上所有点非常紧密的靠近聚类中心。 p值计算：重复采样，略 细胞的熵得计算：略 不足作者自己分析了一下，在下面两种情况出现时算法可能不太灵光： 出现中间过渡态细胞的样本缺失； 出现不直接关联的细胞。 引用Cell Stem Cell. 2016 Aug 4;19(2):266-277. doi: 10.1016/j.stem.2016.05.010. Epub 2016 Jun 23.De Novo Prediction of Stem Cell Identity using Single-Cell Transcriptome DataGrün D1, Muraro MJ2, Boisset JC2, Wiebrands K2, Lyubimova A2, Dharmadhikari G3, van den Born M2, van Es J2, Jansen E2, Clevers H4, de Koning EJP3, van Oudenaarden A5. github：StemIDomicX：stemid-tool]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信分析工具-Wanderlust]]></title>
    <url>%2Fbioinfo%2F20190315-643b.html</url>
    <content type="text"><![CDATA[背景以人类B细胞为例，确定每个细胞在相应细胞过程（例如细胞分化）中的先后顺序。 算法输入：算法的输入是一个 $M{\times}N$ 的矩阵，其中 $M$ 是细胞数量，$N$ 是选取的marker数量。每个marker的丰度由质谱流式细胞技术(Mass Cytometry)测定。 输出：每个细胞的路径打分。此打分值介于0~1之间：0表示路径起点细胞，1代表路径终点细胞。 Wanderlust实际上就是最近邻图(Nearest Neighbour Graph)与EM算法的组合。 下图是摘自原文的算法流程： 图A是输入数据的形象表示。 1.凭先验知识确定一个起始节点（图B红色节点），随机确定 $nl$ 个路标节点（图B紫色节点） 为什么设置路标节点？路标节点起到缓冲噪声干扰的作用。相对于起始点，具有更小的最短路径距离（Shortest Path Distance）。而随机选取可以排除了先验知识的影响。 2.构建k-NNG，该图以邻接矩阵的方式进行存储，计算相连节点间的距离，可选的距离定义有欧式距离、余弦距离、…… 3.NNG下采样：从这个k-NNG构造出 $l$ 个l-k-NNG。算法只在子图上迭代运行，最后取均值作为最终结果。 为什么采样成子图进行计算？前面构造的k-NNG实际上包含了许多与实际情况不符的连接，即“假边”。进一步的随机下采样使得这些“假边”在子图中出现一定程度的缺失，这将增强模型的适应能力（鲁棒性）。 对于每个子图进行迭代优化： 4.初始化每个节点（细胞）的路径打分；起始节点为0，终止节点为1，中间节点的初始打分为该节点到起始节点的最短路径距离。最短路径距离通过Dijkstra算法计算。 初始化两节点连接的方向：距离起始节点路径打分小的节点作为上游节点。 5.对每个目标节点 $t$ 和每个路标节点 $l$ 间的距离进行打分： $$w_{l,t}=\frac{d(l,t)^2}{\sum_m{d(l,m)^2}}$$ 这个打分有什么意义？尚未知晓 6.计算每个目标节点 $t$ 的路径打分，即该目标节点到所有路标节点距离的加权平均： $$traj_t=\frac1{nl}\sum_l{w_{l,t}d(l,t)}$$ 7.根据计算出来的路径打分计算新的方向。 8.重复步骤567直到路径打分收敛。 引用Cell. 2014 Apr 24;157(3):714-25. doi: 10.1016/j.cell.2014.04.005.Single-cell trajectory detection uncovers progression and regulatory coordination in human B cell development.Bendall SC1, Davis KL2, Amir el-AD3, Tadmor MD3, Simonds EF4, Chen TJ5, Shenfeld DK3, Nolan GP6, Pe’er D7.]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10系统激活]]></title>
    <url>%2Fos%2F20190301-83a5.html</url>
    <content type="text"><![CDATA[晚上打开公司发的个人电脑，桌面右下角的“激活windows”提示十分难受，遂上网了一下激活方法。 Win + X 选择命令提示符(管理源)打开cmd 三行命令激活：1234567891011121314//Win10专业版(2019年3月26日测试可用)slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GXslmgr /skms kms.03k.orgslmgr /ato//Win10企业版slmgr /ipk NPPR9-FWDCX-D2C8J-H872K-2YT43slmgr /skms kms.03k.orgslmgr /ato //Win10家庭版slmgr /ipk TX9XD-98N7V-6WMQ6-BX7FG-H8Q99slmgr /skms kms.03k.orgslmgr /ato Win10激活密钥key激活次数有限制，不能保证100%激活成功。 2019年3月最新可用KMS激活服务器地址 KMS一键激活服务 KMS一句命令激活windows/office]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>win10</tag>
        <tag>激活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[监督学习-广义线性模型01-普通最小二乘法]]></title>
    <url>%2Fml%2F20190225-724a.html</url>
    <content type="text"><![CDATA[线性模型：输出是输入的线性组合，即： $$y(w,x)=w_0+w_1x_1+\cdots+w_px_p$$ 在sklearn中，变量 coef_ 存储向量 $w=(w_1,\cdots,w_p)$，变量 intercept_ 存储 $w_0$。 1from sklearn import linear_model 线性回归普通最小二乘法(Ordinary Least Squares)就是简单的计算残差和： $$min_w{||Xw-y||_2}^2$$ 这个算法叫做 线性回归(Linear Regression): 123from sklearn import linear_modelestimator = linear_model.LineaRegression(...) 简单的线性回归只在输入X数据集的各特征之间线性不相关时表现良好。 当X的特征线性相关时，估计结果受随机误差影响大，此时就需要进行模型的矫正。 脊回归/岭回归当特征间 共线性（Collinearity）关系较强时，脊回归（Ridge Regression）可以使模型具有收缩能力。 这通过给线性回归添加L2正则项实现： $$min_w{||Xw-y||_2}^2+\alpha{||w||_2}^2，其中\alpha\le0$$ 1234from sklearn import linear_model# 指定关键超参数α的值estimator = linear_model.Ridge(alpha=0.5) 对于α，可以使用交叉验证进行最优解搜索： 12345from sklearn import linear_model# 给定α的取值范围，默认值如下# cv指定交叉验证的折数，默认如下，默认使用留一交叉验证（Leave-One-Out CV）estimator = linear_model.RidgeCV(alpha=[0.1, 1.0, 10.0], cv=None) LassoLasso是用来估计稀疏系数的线性模型，和其变异体广泛用于语义压缩领域？ $$min_w\frac1{2n_{samples}}{||Xw-y||_2}^2+\alpha||w||_1，其中\alpha\le0$$ 1234from sklearn import linear_model# α默认值如下estimator = linear_model.Lasso(alpha=1.0)]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[获得一组数的全排列]]></title>
    <url>%2Fpython%2F20190221-9924.html</url>
    <content type="text"><![CDATA[自己实现的python函数。123456789101112131415161718192021222324def permutation(xs): if isinstance(xs,str): def str2charArray(str): charArray = [] for i in str: charArray.append(i) return charArray xs = str2charArray(xs) if len(xs) == 0 or len(xs) == 1: return [xs] result = [] for i in range(0,len(xs)): temp_list = xs[:] temp_list.pop(i) temp = permutation(temp_list) for j in temp: j.insert(0,xs[i]) result.append(j) return result]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[glob内建模块]]></title>
    <url>%2Fpython%2F20190221-3133.html</url>
    <content type="text"><![CDATA[glob模块是python的一个很基础、很简单的模块，用于匹配文件路径。 glob这个单词本身有“通配符”的意思，通配的一个很关键的应用就是筛选出符合条件的文件。 与python的另一个专门用于正则匹配的 re模块 不同，glob只需要三个通配符：*、？、[]。 以下是常见的匹配情形： glob.glob(&#39;/a/b/*.txt&#39;): 匹配目录 /a/b/ 下的所有.txt文件 glob.glob(&#39;/a/b/^[xyz]*.txt&#39;): 匹配目录 /a/b/ 下所有文件名以字母xyz中任意一个开始的文件 glob.glob(&#39;/a/*/*.txt&#39;): 匹配目录 /a/ 下所有的.txt文件 此外，glob.glob() 是一次查询完所有结果。在查询结果较多时，可以使用 glob.iglob() 迭代查询，glob.iglob() 返回一个生成器。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os内建模块]]></title>
    <url>%2Fpython%2F20190104-af30.html</url>
    <content type="text"><![CDATA[os模块用于处理文件系统中的文件和目录。 工作目录 os.getcwd() 当前脚本文件的工作目录 os.chdir(DIR) 切换工作目录至DIR，默认工作目录为脚本所在目录 目录的增删查改 新建目录 os.mkdir(“A/B/“) 创建一个目录A/B/，不能递归创建，即要求目录A存在 os.makedirs(DIR) 创建一个目录DIR，与上面不同的是，可以递归创建目录 删除目录 os.rmdir(DIR) 当目录DIR为空时删除目录，不为空时报错 os.removedirs(DIR) 待查 列举目录 os.listdir(DIR) 列出直接属于目录DIR的文件和子目录 os.walk(DIR) 遍历目录DIR下所有的文件和目录，返回生成器，返回结果较复杂，待查 文件和目录的重命名 os.rename(OLD_NAME, NEW_NAME) os.system(“COMMAMD_STRING”) 调用shell命令进行重命名 路径操作 路径类型判断 os.path.isfile(PATH) 判断是否为文件，是文件返回True os.path.isdir(PATH) 判断是否为目录，是目录返回 True os.path.exists(PATH) 判断是否存在，存在返回 True os.path.getsize(PATH) 是文件返回文件大小，是目录返回0 路径的切割、合并 os.path.split(‘1/2/3’) 分割成目录和文件，得到 (&#39;1/2&#39;,&#39;3&#39;) os.path.split(‘1/2/3/‘) 分割成目录和文件，得到 (&#39;1/23&#39;,&#39;&#39;) os.splitext(‘path/name.txt’) 分割出文件后缀，得到 (&#39;path/name&#39;, &#39;.txt&#39;) 路径的连接 os.path.join(A, B) 使用默认路径分割符连接两个字符串，得到 &quot;A/B&quot; 常量 os.sep 或 os.path.sep 当前系统下是使用的 路径分割符 os.linesep 当前系统下使用的 行终止符 os.environ 字典：环境变量]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取当前文件所在的目录]]></title>
    <url>%2Fpython%2F20190103-5c31.html</url>
    <content type="text"><![CDATA[记录下常用的方法。 利用os模块12# 当前脚本文件所在的目录（工作目录）os.getcwd() # Get current work directory 这个方法不一定得到正确结果！ 利用内建数组sys.argvsys.argv 数组的第一个值（sys.argv[0]）存储的永远是当前脚本文件的绝对路径。 从这个路径中去掉文件名就是当前脚本文件所在目录的绝对路径。 12345import sysimport os# current file directorycur_fdir = os.path.split(sys.argv[0])[0]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客改造计划]]></title>
    <url>%2Fcargo%2F20190102-19a0.html</url>
    <content type="text"><![CDATA[比较有意思的优化过程。 博文置顶修改hexo-generator-index插件：备份文件node_modules/hexo-generator-index/lib/generator.js并将其中代码替换为：123456789101112131415161718192021222324252627282930313233'use strict';var pagination = require('hexo-pagination');module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; // 两篇文章top都有定义 if(a.top &amp;&amp; b.top) &#123; // 若top值一样则按照文章日期降序排 if(a.top == b.top) return b.date - a.date; // 否则按照top值降序排 else return b.top - a.top; &#125; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） else if(a.top &amp;&amp; !b.top) &#123; return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; // 都没定义按照文章日期降序排 else return b.date - a.date; &#125;); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, &#123; perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: &#123; __index: true &#125; &#125;);&#125;; 设置hexo new生成博文时自动添加top元数据：将top:添加到文件scaffolds\post.md:1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories: top: tags:--- 设置top值即可，top值越大，文章越靠前 fork me on github效果如下： 在themes\next\layout\_layout.swig中搜索&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;找到如下位置：123&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;&lt;!-- 这个地方 --&gt;&lt;header id=&quot;header&quot; class=&quot;header&quot; itemscope itemtype=&quot;http://schema.org/WPHeader&quot;&gt; 添加代码：12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!-- fork me on github begin --&gt;&lt;!-- 默认启用右上角图标，需要换成左上角请更换注释部分 --&gt; &lt;!-- 右上角 --&gt; &lt;a href=&quot;https://github.com/barwe&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt; &lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; class=&quot;fork-me-on-github&quot; style=&quot;position: absolute; border: 0;&quot; aria-hidden=&quot;true&quot;&gt; &lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;/a&gt; &lt;style&gt; .github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; @keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125; @media (max-width:500px)&#123; .github-corner:hover .octo-arm&#123;animation:none&#125; .github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; &#125; &lt;/style&gt; &lt;!-- 左上角 &lt;a href=&quot;https://github.com/barwe&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt; &lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; class=&quot;fork-me-on-github&quot; style=&quot;position: absolute; border: 0; transform: scale(-1, 1);&quot; aria-hidden=&quot;true&quot;&gt; &lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt; &lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt; &lt;/svg&gt; &lt;style&gt; .github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; @keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125; @media (max-width:500px)&#123; .github-corner:hover .octo-arm&#123;animation:none&#125; .github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125; &#125; &lt;/style&gt; &lt;/a&gt; --&gt; &lt;!-- fork me on github end --&gt; 在themes\next\source\css\_custom\custom.styl文件中添加：12345678//在右上角或者左上角添加fork me on github图块.fork-me-on-github &#123; fill: red // 背景色 color: white // 猫的颜色 top: 0 right: 0 // 在右上角 //left: 0 // 在左上角&#125; 这个功能刷新可能需要重启服务器。]]></content>
      <categories>
        <category>cargo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统信息查询]]></title>
    <url>%2Fos%2F20190101-1ff5.html</url>
    <content type="text"><![CDATA[emmmmmm,先记下来…… 系统uname -a：内核、操作系统、CPU信息 head -n 1 /etc/issue：操作系统版本 cat /proc/cpuinfo：CPU信息 hostname：计算机名 lspci -tv：列出所有PCI设备 lsusb -tv：列出所有USB设备 lsmod：列出加载的内核模块 env：查看环境变量 内存与资源free -m：查看内存使用量和交换区使用量 df -h：查看各分区使用情况 du -sh DIR：查看目录DIR的大小，非本人的目录可能要使用 sudo 提权 grep MemTotal /proc/meminfo：查看内存总量 grep MemFree /proc/meninfo：查看空闲内存量 uptime：查看系统运行时间、用户数、负载 cat /proc/loadavg：查看系统负载 磁盘和分区信息mount | column -t：查看挂接的分区状态 fdisk -l：查看所有分区 swapon -s：查看所有交换分区 hdparm -i /dev/hda：查看磁盘参数 dmesg | grep IDE：查看启动时IDE设备检测状况 网络ifconfig：查看所有网络接口的属性 iptables -L：查看防火墙设置 route -n：查看路由表 netstat -lntp：查看所有监听端口 netstat -antp：查看所有已经建立的连接 netstat -s：查看网络统计信息 进程ps -ef：查看所有进程 top：是实现显示进程状态 用户w：查看活动用户 id USERNAME：查看用户USERNAME的信息 last：查看用户登录日志 cut -d: -f1 /etc/passwd：查看系统所有用户 cut -d: -f1 /etc/group：查看系统所有组 crontab -l：查看当前用户的计划任务 服务chkconfig --list：列出所有系统服务 chkconfig --list | grep on：列出所有启动的系统任务]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>Linux实用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04LTS修改软件源]]></title>
    <url>%2Fos%2F20181122-a661.html</url>
    <content type="text"><![CDATA[修改软件源能大大加快软件更新和下载速度。 1、备份原始源： 1sudo mv /etc/apt/sources.list /etc/apt/sources.list.bak 2、新建源： 1sudo vim /etc/apt/sources.list 写入以下内容（对于Ubuntu16.04LTS）： # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # 预发布软件源，不建议启用 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 其他版本的源列表可以在 这里 查看。 3、刷新：sudo apt-get update]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04LTS更新R到3.5.x]]></title>
    <url>%2Fos%2F20181122-ad7f.html</url>
    <content type="text"><![CDATA[重装R时踩得坑。 1、系统换源 2、检查旧版本R 检查是否有旧版本的R：R --verion 卸载旧版本的R：sudo apt-get remove r-base-core 3、安装R3.5需要先添加源： 1sudo apt-add-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/' 添加上面这个源意味着在Ubuntu16.04LTS上可以安装R3.5。 xenial 意为“非洲地松鼠”，Ubuntu的每一个发行版都会有一个奇怪的名字。 然后更新源： 1sudo apt-get update 4、安装R3.5： 1sudo apt-get install r-base r-base-dev]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python中运行shell命令]]></title>
    <url>%2Fpython%2F20181121-b9c.html</url>
    <content type="text"><![CDATA[记录常用方法。 Method 1 123import subprocessstring = subprocess.check_output("COMMAND_STRING", [shell=True]).decode() Method 2 123import osresult = os.system("COMMAND_STRING")]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HttpRequest对象]]></title>
    <url>%2Fit%2F20181121-f755.html</url>
    <content type="text"><![CDATA[每个view函数函数的第一个参数都是HttpRequest对象，包含当前请求URL的一些信息。HttpRequest对象实例： 1234567891011121314151617181920212223## 属性request.path # str类型，请求页面的全路径，不包括域名request.method # str类型，值为'GET'或者'POST'request.GET # QueryDict实例request.POST # QueryDict实例，注意区分POST为空和POST的请求内容为空，判断是否为POST方法使用method属性request.COOKIES # 标准python字典对象(用&#123;&#125;表示)，&#123;str:str&#125;request.FILES # 类字典对象（？），包含所有上传文件# 形式为：XxxDict&#123;name: &#123;'filename':..., 'content-type':..., 'content':...&#125;&#125;# 上面的name变量的值是&lt;input type="file" name="..."&gt;中name属性的值# 只有POST请求并且啥啥啥的FILES属性才会有值，否则为空request.META # 可用的http头部信息字典request.user # django.contrib.auth.models.User对象实例，代表当前登录的用户# 用户未登录则为django.contrib.auth.models.AnonymousUser对象实例# 通过 request.user.is_authenticated() 判断用户是否登录# 需要激活django的AuthenticationMidlleware属性request.session # 当前会话的字典对象，需要激活啥啥啥request.raw_post_data # 未解析的原始post数据## 方法request.__getitem__(key) # 取出GET/POST中的值，优先POSTrequest.has_key(key) # 检查GET/POST是否包含keyrequest.get_full_path() # 返回包含查询字符串的请求路径字符串request.is_secure() # 如果发出的是HTTPS安全请求返回True]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-request(GET+POST)属性与QueryDict类.md]]></title>
    <url>%2Fit%2F20181121-8d33.html</url>
    <content type="text"><![CDATA[在HttpRequest对象中，GET和POST属性的值都是QueryDict的实例。 QueryDict用来处理单键对应多值的情况 123456789101112131415161718qd = django.http.QueryDict(...)#与普通字典一致的方法qd.__getitem__(key) # 返回key对应值列表的最后一个值qd.__setitem__(key, value_list) # GET和POST属性值字典不允许被直接修改，因此此方法只能用于该字典的拷贝上qd.get(key, IF_NONE) # 如果key存在返回key对应值列表的最后一个值qd.update(d) # D为QD(查询字典)或者D(python字典)都可以,如果key存在，执行添加而不是替换qd.items() # 返回键值对，值是key对应的值列表的最后一个值（单值）qd.values() # 跟items一样使用单值逻辑#特有方法qd.copy() # 返回可更改的拷贝（比如可以使用__setitem__)qd.getlist(key) # 返回key对应的python列表qd.setlist(key, value_list) # 无须拷贝直接修改？？？？？qd.appendlist(key, value) # 给已经存在的key的列表中添加一个值qd.setlistdefault(ket, value_list)qd.lists() # 作用与items类似，不执行单值逻辑，也就是说键值对的值是所有值的列表qd.urlencode(key) # 返回查询字符串格式的字符串，如'a=3&amp;a=4&amp;a=5']]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-模板系统-自动转义]]></title>
    <url>%2Fit%2F20181121-b01.html</url>
    <content type="text"><![CDATA[自动转义的产生背景当对模板变量进行替换时，替换字符串中可能含有能产生非预期影响的元素。例如&quot;&lt;script&gt;alert(&#39;hellp&#39;)&lt;/script&gt;&quot;将弹出警告框用户利用这个特点做一些不可描述的事情 —— 跨域脚本（XSS）攻击。 自动转义的详细操作为了防止这种情况出现，django引入自动转义机制，默认开启。以下5 个字符尤其重要： &lt;自动转为&amp;lt &gt;自动转为&amp;gt &#39;自动转为&amp;#39 &quot;自动转为&amp;quot &amp;自动转为&amp;amp 手动关闭自动转义有时候我们确实是希望模板变量被替换成一段HTML代码 来自用户的数据将进行自动转义 使用safe过滤器关闭单个模板变量的自动转义功能 1&#123;&#123; data | safe&#125;&#125; 控制模板块的自动转义 123&#123;% autoescape off %&#125; ...&#123;% endautoescape %&#125; 是否自动转义可根据模板间的继承进行转移 default过滤器的参数（字符串）的自动转义因为这个常亮字符串是由模板作者定义的，默认已经通过了safe过滤器。所以模板作者需要人工转义：定义1&#123;&#123; data|default:"3 &amp;lt; 2" &#125;&#125; 而不是 1&#123;&#123; data|default:"3 &lt; 2" &#125;&#125;]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django官方文档笔记-模板系统-扩展模板系统]]></title>
    <url>%2Fit%2F20181121-c04f.html</url>
    <content type="text"><![CDATA[绝大部分的模板定制是以自定义标签/过滤器的方式来完成的。 怎么编写自己的模板库文件？这里的自定义模板库指的就是自定义模板标签/过滤器。 模板库文件显然有他固定的书写格式，我们需要在模块开头写上： 12from django import templateregister = template.Library() 模块级变量register是自定义模板标签/过滤器的基础数据结构。 实际上我们在定义自己的模板库文件时可以参考官方的写法: django/template/defaultfilters.py文件 django/template/defaulttags.py文件 然后我们基于register来自定义模板标签/过滤器: 自定义模板过滤器：定义 + 注册过滤器本质上就是带参数的python函数（哈哈，没想到吧）。第一个参数应该传递管道符(|)入口的值，其他参数通过:进行传递。例如下面的这个过滤器： 12def cut(value, arg): return value.replace(arg, '') 在模板中可以用来去掉模板变量值中的字符’A’: 1&#123;&#123;somevalue|cut:"A"&#125;&#125; 过滤器总是有可正常使用的返回值，不能触发异常（关于触发异常我的理解是：使用raise抛出一个异常，而不是使用try...except...进行异常捕获）。 不知道你发现没有，在上面我们定义模板过滤器的过程中还没有用到register，实际上当我们定义好模板过滤器后需要对他们进行注册才能正常使用，使用一下语句注册： 1register.filter('cut', cut) //(过滤器名称，函数本身) 实际上我们可以还可以使用@修饰器在定义的时候进行注册： 1234//无参时直接@register.filter@register.filter(name='cut') def cut(value, arg): return value.replace(arg, '') 自定义模板标签先略过,有需要再研究。。POTAL 模板库文件放在哪里好？模板库是Django能够导入的基本结构。 建议的目录结构如下： 为自定义的模板库单独建一个app并在INSTALLED_APPS中注册（只有注册的模板库才能被导入）。 在合适的app根目录下为模板们建一个单独的文件夹。 在这个文件夹下建立__init__.py文件和自定义模板库文件。 自定义的模板库怎么导入到模板中？ 这里注意对模板库和模板的概念进行区分 我们在编写模板时可以使用如下语句来导入我们自定义的模板库： 1&#123;% load 模板库名 %&#125; load模板标签会检查INSTALLED_APPS，只有已安装的app内的模板库才能被加载。这里模板库虽然是存放在某个特定的APP内的，但是load加载模板库时并没有涉及到这个APP。]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模板系统-RequestContext和Context处理器]]></title>
    <url>%2Fit%2F20181121-276f.html</url>
    <content type="text"><![CDATA[视图函数需要传递一个context给模板完成渲染，然后返回给用户完整页面。当向多个不同的模板中传入大量相同的键值对时会多写很多重复的代码，可以使用 RequestContext。 首先定义这些可复用的键值对 12def common_items(request): return &#123;...&#125; 这个函数接受request纯粹是因为里面构造返回值字典时可能会用到换言之，它不是必须的：当你确定返回值字典用不着request完全可以不传参 构造视图函数 12345from django.template import RequestContext def view_1(request, *args, **kwargs): rcontext = RequestContext(request, d&#123;...&#125;, processors=[...]) ... 视图函数的第一个参数必须是request（HttpRequest对象实例）这里我们使用 RequestContext 对象代替 Context 对象构造RequestContext需要注意： 第一个参数时request 第二个参数是字典，代表非公用的键值对 processors是包含Context处理器的列表/元组 将RequestContext对象传递给render_to_response方法进行渲染与Context对象不同，使用RequestContext除了传递处理器外还需要传递一个额外的字典（见上第二个参数）使用关键字context_instance: 1234return render_to_response(模板文件, 非公用字典, context_instance=RequestContext(request, processors=[...]))#这里在构建RequestContext对象的时候并没有传递非公用字典,#而是将该字典传递给了render_to_response方法！！！ 但是频繁的键入processors还是会产生大量的代码（这个是真的懒。。）所以Django设计了全局context处理器：一般在settings.py的类似于’context_processors’的列表中声明，不同版本可能关键字不一样激活相应的处理器RequestContext将自动包含相对应的一部分变量到context中，具体如下： django.core.context_processors.auth: user：一个django.contrib.auth.models.User/AnonymousUser实例 message：当前登录用户的消息列表 perms：当前登录用户的权限 django.core.context_processors.debug: debug：settings.DEBUG值，检测是否处于debug模式，貌似一直为True？ sql_queries：顺序记录每个SQL查询以及耗费时间 这个处理器还需要满足一些其他的条件… django.core.context_processors.i18n: LANGUAGES LANGUAGE_CODE django.core.context_processors.request django.core.context_processors.messages]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[写论文值得收藏的8个网站]]></title>
    <url>%2Fcargo%2F20181106-e8e2.html</url>
    <content type="text"><![CDATA[① OALib 免费论文搜索引擎 ( http://www.oalib.com/ ) ② HighWire 斯坦福学术文献电子期刊 ( https://www.highwirepress.com/ ) ③ Intute 学术资源搜索工具 ( https://www.jisc.ac.uk/ ) ④ FindaRticles 文献论文站点 ( http://findarticles.com/ ) ⑤ Intechopen 免费科技文献 ( https://www.intechopen.com/ ) ⑥ LolMyThesis 哈佛毕业论文分享网站 ( http://lolmythesis.com/ ) ⑦ 万方数据库 ( http://www.wanfangdata.com.cn/index.html ) ⑧ 全国图书馆论文搜索网 ( http://www.ucdrs.superlib.net/ ) 以上资料来自知乎同学，点击查看图片和详情。]]></content>
      <categories>
        <category>cargo</category>
      </categories>
      <tags>
        <tag>资源列表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方差分析1-基本原理、F检验、多重比较]]></title>
    <url>%2Fstat%2F20181022-111b.html</url>
    <content type="text"><![CDATA[一、问题引入1.什么是方差分析？方差分析用于两个及两个以上样本的均数差异的显著性检验。 中文名 方差分析 中文别名 变异数分析、变量分析、F检验 外文名 Analysis of Variance, F test 简称 ANOVA 提出者 R.A.Fisher 2.t检验为什么不能代替方差分析？t检验只适用于两个样本均数间的差异分析，当设计两个以上的样本时只能进行二元拆分，这种操作并不优雅。 1.检验过程繁琐。m个样本需要进行$\mathrm{C}_m^2$组t检验，自寻烦恼。 2.无统一的试验误差，误差估计的精确性和检验的灵敏性低。（？？？） 3.二元切分后进行多次t检验会使犯第Ⅰ类错误（假阳性错误）的概率大大增加。t检验只能将每次的检验犯错误概率控制在$\alpha​$以内，在多次检验时这个概率会逐渐变大。例如一个三样本的均值差异检验共需要进行$C_3^2=3​$次t检验，每一次犯错误的概率为$\alpha​$，这意味着每一次检验不犯错误的概率为$1-\alpha​$，连续三次不犯错误的概率为$(1-\alpha)^3​$，所以这三次t检验犯第一类错误的概率为$1-(1-\alpha)^3=0.142615​$（当$\alpha=0.05​$时），这个概率相比于0.05已经大了很多了。由此可知，当$n{\to}\infty​$时，犯第一类错误的概率将会趋近于1。 二、方差分析的基本原理1.思想多个样本一般可以来自多个不同的处理(treatment)，比如注射不同的药物等，这些处理是我们可以控制的。 不同的处理有可能影响并致使实验结果产生差异，这种影响一般是可以人为控制的，这种效应叫做处理效应(treatment effect)。 对每个个体的观测值不同，除了处理效应施加影响，还包括实验过程中偶然性因素和测量误差的干扰。 2.目的方差分析可以确定上面提到的处理效应和实验误差对总差异的贡献程度。 一般来说，实验误差是合理不可控的，同时它对总差异的影响也是很小的。因此我们可以比较处理效应和实验误差。 而实验误差显然不是我们关心的东西，我们关心的应该是处理间（组间）均值是否有差异，即我们关心应该是处理效应：● 处理效应和实验误差相差不大，意味着处理效应对指标影响不大。● 处理效应比实验误差大很多，处理效应对实验结果差异有很大影响。 3.用途 多样本均数的比较 多因素间相互作用的分析 回归方程的假设假设 方差的同质性检验 三、数学模型根据上面对总差异的定性分析（由处理效应和实验误差构成）建立定量的数学模型。 观测数据记录形式一般如下： 其中每列（1,…,k）代表k个不同的处理，即k个样本；行数代表每种处理的重复次数，即一个样本的样本容量。 1.线性等权加和模型这里采用最简洁的线性模型定量描述上述关系： $$\begin{split}x_{ij}&amp;=\mu+\tau_i+\epsilon_{ij}\\i&amp;=1,…,k\\j&amp;=1,…,n\end{split}$$ $\mu$是总体均数； $\tau_i$是第$i$次处理的处理效应； $\epsilon_{ij}$是第$i$次处理中第$j$个样本的实验误差； $x_{ij}$是第$i$次处理中第$j$个样本的观测值； 上面的线性模型只是简单的对总体均值、处理效应和实验误差进行等权加和进而构造出观测值。这种构造形式可能不是最优的形式，但却是最简单最易理解的形式。这进一步印证了解决问题总是一个化繁为简、由简入繁的过程。（当前更优的模型构建方式？？？） 2.处理效应不总是人为可控的虽然说大多数情况下通过实验设计我们可以控制处理的种类，即控制处理效应，但是这在某些实验中$\tau_i$却是不可控的。根据$\tau_i$是否可控将上面的数学模型分为三类： $\tau_i$取值 泛化能力 固定模型 $\tau_i=\mu_i-\mu$为常数且$\sum_{i=1}^{k}{\tau_i}=0$ 结果不能扩展到其他处理 随机模型 $\tau_i$从$N(0,\sigma^2)$中采样 结果可以扩展到其他处理 混合模型 略略略 略略略 3.样本方差表达式求解首先我们要明确方差求解式的结构：$\frac{平方和}{自由度}$ 下面分别求解分子和分母。 (1) 求分子（平方和）上面的模型虽然说简洁，但毕竟还是个花架子，我们需要把它的各个单项同真正的样本联系起来。 那么怎么将处理效应和随机误差同已知样本联系起来呢？这里有两句话自行体会： 组间的平均数差异是由处理效应引起的：$\overline{x_i}-\overline{x}$ 组内的差异是由随机误差引起的：$x_{ij}-\overline{x_i}$ 然后简单变换： $$\begin{split}(x_{ij}-\bar{x})&amp;=(x_{ij}-\overline{x_i})+(\overline{x_i}-\bar{x})\\(x_{ij}-\bar{x})^2&amp;=[(x_{ij}-\overline{x_i})+(\overline{x_i}-\bar{x})]^2\\&amp;=(x_{ij}-\overline{x_i})^2+(\overline{x_i}-\bar{x})^2+2(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})\end{split}$$ 针对某个特定的处理对样本内所有观测数据求和： $\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2+2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})$ 这里有个玄学项 $2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})$，我们将与 $j$ 无关的项提前： $2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})=2(\overline{x_i}-\bar{x})\sum_{j=1}^{n}(x_{ij}-\overline{x_i})$ 均值具有这样一个特性：一个样本内所有观测数据到均值的距离（包含正负号）和为0。 所以有这样一个结论：$2\sum_{j=1}^{n}(x_{ij}-\overline{x_i})(\overline{x_i}-\bar{x})=0$ 即$\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2$ 然后我们将不同处理的观测数据的离均差平方进行累加： $\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\bar{x})^2=\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2+\sum_{i=1}^{k}\sum_{j=1}^{n}(\overline{x_i}-\bar{x})^2$ 化简得： $\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\bar{x})^2}_{SS_T}=\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n}(x_{ij}-\overline{x_i})^2}_{SS_e}+\underbrace{n\sum_{i=1}^{k}(\overline{x_i}-\bar{x})^2}_{SS_t}$ 即：$SS_T(总平方和)=SS_e(组内平方和)+SS_t(组间平方和)$ 预算 $$\begin{split}T&amp;=\sum_{i=1}^{k}\sum_{j=1}^{n}x_{ij}\\T_i&amp;=\sum_{j=1}^{n}x_{ij}\\C&amp;=\frac{T^2}{kn}\end{split}$$ 得 $$\begin{split}SS_T&amp;=\sum_{i=1}^{k}\sum_{j=1}^{n}x_{ij}^2-C\\SS_t&amp;=\frac{1}{n}\sum_{i=1}^{k}T_i^2-C\\SS_e&amp;=SS_T-SS_t\end{split}$$ (2) 求分母（自由度）分母就是与$SS_T$、$SS_t$和$SS_e$相关的自由度$df$: $$\begin{split}df_T&amp;=nk-1\\df_t&amp;=k-1\\df_e&amp;=df_T-df_t\\&amp;=k(n-1)\end{split}$$ (3) 求方差组间方差 $$\begin{split}s_t^2&amp;=\frac{SS_t}{df_t}\\s_e^2&amp;=\frac{SS_e}{df_e}\end{split}$$ 四、$F$ 检验1.$F$ 值的定义$F$ 值的定义：$$F=\frac{s_1^2}{s_2^2}$$这里 $s_1^2$ 和 $s_2^2$ 都是随机采样于正态总体($\mu$, $\sigma$)，它们可能具有不同的样本容量（自由度）。习惯上，我们让 $F$ 值大于1，即大方差做分子，小方差做分母。 2.公式迁移在方差分析中，我们计算出了两个方差：组间方差 $S_t^2$ 和组内方差 $S_e^2$，它们分别代表了处理效应和实验误差，即$$F=\frac{S_t^2}{S_e^2}=\frac{处理效应}{实验误差}$$ 对于上面的定义需要解释两点： (1) 按照 $F$ 值的定义，分子和分母应该来自同一个正态总体 如果我们假设处理效应和实验误差相差不多（这将作为我们假设检验的原假设/无效假设/零假设），即 $S_t^2{\approx}S_e^2$，此时我们可以认为 $S_t^2$ 和 $S_e^2$ 来自于同一个正态总体。 (2) 按照 $F$ 值的定义，分子是大方差，分母是小方差 方差分析的目的是确定 $S_t^2$（处理效应）和 $S_e^2$（实验误差）的相对大小，由于实验误差这种东西一般情况下都是不可控的，而且也不会很大，所以我们认为一般只存在一下两种情况： 处理效应比实验误差大得多 处理效应与实验误差相差不多 3.$F$ 临界值表 五、多重比较比较其中两组处理平均数间差异的显著性，本质上与t检验没有任何区别 1.最小显著差数法（LSD）Least Significant Difference (1) 检验方法本质上是两均数的t检验。 计算达到差异显著的最小差数，记为 $LSD_\alpha$ $$LSD_\alpha=t_\alpha{\cdot}S_{\overline{X_1}-\overline{X_2}}$$ 其中 $S_{\overline{X_1}-\overline{X_2}}$ 为平均数差数的标准误： $$\begin{split}S_{\overline{X_1}-\overline{X_2}}&amp;&amp;=\sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}\\&amp;&amp;=\sqrt{S_e^2{\cdot}(\frac{1}{n_1}+\frac{1}{n_2})}\\&amp;&amp;=\sqrt{\frac{2S_e^2}{n}}{\quad}if{\quad}n_1=n_2=n\end{split}$$ 为什么令 $S_1^2=S_2^2=S_e^2$ ？上述公式的推导本身基于我们做出的无效假设（处理效应与实验误差相差不多）！ ☺ 查表的自由度依据？因为本质上是两个均数差异的t检验，因此需要查t临界值表。因为均数差数标准误最终化归到了 $S_e^2$，因此查表的自由度依据的是 $df_e$ 而不是 $df_t​$。 将两个处理平均数的差值绝对值 $\overline{x_1}-\overline{x_2}$ 与 $LSD_\alpha$ 进行比较 (2) 结果表示结果表示方法仅仅是用于优化两两比较次数，在使用结果表示方法之前都需要计算 $LSD_\alpha$。 a. 标记字母法 b. 梯形比较法/三角形法 2.最小显著极差法（LSR）Least Significant Ranges (1) 新复极差检验（SSR）新复极差法用于方差分析后的两两比较，有助于减少第二类错误，但是会增加第一类错误。 原假设仍然是假设两个均值无差异：$\mu_A-\mu_B=0$ 步骤如下： 1.平均数降序排序 2.计算平均数标准误：$$S_{\bar{X}}=\sqrt{\frac{S_e^2}{n}}$$ 这里两组处理的样本容量是一致的：$n_1=n_2=n$，实验设计时一般也不会设计不一致这种骚操作。 3.根据自由度 $df_e$ 和排序平均数中相应两个数之间包含的平均数个数M查SSR值表，计算最小显著极差值（LSR值）： $$LSR_\alpha=SSR_\alpha{\cdot}S_{\bar{X}}$$ M值是个什么东西？排序平均数中相应两个数之间包含的平均数个数，即索引值差+1，相邻两个均数的M值为2。 (2) q检验q检验也叫NK检验，与SSR检验十分相似，不同的是在第二步计算 $LSR_\alpha$ 时使用的是 $q_\alpha$ 值，而不是 $SSR_\alpha$ 值，因此需要查 $q_\alpha$ 值表。 $$LSR_\alpha=q_\alpha{\cdot}S_{\bar{X}}$$ 3.各个方法应用场景上面提到的LSD检验、SSR检验（新复极差检验）、q检验分别有自己的适用场景 因此应用时 对于精度要求高的检验，请使用q检验； SSR检验适用于一般的检验； LSD检验适用于将各个实验组与对照组进行比较。 六、方差分析的基本步骤从上面的过程我们可以总结出方差分析的三个大步骤： 对样本数据的总平方和和总自由度分解为各个变异因素的平方和和自由度； 进行 $F$ 检验，检验各个变异因素在总变异中的重要程度； 组间均数进行两两比较。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验-显著性假设检验]]></title>
    <url>%2Fstat%2F20181018-327a.html</url>
    <content type="text"><![CDATA[不管是下面的单样本还是双样本都有两个大前提：总体服从正态分布、方差齐性！ 1. 单样本平均数$\bar{x}$的差异显著性检验这里指的实际上是一个样本（的平均数）和总体（的平均数）间的差异显著性，因为我们只有一个样本，这就要求总体参数（这里是均值）已知，这通常是一个理论值、经验值或者期望值。 (1). 大样本（n≥30）的均数检验大样本通常指的是n≥30，此时样本的均值将服从于正态分布，我们可以将其标准化为标准正态分布，进而使用u检验进行假设检验。 a. 总体方差$\sigma^2$已知时总体方差$\sigma^2$已知时直接计算相应的u值： $$\sigma_{\bar{x}}=\sqrt{\frac{\sigma^2}{n}}=\frac{\sigma}{\sqrt{n}}$$ $$u=\frac{\bar{x}-\mu_0}{\sigma_{\bar{x}}}=\frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}{\sim}N(0,1)$$ 式中$\bar{x}$是样本均值，$\mu_0$是总体均值，$\sigma$是总体方差，$n$是样本容量。 b. 总体方差$\sigma^2$未知时在大样本的条件下直接用样本方差$s^2$代替总体方差$\sigma^2$，计算u值的方法同上： $$s_{\bar{x}}=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$$ $$u=\frac{\bar{x}-\mu_0}{s_{\bar{x}}}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}{\sim}N(0,1)$$ 样本方差$s^2$的计算： $$s^2=\frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\bar{x})^2}$$ (2). 小样本（n&lt;30）的均数检验小样本只能使用t检验，此时我们要求的是t值而不是u值。 小样本的方差$s^2$与总体的方差$\sigma^2$往往差距较大。 t检验时查临界值表依据的是自由度df，这个值通常是n-1。 $$s^2=\frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\bar{x})^2}$$ $$s_{\bar{x}}=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$$ $$t=\frac{\bar{x}-\mu_0}{s_{\bar{x}}}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}{\sim}t(n-1)$$ 2. 两个样本平均数$\bar{x_1}$和$\bar{x_2}$间的差异显著性检验(1). 两个大样本（$n_1\ge30$且$n_2\ge30$）的均数检验当两个样本都是大样本时，与单样本检验类似，仍可以使用u检验。 a. 如果两个样本的方差$\sigma^1$和$\sigma^2$已知$$\sigma_{\bar{x_1}-\bar{x_2}}=\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$$ $$u=\frac{\bar{x_1}-\bar{x_2}}{\sigma_{\bar{x_1}-\bar{x_2}}}{\sim}N(0,1)$$ b. 如果两个样本的方差$\sigma^1$和$\sigma^2$未知使用$s_{\bar{x_1}-\bar{x_2}}$来代替$\sigma_{\bar{x_1}-\bar{x_2}}$: $$s_{\bar{x_1}-\bar{x_2}}=\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}$$ $$u=\frac{\bar{x_1}-\bar{x_2}}{s_{\bar{x_1}-\bar{x_2}}}{\sim}N(0,1)$$ (2). 两个小样本（$n_1\le30$且$n_2\le30$）的均数检验小样本使用t检验。 按照实验设计的不同，两个样本可以是独立样本，也可以是配对样本。 A. 配对样本均数的t检验此时两组数据将形成对子，因此两个样本的样本容量是一样的，即$n_1=n_2$。 配对可以减少个体差异对实验的影响，但是也会增加数据处理的复杂度。 配对情况通常有以下三类： 两个相似个体组成一对进行不同的实验处理：此时并没有完全消除个体差异的影响。 同一个个体同时进行两种不同的处理：处理的空间位置不同。 同一个个体先后进行两种不同的处理：不同时间个体的状态可能不同。 简单理解就是：第一类使用相似的两个个体，第二类同时使用同一个个体不同的部位，第三类先后使用同一个体相同的部位。 计算过程如下： $$d_i=x_i-y_i$$ $$\bar{d}=\frac1n\sum_{i=1}^{n}d_i$$ $$s_d^2=\frac{1}{n-1}\sum_{i=1}^{n}(d_i-\bar{d})^2$$ $$t=\frac{\bar{d}}{s_d/\sqrt{n}}$$ $$df=n-1$$ 查t检验临界值表时注意使用df值而不是n值。 B. 独立样本均数的t检验对于独立样本不存在配对情况，因此$n_1$和$n_2$往往不同。 两独立样本对应的总体方差可能不同，因此： (a). 当两样本方差相同或者假设相同时此时计算自由度$v=n_1+n_2-2$ 计算两个样本的均值$\bar{x_1}$和$\bar{x_2}$、方差$S_1^2$和$S_2^2$。 $$S_c^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{v}$$ $$S_{\overline{X_1}-\overline{X_2}}=\sqrt{(\frac{1}{n_1}+\frac{1}{n_2})S_c^2}$$ $$t=\frac{\overline{x_1}-\overline{x_2}}{S_{\overline{X_1}-\overline{X_2}}}$$ (b). 经过F检验两样本方差不同时如果$n_1=n_2=n$: 计算方法与假设方差相同时一致，只是自由度为$n-1$而不是$2n-2$。 没看懂，先抄下来…… 3. u分布与t分布的区别和联系u检验理论上要求大样本（即$n\to+\infty$），但是这在实际情况中是不可能的，所以教科书设置标准为n=30。 按照中心极限定理：在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。这就是说，只有在大（无穷）样本时，样本的均值才会服从正态分布。按照正态分布的概率密度函数： $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2{\sigma}^2}}$$ 它只与总体的参数$\mu$和$\sigma$有关，而与样本容量n无关。 有限样本特别是小样本下样本均值是不严格服从正态分布的，此时有个更好的分布可以刻画样本均值，这就是t分布。 $$f(t)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{\frac{-(\nu+1)}{2}}$$ $$\nu=n-1$$ 它只与样本容量n有关！ 但是请注意，当$n\to+\infty$时，t分布将会十分接近标准正态分布，有图为证： 所以我们可以说，标准正态分布只是t分布在$n\to+\infty$时的一个特例！ 因此在有限样本容量情况下，t检验应该更合理和精确，所以在t检验可行的情况下应该优先采用t检验。什么时候不可行呢？t检验是要根据自由度查表的，如果自由度值不合适导致无法查表，此时就比较尴尬了。也不知道现在这个问题是怎么解决的~。 下面是一张常见的t分布临界值表，我们可以发现当自由度超过30以后就不连续了~（不是绝对的，但道理大抵如此吧） 同时由上图我们知道，当n=30的时候，t分布和u分布已经十分近似了，所以才有了这个样本容量大于30时用u检验的规则。 所以 ▶ u检验适用于大样本的情景，t检验适用于小样本的情景。 ▶ t分布更加符合有限样本的场景，理论上t检验更加精确。 4. 两独立样本的方差的齐性检验原假设$H_0$: $\sigma_1=\sigma_2$ (方差齐性) 备择假设$H_1$: $\sigma_1\ne\sigma_2$ (方差不齐性) $$F=\frac{max(S_1^2,S_2^2)}{min(S_1^2,S_2^2)}$$ $$v_1=n_1-1$$ $$v_2=n_2-1$$ 查F检验临界值表。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验-假设检验的基本原理]]></title>
    <url>%2Fstat%2F20181017-aa04.html</url>
    <content type="text"><![CDATA[显著性检验的意义以均数差异显著性检验来说，两样本平均数为$\bar{x_1}$和$\bar{x_2}$，样本背后对应的总体均值相应的为$\mu_1$和$\mu_1$。下式十分容易理解： $$\bar{x_1}=\mu_1+\bar{\epsilon_1}$$ $$\bar{x_2}=\mu_2+\bar{\epsilon_2}$$ 其中$\bar{\epsilon_1}$和$\bar{\epsilon_2}$均表示实验误差。 当我们检验这两个样本的均数是否存在差异时依据的实际上是下面这个式子： $$\bar{x_1}-\bar{x_2}=(\mu_1-\mu_2)+(\bar{\epsilon_1}-\bar{\epsilon_2})$$ 即样本均数间的差异（表面差异）是由总体间的真实差异和抽样误差共同决定的。 所以显著性检验的目的就是：判明表面差异$\bar{x_1}-\bar{x_2}$是来源于总体真实差异$\mu_1-\mu_2$还是抽样误差$\bar{\epsilon_1}-\bar{\epsilon_2}$。 原假设与备择假设的选取统计学为实际目的服务，我们对某个总体进行抽样都是有确切目的的。换句话说，我们往往希望通过自己的努力搜集一定的证据去证明一些东西，这实际上是一个立场问题。譬如质量检测人员希望检测出劣质产品而不是合格产品，因为其工作内容就是努力找出不合格的产品；给作物施加改良药物希望的是作物产量提高而不是一成不变… 一般情况下我们都希望总体的参数存在差异（当然这也不是绝对的~），这是我们假设检验的方向，此时我们进行试验的初衷也是希望改变总体的参数已完成我们的工作目标。我们将与假设检验方向相同的假设称为备择假设，与备择假设再逻辑上完备互斥的假设称为原假设（无效假设、零假设）。 所以原假设往往希望总体参数不发生变化，即表面差异由抽样误差决定。我们假装接受原假设，然后对原假设进行检验，实际上就是对表面差异由抽样误差决定这一基于原假设的结论进行检验。后续通过求解P(表面差异由抽样误差决定)通过小概率事件不可能发生原理对原假设予以接受或者拒绝。 ◆ 由于原假设假定总体参数未发生变化，所以等号往往出现在原假设中 ◆ 原假设与备择假设逻辑上完备互斥，且原假设由备择假设确定 ◆ 备择假设的选择往往和实验人员的立场息息相关 下面对案例进行详细分析： 案例1 零件质量问题背景：一汽车配件生产企业生产的某种汽车零件长度标准为70毫米，为对零件质量进行控制，质量监测人员需要对生产线上的一台加工机床进行检查，以确定这台机床生产的零件是否符合标准要求。如果零件的平均长度大于或小于70毫米，则表明该零件质量不正常，必须对机床进行检查。分析：“70毫米”是（合格产品）总体的参数，质量检测人员将进行抽样，需要比较这个样本对应的总体和合格产品总体。质量检测人员希望通过搜集证据证明的是样本不合格（其工作任务总是希望找出不合格的产品），因此备择假设为“零件的平均长度大于或小于（不等于）70毫米”（这是检测人员希望通过努力证明的结果）。由备择假设确定原假设为“零件的平均长度等于70毫米”。 案例2 合格率问题背景：一采购商需要采购一批构件，某供应商称其提供的构件合格率超过95%，为了检验其可信度，采购商随机抽取了一批样本进行检验。 分析：“95%”是总体的参数。采购商的立场总是尝试证明构件合格率不超过95%，这也是总体参数发生改变的假设，因此我们将之作为备择假设。由备择假设确定的原假设为“构件合格率≥95%”。 案例3 成分含量问题背景：一乳制品生产商生产的奶粉被媒体曝光某种营养成分大大低于国家规定的2%的含量标准。 ▶ 现质量监督部门从保护消费者权益角度出发，对曝光的奶粉进行抽查，请对检验做出假设。 分析：质量监督部门的立场是通过搜集证据证明该成分含量低于2%，因此备择假设为“成分含量&lt;2%”，原假设为“成分含量≥2%”。 ▶ 如果生产商相信其产品不存在上述问题，判断这是由竞争对手操纵的不正当竞争手段，并委托市场上的第三方检测机构进行检测，请对这一检验作出假设。 分析：第三方检测机构的立场是通过自己的努力证明成分含量是合格的（因为这也是生产商希望看到的，如果这个检测机构跟金主爸爸做对就是有病了~），因此备择假设为“成分含量≤2%”，原假设为“成分含量&gt;2%”。 ◆ 有趣的是，上面同一个情景却因为立场和初衷的不同导致备择假设和原假设完全相反，这表明统计学真的源于生活啊hahaha （注：尊重原创，以上案例均来自于博客。） 显著性检验的一般步骤1.提出假设大多数情况下都是： $$H_0: \mu=\mu_0$$ $$H_1: \mu\ne\mu_0$$ 2.确定显著水平显著水平$\alpha$（significance level）即否定$H_0$的概率标准，人为规定的小概率事件分界线。生物统计学中通常取0.05和0.01。 3.计算显著概率在假定原假设的条件下求表面差异是由抽样误差造成的的概率。 对于总体参数μ已知、单样本均值$\bar{x}$可求条件下，表面差异为$\bar{x}-\mu$，这个不难理解哈。 ◆ 判定是两尾检验还是单尾检验。 4.统计推断根据小概率事件实际不可能性对原假设予以接受或者否定。 检验结果被表述为“在α水平上$\bar{x}$与μ差异不显著/显著/极其显著”。 不显著：|u|&lt;1.96，即p&gt;0.05 显著：1.96≤|u|&lt;2.58，即0.01&lt;p≤0.05 极其显著：|u|≥2.58，即p≤0.01 显著水平与两种类型的错误实际上原假设正确，但我们却否定了原假设而接受备择假设时会犯第Ⅰ类错误：原假设正确意味着总体参数本来是相同的，但是我们推断认为他们不同，即将抽样误差错判为了真实差异。 犯第Ⅰ类错误的概率是α，因此减少犯第Ⅰ类错误的概率只需要减少α的值即可。 本来我们应该否定原假设，但是推断后却没有否认原假设时会犯第Ⅱ类错误：本应该否认原假设意味着总体参数间是存在差异的，但是我们没有否认原假设说明我们将这个真实差异错判为了抽样误差。 当表面差异很小或者抽样误差很大时就十分容易犯第Ⅱ类错误。 表面差异或许不可控制，但是我们可以通过增大样本量来减小抽样误差 ◆ 第Ⅰ类错误拒绝了正确的原假设，第Ⅱ类错误接受了错误的原假设。 ◆ 第Ⅰ类错误又叫做假阳性错误：因为我们接受了错误的备择假设。 两尾检验和一尾检验◆ 两种检验的适用情况？如果仅仅检验是否存在差异，即不在乎谁大谁小，采用两尾检验； 凭经验和专业知识断定取大（或者取小），采用一尾检验。 ◆ 一尾检验的临界值怎么确定？ 对于两位检验可以直接查表获取临界值，一尾检验需要先转化为两尾检验再求临界值，如上图所示：$u_{2\alpha}$的值可以直接通过两尾检验的表获取。所以有下面的这个公式： $$一尾检验的u_{\alpha}=两尾检验的u_{2\alpha}$$ $$一尾检验的u_{0.05}=两尾检验的u_{0.10}=1.64$$ $$一尾检验的u_{0.01}=两尾检验的u_{0.02}=2.33$$]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[latex数学符号表]]></title>
    <url>%2Fcargo%2F20181017-aedb.html</url>
    <content type="text"><![CDATA[LaTeX让你写公式爽到爆！ 重音符号 $\overline{m+n}$，代码：\overline{m+n} → \overline{} $\underline{m+n}$，代码：\underline{m+n} → \underline{} $\overbrace{a+b+\cdots+z}^{26}$，代码：\overbrace{a+b+\cdots+z}^{26} → \overbrace{}^{} $\underbrace{a+b+\cdots+z}_{26}$，代码：\underbrace{a+b+\cdots+z}{26} → \underbrace{}&#95;{} 为避免markdown将 _..._ 解释为斜体，可用 &amp;#95; 替换其中一个 _。 符号顶部箭头，常用来表示向量： 显示 代码 $\vec{a}$ \vec{a} $\overrightarrow{AB}$ \overrightarrow{AB} $\overleftarrow{AB}$ \overleftarrow{AB} 独立符号希腊字符 其他数学符号 其他AMS数学符号 非数学符号 字体 关系符号关系符 AMS关系符 AMS否定关系符 运算符号运算符 大运算符 AMS运算符 箭头箭头 AMS箭头 括号及定界符定界符 AMS定界符 大定界符 空格 函数 函数 举例 代码 分数 $\frac{x-y}{x+y}$ \frac{x-y}{x+y} 累加 $\sum_{i=1}^{100}{x_i^2}$ \sum_{i=1}^{100}{x_i^2} 累乘 $\prod_{i=1}^{100}{x_i}$ \prod_{i=1}^{100}{x_i} 偏微分 $\partial{y}$ \partial{y} 一重积分 $\int_{-1}^{1}{x_3}dx$ \int_{-1}^{1}{x_3}dx 排列组合 $\binom{n}{k}$ \binom{n}{k} 排列组合 $\mathrm{C}_n^k$ \mathrm{C}_n^k 其他小技巧基本运算的格式控制 显示 代码 $\frac{a}{b}$ \frac{a}{b} $a_{1}^{(2)}$ a_{1}^{(2)} $\sqrt{5}$ \sqrt{5} $\sqrt[3]{5}$ \sqrt[3]{5} 公式末尾序号 自动标签（不建议使用）：\begin{equation}...\end{equation} 手动标签：...\tag{...} 单个公式的连等换行12345$$\begin&#123;split&#125;y&amp;=x^2-4x+5\\\\&amp;=(x^2-4x+4)+1\\\\&amp;=(x-2)^2+1\end&#123;split&#125;\tag&#123;xx&#125;$$ $$\begin{split}y&amp;=x^2-4x+5\\&amp;=(x^2-4x+4)+1\\&amp;=(x-2)^2+1\end{split}\tag{xx}$$ 要点如下： 公式代码用begin{split}...\end{split}标识以说明需要多行显示一个单行公式； \tag{...}需要放到split区域外面，因为它不属于公式代码； 换行的地方用\\标识，如果在windows系统下可能是\\\\； 在所有行中使用且仅使用一个&amp;标明对齐位置。 分段函数Sign(x)= \begin{cases} 1, & \text{if $x$ > 0;}\\\\ 0, & \text{if $x$ = 0;}\\\\ -1, & \text{else} \end{cases} $$Sign(x)=\begin{cases} 1, &amp; \text{if $x$ &gt; 0;}\\ 0, &amp; \text{if $x$ = 0;}\\ -1, &amp; \text{else}\end{cases}$$ 矩阵12345$$\left[\begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\\\4 &amp; 5 &amp; 6 \\\\7 &amp; 8 &amp; 9 \end&#123;matrix&#125;\right]\tag&#123;2&#125;$$ $$\left[\begin{matrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{matrix}\right]\tag{2}​$$]]></content>
      <categories>
        <category>cargo</category>
      </categories>
      <tags>
        <tag>知识手册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[估计量选择的原则]]></title>
    <url>%2Fstat%2F20180928-e722.html</url>
    <content type="text"><![CDATA[以点估计为例，我们可以通过矩估计法和最大似然估计法对总体的某个参数进行估计。我们通过不同的方法对同一个参数的估计结果可能不同，因此我们需要选择一个最好的估计量。 下面是选择估计量时常用的三个标准： 无偏性什么是无偏性？若${X_1,X_2,\cdots,X_n}$是总体的一个样本，样本容量为$n$，$\theta$是与总体分布相关的一个待估参数。 我们基于样本${X_1,X_2,\cdots,X_n}​$构造了一个估计量$\hat{\theta}=\theta(X_1,X_2,\cdots,X_n)​$，当这歌估计量的期望$E(\hat{\theta})​$存在且$E(\hat{\theta})=\theta​$时称$\hat{\theta}​$是$\theta​$的无偏估计量。 简而言之，估计量的期望等于待估参数就是无偏估计。 无系统误差无偏估计意味着没有系统误差，那么有偏估计就意味着有系统误差，而系统误差是可以通过校正去除的，所谓我们可以对有偏估计进行无偏化处理得到完成无偏估计，典型的例子就是方差估计。 期望是无偏的不论总体的分布形式如何，k阶样本矩（样本的k阶原点矩）$A_k=\frac1n\sum_{i=1}^{n}X_i^k$都是k阶总体矩（总体的k阶原点矩）$\mu_k$的无偏估计。只需证明$E(A_k)=\mu_k$即可。 样本常见的数字特征有：样本均值、未修正样本方差、修正样本方差、样本k阶原点矩和样本k阶中心矩。 这意味着：样本均值总是总体期望的无偏估计。 方差是有偏的对于方差存在的总体，用未修正的样本方差$\hat{\sigma}^2=\frac1n\sum_{i=1}^{n}(X_i-\bar{X})^2$来估计总体方差$\sigma^2$是有偏的。 证明：针对样本，由$E(X^2)=[E(X)]^2+D(X)$知$$\begin{split}\hat{\sigma}^2=D(X)&amp;=E(X^2)-[E(X)]^2\\&amp;=A_2-\bar{X}^2\end{split}$$因为期望是无偏的，所以$$\underbrace{E(A_2)=}_{期望无偏}\mu_2\underbrace{=\sigma^2+\mu^2}_{总体期望和方差的关系}$$上式中 $\mu_2$ 是总体的二阶原点矩，$\mu$ 是总体的一阶原点矩（期望）。对于复合随机变量$\bar{X}^2$有$$E(\bar{X}^2)=D(\bar{X})+[E(\bar{X})]^2=\frac{\sigma^2}n+\mu^2$$证明$D(\bar{X})=\frac{\sigma^2}n$:$$\begin{split}D(\bar{X})&amp;=D(\frac{X_1+X_2+\cdots+X_n}n)\\&amp;=\frac1{n^2}D(X_1+X_2+\cdots+X_n)\\&amp;=\frac1{n^2}[D(X_1)+D(X_2)+\cdots+D(X_n)]\\&amp;=\frac1{n^2}(\underbrace{\sigma^2+\sigma^2+\cdots+\sigma^2}_{n个})\\&amp;=\frac{n\sigma^2}{n^2}\\&amp;=\frac{\sigma^2}n\end{split}$$所以$$\begin{split}E(\hat{\sigma}^2)&amp;=E(A_2-\bar{X}^2)\\&amp;=E(A_2)-E(\bar{X}^2)\\&amp;=(\sigma^2+\mu^2)-(\frac{\sigma^2}n+\mu^2)\\&amp;=\frac{n-1}n\sigma^2\ne\sigma^2\end{split}$$即证非修正的样本方差$\hat{\sigma}^2$是总体方差的$\sigma^2$的有偏估计。 我们令$$S^2=\frac{n}{n-1}\hat{\sigma}^2=\frac1{n-1}(X_i-\bar{X}^2)$$此时有$$E(S^2)=E(\frac{n}{n-1}\hat{\sigma}^2)=\frac{n}{n-1}E(\hat{\sigma}^2)=\sigma^2$$即得修正的样本方差$S^2$是总体方差的$\sigma^2$的无偏估计。 这就是为什么样本方差为什么总是除以n-1而不是n的原因~ 有效性当我们通过无偏性判断出多个无偏估计量$\hat{\theta_1}$、$\hat{\theta_2}$、…时我们需要进一步筛选。 由于无偏估计量的期望总是待估参数，因此方差越小越好 $$\hat{\theta_{opt}}=\mathop{\arg\min}_{i=1,2,\cdots}\hat{\theta_i}​$$ 相合性相合性指的是当样本容量$n\to\infty$时$\hat{\theta}(X_1,X_2,\cdots,X_n)$依概率收敛于$\theta$。 相合性是估计量最基本的特性，估计量可以有偏但不能不依概率收敛。]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数估计-点估计]]></title>
    <url>%2Fstat%2F20180921-469f.html</url>
    <content type="text"><![CDATA[参数估计参数估计作为一个名词短语，实际含义指的是估计参数，意为估计总体分布参数。事实上我们能观测到的数据成为样本，所以参数估计的内容就是通过样本对总体分布的某些指标/参数进行估计。 参数估计分为点估计和区间估计。 其中点估计主要包含矩估计、极大似然估计。 矩估计基于矩的概念对总体分布的均值μ和方差σ进行估计，总体分布的其他参数通过μ和σ进行推断。 我们不需要知道总体分布的分布形式就可以进行矩估计，但是当总体分布已知时矩估计却没有彻底利用已知信息，显然不是一种较好的估计方法。 矩随机变量X的期望是E[X] 令k是自然数，a为任意实数：期望$E[(X-a)^k]$称为随机变量X对实数a的k阶中心矩，期望$E[X^k]$称为随机变量X的k阶原点矩。 矩估计矩估计基于总体的k阶矩等于样本的k阶矩，即假设样本的期望和方差与总体的期望与方差相同。 为什么呢？这里引用一个不加证明的专业说法： 样本的k阶原点矩$\frac{1}{n}\sum_{i=1}^nX_i^k$依概率收敛于总体的k阶原点矩$\mu_k$同理，样本矩的连续函数也将依概率收敛于总体矩的连续函数 什么叫依概率收敛？ $X_n$依概率收敛于$X$，记做$X_n\xrightarrow{P}X$，表示：对$\forall\epsilon&gt;0$，当$n{\rightarrow}{\infty}$时，$P(|X_n-X|\leqslant\epsilon)\rightarrow1$。 定义式如下： 样本 总体 对$\overline{X}$的k阶中心矩 $\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})^k$ $E[(X-E[X])^k]$ k阶原点矩 $\frac{1}{n}\sum_{i=1}^nX_i^k$ $E[X^k]$ 基于一阶样本原点矩推测总体期望： $$EX=E[X]\approx\frac{1}{n}\sum_{i=1}^{n}X_i=\overline{X}$$ 基于二阶样本中心矩推测总体方差： $$DX=D[X]\approx\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})^2$$ 通过上式我们发现，因为在估计期望和方差的过程中并未涉及到总体分布的形式，因此上述估计式实际上是通用的。 极大似然估计使用极大似然估计，总体的分布形式一定要已知，不然没法进行计算。 极大似然估计基于极大似然原理：概率更大的事件在一次观测中更容易出现。 因此，我们求的是使当前样本出现概率最大的那组参数。 我们要想求出这个/这组参数，就必须先构建一个/一组方程。 通常这个/这组方程由某个函数求导/求偏导得出，这个函数就叫做似然函数，常记做 $L(\theta)$。 构造似然函数1.离散型随机变量：$P(X=x)=p(x;\theta)$ 联合概率 $ L(\theta)=\prod_{i=1}^{n}(x_i;\theta)=p(x_1;\theta)p(x_2;\theta){\cdots}p(x_n;\theta) $ 2.连续性随机变量：$P(X=x)=\int_{-\infty}^{x}f(t;\theta)dt$ 联合概率密度 $ L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)=f(x_1;\theta)f(x_2;\theta){\cdots}f(x_n;\theta) $ 求解似然函数取对数 -&gt; 求导/求偏导 -&gt; 求解得到驻点 对于一般属于指数分布族的分布形式（正态分布、泊松分布、伯努利分布…），令导函数为0的点（驻点）一般都是极值点，但是也有奇葩特例。 贝叶斯估计这里只简述贝叶斯估计的思想。 最大似然估计将参数θ看做一个确定变量，而贝叶斯估计将参数θ看做一个服从一定先验分布的随机变量。 我们期望通过p(x|θ)求出后验概率p(θ|x)，同时p(θ|x)在真实的θ处有一个峰，于是乎我们就求得了真正的θ，耶耶耶 搬运个更加直观的图：]]></content>
      <categories>
        <category>stat</category>
      </categories>
      <tags>
        <tag>UCAS生物统计与实验设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程思想-闭包]]></title>
    <url>%2Fpython%2F20180912-a712.html</url>
    <content type="text"><![CDATA[什么是闭包（Closure）？ 先看看wiki百科是怎么解释的： In programming languages, a closure(also lexical closure or function closure) is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created. A closure (unlike a plain function) allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope. 之我见：我们一般说起将属性和方法进行封装就会想到类。闭包的作用类似于类，只是它的专能性似乎更强。 用栗子来解释闭包为什么会和类相似： 123456def closure_def(x): y = 1 z = 2 def fn(): print((x+y)*z) return fn 对于上面这个简单的栗子，我们发现以下几个特点： y 和 z 是函数 closure_def 作用域下的局部变量 函数 fn 定义在 函数 closure_def 的内部，同时它将直接调用 y 和 z 函数 closure_def返回的是函数 fn 的引用 我们获取一个闭包： 123closure_1 = closure_def(4)closure_1()#输出结果为 10 上述的 closure_1 就是传说中的闭包。因此，闭包本质上还是一个函数,我们通过 closure_1() 调用这个函数后将会输出结果10. 其实到这里我还是没弄清楚闭包与类到底相似在哪里。但是，这里有个很有意思的东西：我们通过 closure_def(4) 调用了闭包定义函数后，该函数内的局部变量（y和z)应该就失效了，但是函数fn实际上却被我们保存了下来（在closure_1中)。然而，这个closure_1需要使用y和z，按照我们前面的逻辑，这两个变量已经失效了，这就产生了一个十分尴尬的问题。 真实情况并非这样，y和z变量明显还存在于内存中，并且它们与这个fn函数绑定在一起了。这就是闭包的特性：将y和z这两个变量和函数fn绑定在一起，就好比类中将属性与方法绑定一样。 在python中，函数同基本数据类型一样同属于一级类对象，函数名就是这个一级类实例的引用。对于闭包，函数需要用到的变量值实际上存在于其中，我们可以这样访问这些变量： .__closure__ 返回一组存有变量值的cell对象 .__closure__[0].cell_centents 将获取相应变量的值 从上面我们发现，闭包就是将一组变量和一个函数进行封装，如下所示： 我们保存的只有蓝色框框部分，但是蓝色框框需要使用红色框框内的变量，闭包就将这两个框框进行封装。橙色框框返回闭包中的那个主函数。 而且，当我们只是对一个函数进行简单的封装（修饰）时，使用闭包将更加优雅。 闭包的一个重要应用就是python的装饰器（Decorator），且等下回之我见。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup]]></title>
    <url>%2Fspider%2F20180911-ef0f.html</url>
    <content type="text"><![CDATA[你知道吗？其实HTML就是一棵树…… 创建BeautifulSoup对象导入BeautifulSoup类： 1from bs4 import BeautifulSoup 初始化BeautifulSoup对象：传入一个html纯文本，可以open一个本地文件或者requests获取一个在线html文本。 12345678910111213141516171819import requestshtml = requests.get(&lt;URL&gt;, headers = &lt;HEADERS&gt;, cookies = &lt;COOKIES&gt;).textbsobj = BeautifulSoup(html, 'lxml')## 一个例子def get_html(encoding = 'gbk'): headers = [ 'Mozilla/5.0 (Windows NT 10.0; WOW64)', 'AppleWebKit/537.36 (KHTML, like Gecko)', 'Chrome/70.0.3538.67', 'Safari/537.36' ] headers = &#123;'User-Agent': ' '.join(headers)&#125; def open_url(url): html = requests.get(url = url, headers = headers) html.encoding = encoding html = html.text return(html) return(open_url) 格式化打印BeautifulSoup对象内容： 1print(bsobj.prettify()) bs4包的四种基本类型 element.Tag：HTML标签，具有name属性（标签名）和attrs属性 （标签的属性字典） element.NavigableString：标签包含的文本 BeautifulSoup：初始化的对象，name属性值为[document]，它只有一个子标签.html; element.Comment：注释内容 遍历文档树点号遍历 TAG.contents：list对象，直接子标签 TAG.children：iterator对象，直接子标签 TAG.descendants：iterator对象，所有子孙标签 TAG.string：标签内只有一个字符串，获取它 TAG.strings：标签内不止一个字符串，获取它们 TAG.striped_strings：标签内不止一个字符串，获取它们并去掉空白符 TAG.parent：当前标签的直接父标签，很明显只会有一个 TAG.parents：iterator对象，当前标签的所有祖先标签，很明显可能不止一个 下面的四个感觉用途不大，因为不够精准： TAG.previous_sibling或TAG.next_sibling：并列的上一个/下一个兄弟节点 TAG.previous_siblings或TAG.next_siblings： TAG.previous_element或TAG.next_element：前后节点 TAG.previous_elements或TAG.next_elements find/select查找$\blacksquare$ 表示该方法很常用，$\square$ 表示该方式不是很常用 $\blacksquare$.find_all在所有子孙节点中搜索特定标签，返回一个list对象 12345678910111213141516TAG.find_all(name, attrs, recursive, text, **kwargs)# name 限制标签名，形式丰富多样：# |- 1. 特定名称，如 name = 'title'# |- 2. 正则表达式，如 name = re.compile(r'^b')# |- 3. 标签列表，如 name = ['a', 'img', 'p']# |- 4. name = True 会返回所有子孙节点# |- 5. 一个特殊的函数：接受一个TAG，返回布尔值表示这个TAG是否被选择，例如# def has_class_but_no_id(tag):# return tag.has_attr('class') and not tag.has_attr('id')# attr 一个字典，根据属性进行过滤，例如# attr = &#123;'class': '...', 'id': '...'&#125;# recursive 布尔值，True搜索子孙标签，False只搜索直接子标签# text 匹配标签内的字符串，包含了NavigableString和Comment# |- 与name类似，可以传入正则表达式、字符串列表、特殊函数# kwargs 不建议使用# limit 限制返回结果的数量，当文档树过大时有用 $\blacksquare$.find12# 只返回搜索到第一个结果TAG.find(name, attrs, recursive, text, **kwargs) $\square$.find_xxx 父标签/祖先标签查找：.find_parent / .find_parents 上一个/下一个兄弟标签：.find_previous_sibling / .find_next_sibling 之前的/之后的所有兄弟标签：.find_previous_siblings / .find_next_siblings 之前的/之后的一个标签：.find_previous / .find_next 之前的/之后的所有标签：.find_all_previous / .find_all_next $\blacksquare$.selectselect方法接收一个特殊的字符串，该字符串的形式应按照CSS标签选择器的语法书写。 常见的CSS标签选择器语法有： 12345678910# 标签名、样式、id是查找标签的三个常用的属性TAG.select('title') # 查找名为title的所有标签，返回listTAG.select('.cls') # 查找样式中包含cls的所有标签，返回listTAG.select('#myid') # 查找id为myid的所有标签，返回list# 三个常用属性可以进行自由组合TAG.select('head &gt; title') # '&gt;'连接的标签从属关系TAG.select('table .cls #myid') # 并列查找，同时满足TAG.select('table[class="cls",id="myid"]') #联合查找的另一种格式TAG.select('div[class="cls"] &gt; table[id="gold"]') # 联合查找可以和从属标签结合 其实掌握上面的几种CSS标签选择语法就足以应付大多数标签查找了，更多CSS选择器语法参考w3school教程。]]></content>
      <categories>
        <category>spider</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性代数-逆矩阵的存在条件]]></title>
    <url>%2Fml%2F20180815-dbe6.html</url>
    <content type="text"><![CDATA[我们现在要求解一个线性方程组： $$ A{\cdot}x=b $$ 其中$A{\in}R^{m{\times}n}$是一个已知矩阵，$x{\in}R^{n{\times}1}$是一个n维未知列向量，$b{\in}R^{m{\times}1}$是一个m维已知列向量。 按照矩阵×向量的定义，A中的每一行和b中的每一个对应元素形成约束，即b中对应元素都是由A中对应行向量和x运算（內积）而来。 上式的本质还是 $ A_{i,:}{\cdot}x=b_i $ 定义单位矩阵$I_n{\in}R^{n{\times}n}$使得对任意的$x{\in}R^n$都有 $$ I_n{\cdot}x=x $$ 定义逆矩阵$A^{-1}$: $$ A^{-1}{\cdot}A=I_n $$ 这样就可以得到$A{\cdot}x=b$的解为： $$ x=A^{-1}{\cdot}b $$ 那么重点来了，我们怎么求定义的$A^{-1}$？换句话说，$A^{-1}$什么时候存在？（毕竟如果不存在我们期望的这个$A^{-1}$的话怎么求都是徒劳的）因此我们需要对$A^{-1}$的存在条件进行分析。 $A^{-1}$的存在条件分析对于方程组$A{\cdot}x=b$，如果$A^{-1}$存在，那么$x$具有唯一解且这个解为$x=A^{-1}{\cdot}b$。 但是我们的b向量是在$R^m$空间任意取的，对于某些b向量实际实际上可能不存在解，而对另一些b向量则可能存在无穷多的解。那么是否存在一些$b$向量使得x存在不止一个但是又是有限个解呢？答案是不存在的，因为假设向量$x_1$和向量$x_2$是方程组$A{\cdot}x=b$的解，那么向量${\alpha}{\cdot}x_1+(1-{\alpha}){\cdot}x_2$（$\alpha$是任意实数）一定是方程组$A{\cdot}x=b$的解！ 与点的运动关联从空间路径的角度考虑方程组的解的可能个数： $A{\in}R^{m{\times}n}$，即A是一个m行n列的矩阵，A的列向量都是$R^m$空间里的向量，向量b也是$R^m$空间里的一个向量。 我们可以这样想，现在有一个$R^m$空间，它应该有m个坐标轴（直角坐标系），A的每一个列向量在这个坐标系里面实际上都指明了一个方向。 下图是一个例子： $$\begin{pmatrix}A_{1,1} &amp; … &amp; A_{1,n} \\ … &amp; … &amp; … \\ A_{m,1} &amp; … &amp; A_{m,n} \\ \end{pmatrix}{\cdot}\begin{pmatrix}x_1 \\ … \\ x_n \\ \end{pmatrix}=\begin{pmatrix}b_1 \\ … \\ b_m \\ \end{pmatrix}$$ 注意 $$\begin{pmatrix}b_1 \\ … \\ b_m \\ \end{pmatrix}=\begin{pmatrix} A_{1,1}{\cdot}x_1+…+A_{1,n}{\cdot}x_n \\ … \\ A_{m,1}{\cdot}x_1+…+A_{m,n}{\cdot}x_n \\\end{pmatrix}=x_1{\cdot}\begin{pmatrix}A_{1,1} \\ … \\ A_{m,1} \\ \end{pmatrix}+…+x_n{\cdot}\begin{pmatrix}A_{1,n} \\ … \\ A_{m,n} \\ \end{pmatrix}$$ 其中A的某个行向量$A_{i,:}$将控制向量b的某个分量$b_i$，而A的列向量$A_{:,j}$的对应分量将分别影响向量b的分量$b_j$（这里的M影响N意味着M是N的一部分，M决定N表示在其他因素不变的情况下N完全可以由M推导出来！）。有趣的是，A的列向量在影响向量b的分量时都是乘以了某一个常数（x对应的分量），这可以看做是对A的列向量在其本来的方向上进行缩放。 根据上面这种解释，A的所有列向量对b的影响的和（与x的內积）实际上就决定了向量b，即等效完成了A的行向量对向量b分量的控制过程。 上面提到，我们在$R_m$空间里建立了直角坐标系，A的所有列向量和向量b也位于这个空间中。从空间轨迹角度我们可以这样思考： O是$R_m$空间直角坐标系的原点，它首先沿着$A_{:,1}$列向量所指方向运动，运动距离是$x_1{\cdot}|A_{:,1}|$，然后沿着$A_{:,2}$列向量所指方向运动……以此类推，最终到达了向量b表示的B点。 换个角度看，上述过程实际上就是在对A的列向量进行线性组合，使之结果等价于b。 关于A的形式讨论在这里我们发现可以对A进行一些特殊处理，我们让A的列向量仅仅表示方向而不参与点的运动距离计算，距离计算将仅仅依赖于x的分量值，因此我们对A的列向量单位化（使其模为1），当然这不是必须的。 我们对A的一些可能情况进行讨论：假设我们将A的n个列向量沿$R_m$空间的坐标轴进行正交分解 当n&lt;m时（即列向量个数少于列向量的维度数，A的列数少于行数，A是瘦矩阵）：我们只知道一维空间最少只需要一个非零向量就可以通过线性变换覆盖整个空间，二维平面最少需要两个线性无关的向量才能通过线性组合覆盖整个平面，三维空间至少需要三个线性无关的向量才能通过线性组合覆盖整个三维空间。A最多也就只能有n个线性无关的列向量，因此A的列向量的生成子空间根本不可能覆盖整个$R_m$空间，结果就是，对于那些在A列向量生成子空间外的向量b，没有办法通过对A的列向量进行线性组合来等效。结论：A列向量的生成子空间的有效维度少于A列向量的维度时，对于部分向量b，x无解。 当n=m时（即列向量的个数等于列向量的维度数，A的列数等于行数，A是方阵）：从第一点我们可以推断，当且仅当A的列向量线性无关时其列向量的生成子空间才能覆盖A的列向量空间，此时对于任意的$b{\in}R^{m{\times}1}$都能够找到一组x使得原点运动到B点。结论：A列向量的生成子空间的有效维度等于A列向量的维度时，对于任意的$b{\in}R^{m{\times}1}$都有唯一的x解。 当n&gt;m时（即列向量的个数大于列向量的维度数，A的列数等于行数，A是胖矩阵）：注意A的列向量在$R_m$空间中，所以这一组n个列向量中只能找到一组或多组不超过m个列向量的向量组线性无关，剩余列向量一定能被这组里的部分列向量所表示。有了这些列向量的参与，分解方式瞬间变为无数种，如下图(${\alpha}{\in}(0,180^{\circ})$)： 唯一解问题因为我们定义的$A^{-1}$（如果存在）是通过A唯一确定的，基于$A^{-1}$我们求得的$x=A^{-1}{\cdot}b$也应是唯一解。 这就是说，当x有唯一解时我们就可以找到这样一个$A^{-1}$。 从上面的讨论中我们可以知道，当且仅当A的列向量的个数等于列向量的维度且线性无关时，即A是方阵且A的列向量线性无关时，x将有唯一解。 于是乎我们得到结论：只有矩阵A是方阵且列向量线性无关时才存在$A^{-1}$。 奇异矩阵（方阵）：Singular Matrix，列向量相关的方阵 于是乎，只有非奇异矩（方）阵才存在$A^{-1}$。]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法-排序-鸡尾酒排序]]></title>
    <url>%2Fml%2F20180814-6125.html</url>
    <content type="text"><![CDATA[鸡尾酒排序本质上就是双向冒泡排序。 冒泡排序（Bubble Sort）元素分为已经排序的元素和待排序的元素，每轮排序之增加一个已经排序的元素。 每轮排序都要从数组中待排序的那一端进行相邻元素的逐个比较，比较的结果是减少一个待排序元素，因此最多的比较次数将会是 (n-1)+…+1, 时间复杂度为 o(n^2)。 对于大部分有序的数组将会产生大量没有必要的比较，即当一轮比较不产生交换时说明已经全部有序，因此需要设置一个变量标识是否全部有序及时退出循环。 鸡尾酒排序（Cocktail Sort）经典的冒泡排序实际上是单向的，而鸡尾酒排序是双向的。 但是，在鸡尾酒排序中每一轮只能在待排序元素中确定一个最大值或者最小值，在最坏情况下它并不能减少比较次数，它的时间复杂度和空间复杂度与经典的冒泡算法相同，只是在大部分有序时可能少于经典冒泡排序的比较次数。 在大部分元素无序时两种排序半斤八两。]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网络基础-TCP数据包结构及TCP建立连接时的三次握手]]></title>
    <url>%2Fit%2F20180814-5f47.html</url>
    <content type="text"><![CDATA[三次握手，礼尚往来！ 什么是Socket？Socket意为“插座”，可以说这是一个十分形象的比喻了，更专业的称呼叫“套接字”。网络上的两个程序（一般位于两台机器上）需要进行通讯，就必须在它们之间建立一个双向通信通道：一端连接服务器Server，一端连接者客户端Client。但是与一台Server（或者Client）进行通信的网络程序可能不止一个，它们怎么区分开呢？答案是通过端口（port）进行区分：不同的端口事实上会绑定不同的服务，与该主机的通讯实际上是通过某个特定的端口与该主机进行数据交流。端口就像主机身上的插孔，供外界取用信息。ip:port的格式就称之为一个套接字。显然，连接两个网络程序的这个双向通信通道需要有两个套接字：一个连接Client，一个连接Server。 TCP数据包结构 源始端口：16位，发送数据包的端口，取值 0 ~ 65535 目的端口：16位，接受数据包的端口，取值 0 ~ 65535 数据序号（Seq）：32位，数据包发送端指定的序号（可能是随机产生的） 确认序号（Ack）：32位，数据包发送端指定的序号 如果是第一次提出连接请求，例如Client请求连接Server，Seq可以随机产生，由于标志位A为0所以Ack值无效 如上，如果是Server回应Client的连接请求，当前数据包的Seq可以随机产生，Ack值必须是上述Seq值加1 UAPRSF：六个标志位，共6位 U：URG，urgent，为1时表示 紧急指针有效 A：ACK，acknowledge，为1时表示 确认序号（Ack）有效 P：PSH，为1时表示 接受端在接收到数据包后应该优先处理交付应用端 R：RST，reset，为1时表示 TCP连接出现严重问题需要释放当前连接创建新的连接 S：SYN，synchronous，为1时表示 当前数据包是一个与建立连接有关的数据包（请求或接受） F：FIN，finish，为1时表示 当前数据包已是发送端的最后一个数据包，连接需要关闭 三次握手 客户端调用 socket() 函数创建套接字，客户端连接处于 CLOSED 状态。 服务器调用 listen() 函数使服务器端的套接字进入 LISTEN 状态：监听客户端的请求。 客户端调用 connect() 函数构建数据包：请求连接服务器。 这个数据包的 SYN=1，ACK=0：SYN为1表示当前数据包为一个有关建立连接的数据包，ACK为0表示该数据包的作用是客户端请求连接服务器。 这个数据包的 Seq 由客户端产生，用于标识当前数据包的序号。 发送 请求连接 数据包后客户端进入 SYN-SEND 状态：连接请求已发送。 服务器收到 SYN=1, ACK=0 的数据包，判断这是客户端发来的连接请求。 服务器构建表示 接受连接 的数据包： 这个数据包的 SYN=1，ACK=1：当前数据包是一个关于建立连接的数据包，ACK有效意为着这是一个回应性的数据包，即接受客户端的连接请求。 这个数据包的 Seq 由服务器生成。 这个数据包的ACK=1意味着确认序号有效，这个确认序号应该为 收到的客户端的请求性的数据包的数据序号加1。 发送 接受连接 的数据包之后服务器进入 SYN-RECV 状态：连接已接受。 客户端收到 SYN=1，ACK=1 的数据包，判断这是某个服务器发来的接受自己连接的数据包。 客户端检查这个数据包的确认序号是否是自己请求连接数据包的数据序号加1，如果是，则客户端构建 确认 数据包：确认收到服务器接受了连接这一条消息： 这个数据包的 ACK=1：客户端需要最后向服务器发送确认消息因此需要设置ACK=1. 这个数据包的 确认序号 值为服务器发来的 接受连接 数据包的 数据序号。 客户端进入 ESTABLISED 状态：通道客户端已就绪。 服务器收到 ACK=1 的数据包，判断这是客户端发来的最后的确认包。 服务器检查这个数据包的 确认序号，检查是否为自己发送的 接受请求 数据包的 数据序号 加1，如果是则连接建立，服务器进入 ESTABLISED 状态：通道服务器端已就绪。 至此，通道两端都已就绪，下面就可以收发数据了。 从上图我们可以发现，整个连接建立的过程一共需要发送三个数据包，即三次握手。 其中两个由客户端发送，分别表示“请求连接”和“通告服务器自己已知晓服务器接受了请求”；一个由服务器发送，表示“接受连接”。 三个数据包的区分通过SYN和ACK的组合来判定。 服务器只会收到两个数据包：一个ACK为0表示这是始发包，表示客户端的“请求连接”；一个ACK为1表示这是客户端的确认包。 客户端只会收到一个数据包：与建立连接有关的（SYN=1）、属于回应性（ACK=1）的数据包。]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python读取json文件时书写的语法差异]]></title>
    <url>%2Fbug%2F20180809-575c.html</url>
    <content type="text"><![CDATA[使用json文件作为配置文件时如果涉及到列表（list）类型，最后一个元素后面千万不能加逗号。 对于较多参数的程序我们可以通过命令行进行参数传入，也可以定义额外的配置文件。 这里我使用json格式的文件作为额外的配置文件。 json.decoder.JSONDecodeError使用json文件作为配置文件时如果涉及到列表（list）类型，最后一个元素后面千万不能加逗号。 即 [&quot;a&quot;] 是正确的，[&quot;a&quot;, ] 是不正确的。这个规则适用于所有json文件。 加逗号将报错如下： Traceback (most recent call last): File "", line 1, in File "......\lib\json\__init__.py", line 268, in load parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) File "......\lib\json\__init__.py", line 319, in loads return _default_decoder.decode(s) File "......\lib\json\decoder.py", line 339, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File "......\lib\json\decoder.py", line 357, in raw_decode raise JSONDecodeError("Expecting value", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 16 column 9 (char 491) 其中 raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None 应该是指，json解析函数认为逗号后面应该还有元素，因此尝试去解析后面的元素，结果却什么也没得到，抛出 json.decoder.JSONDecodeError 错误，告诉你这里本该有值的（”Expecting value: …”）。模块作者有可能受到了其他语言习惯的影响？]]></content>
      <categories>
        <category>bug</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[warnings内建模块]]></title>
    <url>%2Fpython%2F20180730-5f87.html</url>
    <content type="text"><![CDATA[不同于异常，不想影响到程序的正常执行，同时又想让使用者知道点什么 使用警告：warnings.warn(...) 忽略RuntimeWarning警告：warnings.filterwarnings(action=&#39;ignore&#39;, category=RuntimeWarning)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之飘逸的format格式化字符串控制]]></title>
    <url>%2Fpython%2F20180730-ee75.html</url>
    <content type="text"><![CDATA[除了传统的 % 格式化字符串外，python还提供了一种观感极强的格式化方法：{:}.format(...) 传参123456789101112131415161718192021222324252627282930313233### 1. 按索引取值&gt;&gt;&gt; '&#123;&#125;-&#123;&#125;'.format(111,222) # 默认索引，等价于 '&#123;0&#125;-&#123;1&#125;'.format(111,222)111-222# 索引顺序可以手动指定&gt;&gt;&gt; &#123;0&#125;-&#123;1&#125;-&#123;0&#125;'.format(111,222)111-222-111# 传入序列也可以按索引取值，但是格式很奇葩&gt;&gt;&gt; A = [1, 2, 3, 4]&gt;&gt;&gt; B = ['a', 'b', 'c', 'd']&gt;&gt;&gt; '&#123;0[0]&#125;-&#123;1[1]&#125;-&#123;0[2]&#125;-&#123;1[3]&#125;'.format(A, B)1-b-3-d### 2. 通过关键字取值&gt;&gt;&gt; '&#123;k2&#125;-&#123;k1&#125;'.format(k1=111,k2=222)222-111# 关键字和列表一起使用&gt;&gt;&gt; '&#123;L[0]&#125;-&#123;L[1]&#125;'.format(L=A) # 或者 '&#123;0[0]&#125;-&#123;0[1]&#125;'.format(A)1-2### 3. 传入对象&gt;&gt;&gt; '&#123;obj.attr1&#125;-&#123;obj.attr2&#125;'.format(obj)### 4. 使用*args和**kwargs&gt;&gt;&gt; args = ['friend', 'You']&gt;&gt;&gt; kwargs = &#123;'whose': 'my', 'how': 'are'&#125;# 在下面的例子中format直接在**参数中查找对应的键&#123;&#125;，索引&#123;&#125;按照索引顺序在*参数中查找&gt;&gt;&gt; '&#123;1&#125; &#123;how&#125; &#123;whose&#125; best &#123;0&#125;'.format(*args, **kwargs)You are my best friend 格式控制 {:}（借助于:符号）定宽&amp;填充12345678&gt;&gt;&gt; '&#123;:w&gt;8&#125;'.format(1314)wwww1314''': 格式控制必须字符w 是填充符，必须为单字符，可以不指定，默认为空格，例如 &#123;:&gt;8&#125;&gt; 左对齐，相应地'&lt;'为右对齐, '^'为居中对齐8 域宽''' 浮点数精度1234567&gt;&gt;&gt; '&#123;:.2f&#125;'.format(1.2345) # .2 代表保留两位小数，而不是两位有效数字1.23&gt;&gt;&gt; '&#123;:.&#123;&#125;f&#125;'.format(1.2345, 3) # &#123;&#125;的嵌套使用，按照从左至右，从外到内的顺序1.234&gt;&gt;&gt; '&#123;k:.2f&#125;'.format(k=1.2345) # 配合关键字使用 域宽和浮点数格式控制可以一起使用： 1234567891011&gt;&gt;&gt; '&#123;num:&#123;fill&#125;&#123;align&#125;&#123;width&#125;.&#123;frac&#125;f&#125;'.format( num=3.1415926, fill='-', align='&gt;', width='10', frac='2') # 带关键字的写法------3.14# 与下面的方式等效&gt;&gt;&gt; ':-&gt;10.2f'.format(3.1415926)------3.14 进制转换1234567891011&gt;&gt;&gt; '&#123;:b&#125;'.format(520) # 二进制1000001000&gt;&gt;&gt; '&#123;:d&#125;'.format(520) # 十进制520&gt;&gt;&gt; '&#123;:o&#125;'.format(520) # 八进制1010&gt;&gt;&gt; '&#123;:x&#125;'.format(520) # 十六进制208 数字分隔符12&gt;&gt;&gt; &#123;:,&#125;.format(1234567)1,234,567]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[时间内建模块]]></title>
    <url>%2Fpython%2F20180728-42ac.html</url>
    <content type="text"><![CDATA[包括time, datetime, calendar, business_calendar, pytz, dateutil等等模块。 time模块ticks = time.time() 返回一个表示当前时刻的浮点数，只能表示1970年到2038年。 struct_time = time.localtime(time.time()) 返回本地时间 返回时间元组对象 struct_time，并不能自动转化为字符串 返回例如time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0) tm_wday 表示一周的第几天 tm_yday 表示一年的第几天 tm_isdst 表示是否是夏令时 time.asctime(struct_time) 返回给定时间的字符串表示，只能传入时间元组而不是ticks time.strftime(FORMAT, STRUCT_TIME) 可以定制时间字符串格式 时间格式化符号 %y (两位数年份00-99)，%Y (四位数年份0000-9999) %m (月份01-12)，%d (月中的第几天01-31) %H (24小时制00-23)，%I (12小时制01-12) %M (分钟数00-59)，%S (秒数00-59) %w (周几0-6，0代表周日)，%a (本地简化星期名称，如Sat)，%A (本地完整星期名称，如Saturday) %U (一年中的第几周00-53，周日始)，%W (一年中的第几周00-53，周一始) %a (本地简化月份名称，如Jul)，%A (本地完整月份名称，如July) %j (一年内的第几天001-366) %p (值为AM或者PM) 常用格式： %Y-%m-%d %H:%M:%S time.strptime(STRING[, FORMAT]) 将字符串转换为struct_time对象。如果STRING是标准的时间字符串可以不用指定FORMAT，否则还是指明的好。 time.mktime(struct_time_object) 将struct_time对象转化为一个浮点数（时间戳） time.clock() 返回当前CPU时间，用于计算程序耗时。 datetime模块是time模块的高级封装，同时解除了time模块对年份（1970 - 2038）的限制。 datetime模块包含了5个类，实际常用的是datetime类。 ddnow = datetime.datetime.now() 获取当前时间，是一个datetime.datetime对象 datetime.datetime(2018, 7, 28, 9, 49, 22, 994058)这个可比time模块舒服多了… API Function ddnow.year 获取相关字段信息，类似的还有.month, .day, .hour, .minute, .second, .tzinfo, .microsecond等等 ddnow.replace(...) 修改上述字段的值 ddnow.strftime(FORMAT) 格式化时间字符串，与time.strftime()作用相同 datetime.datetime.strptime(STRING, FORMAT) 相对的datetime模块也能从时间字符串重构datetime对象 ddnow.date()ddnow.time() 和datetime模块其他两个类（date，time）交互 ddnow.timetuple() 返回 time.struct_time 对象 calendar模块与日历相关 business_calendar模块工作日相关 pytz模块时区相关 dateutil模块dateutil.parser 非常强大的时间字符串解析函数，返回datetime.datetime对象 dateutil.rrule 根据定义的规则生成datetime.datetime对象 pandas.to_datetime略]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logging内建模块（入门）]]></title>
    <url>%2Fpython%2F20180724-1605.html</url>
    <content type="text"><![CDATA[Python版本：Python 3.7 logging基础教程日志指的是一些软件运行时我们记录下的事件的发生轨迹。软件的开发者在他们的代码里加入日志调用来查看哪些事件跟随着软件的运行发生了。一个事件通过一条描述性的消息进行记录，这条消息可以选择性地包括各种各样的数据（例如这些数据可能描述了某个事件在不同时刻的差异等）。事件也是开发者归咎错误等级的重要手段，这也叫做严重等级（日志等级）。 什么时候使用日志？日志模块提供了一些简单便利的函数以实现简单的日志功能，他们是debug(), info(), warning(), error(),和critical()。什么时候使用什么等级的日志函数见下表： 你想要执行的任务 该任务最适合的工具 命令行脚本或程序简单地将输出显示到控制台 报告在程序正常运行期间发生的事件，例如状态监控、断层调查 报告在程序正常运行期间发生的事件，例如状态监控、断层调查 logging.info()（或者以诊断为目的获取更加详细的输入的logging.debug() 对一个特定的程序运行时产生的事件发出警告 使用库函数warnings.warn()，警告是可以避免的，应该修改程序代码以消除这样的警告logging.warning()，当我们对这样的情况不能作出任何修改而且警告必须被保存下来时就需要使用这个函数 将特定程序运行时产生的事件报告为错误 抛出异常即可 不抛出异常而报告一个错误（例如长期运行的服务器需要错误处理器） logging.error() logging.exception()logging.critical() 默认的等级是logging.WARNING，这意味着只有当前等级以及大于当前等级的事件才会被跟踪，除非额外配置日志处理器的行为。 被跟踪的事件可以通过不同的方式进行处理，处理跟踪事件最简单的方式是把它们打印在控制台上。而另外一种常见的方式是把它们写入磁盘文件。 一个简单的例子一个非常简单地例子如下： 123import logginglogging.warning('Watch out!') # 将会打印信息到控制台logging.info('I told you so') # 将不会打印任何东西 如果你将这些语句写入脚本并且运行它，你将看到： WARNING:root:Watch out! 当打印到控制台时，`INFO`信息并没有出现，原因是默认的严重等级是`WARNING`。我们看到的打印消息实际上包含了严重等级以及调用日志函数时提供的描述信息，比如"Watch out!"。别担心打印消息中的'root'部分，我们接下来将会对其进行解释。实际的输出结果可以灵活地定制格式以满足各自的需要；格式的定制也将在后面部分进行介绍。 ## 将日志写入到文件 一种非常典型的情景就是在文件中记录日志事件，我们接下来详细说说这种情况。确保接下来的语句在一个新打开的Python解释器中运行，千万不要使用上一部分的环境，否则你可能得到与我们不一样的结果： 12345import logginglogging.basicConfig(filename='example.log', level=logging.DEBUG)logging.debug('This message should go to the log file')logging.info('So should this')logging.warning('And this, too') 现在如果我们打开文件看看里面有什么，我们会发现以下日志信息： DEBUG:root:This message should go to the log file INFO:root:So should this WARNING:root:And this, too 这个例子也展示了你应该如何设置日志的等级，这个等级将作为是否跟踪事件的阀门。在这个例子中因为我们将阀门设置为了DEBUG，所有的信息都将被打印出来。 如果你想在命令行选项参数里设置日志等级，请使用如下格式： --log=INFO 你也可以通过变量 *loglevel* 获取`--log`参数传递的值： 1getattr(logging, loglevel.upper()) 获取你想要传递给`basicConfig()`的 *level* 参数的值，检查以下用户从命令行传入的参数是否合法： 123456# 假设loglevel一定是从命令行参数获取的字符串，后台自动将其转换为大写的形式，# 这样用户就可以在命令行中指定 --log=DEBUG 或者 --log=debugnumeric_level = getattr(logging, loglevel.upper(), None)if not isinstance(numeric_level, int): raise ValueError('Invalid log level: %s' % loglevel)logging.basicConfig(level=numeric_level, ...) 调用`basicConfig()`应该在调用`debug()`, `info()`等方法之前。这种方式被有意设计成一次性的配置，只有第一次配置发挥作用，接下来再进行子参数的配置将起不到任何作用。 如果你多次运行上面的脚本，来自连续运行脚本的日志信息都将被添加到文件 *example.log* 中。如果你想要每个程序运行时都使用全新的日志文件而不是记住之前的日志信息，你可以指定 *fielmode* 参数，我们修改上面对`basicConfig()`的调用： 12345logging.basicConfig( filename='example.log', filename='w', level=logging.DEBUG) 输出将和以前一样，但是日志文件将不再是**末尾添加**模式，先前运行的日志信息已经丢失。 ## 多个模块的日志信息 如果你的程序是由多个模块组成的，这里有个例子展示你该如何在多个模块中组织的你的日志记录： 123456789101112# myapp.pyimport loggingimport mylibdef main(): logging.basicConfig(filename='myapp.log', level=logging.INFO) logging.info('Started') mylib.do_something() logging.info('Finished') if __name__ == '__main__': main() 1234# mylib.pydef do_something(): logging.info('Doing something') 如果你运行 *myapp.py*，你将在 *myapp.log* 中看到如下内容： INFO:root:Started INFO:root:Doing something INFO:root:Finished 见心之所想实在是一件鼓舞人心的事情。你可以使用myapp.py中的方法将这种模式推广到更复杂的多模块程序中去。需要注意的是，仅仅通过这种简单的方式你并不能根据日志文件的内容来判断日志信息来自哪一个模块，除非借助于消息描述本身。如果你想要跟踪你的日志消息的源位置，你需要放下这个基础教程去看看高级日志教程 记录变化的数据为事件的描述信息使用格式化字符串并将变量作为参数传递给日志函数： 12import logginglogging.warning('%s before you %s', 'look', 'leap!') 将会显示： WARNING:root:Look before you leap! 正如你所看到的，将变量中的信息合并到日志描述信息中可以使用老式的%风格的格式化字符串。事实上，格式化字符串是向后兼容的，也就是说，你也可以使用`str.format()`或者`string,Template`。这些新的格式化字符串也被支持，这里不做详细探讨。 ## 改变消息显示的格式 为了改变用于显示的消息的格式，你需要指定你想用的格式： 12345678import logginglogging.basicConfig( format='%(levelname)s:%(message)s', level=logging.DEBUG)logging.debug('This message should appear on the console')logging.info('So should this')logging.warning('And this, too') 将打印： DEBUG:This message should appear on the console INFO:So should this WARNING:And this, too 注意在前面出现的’root’在这里又出现了。对于控制格式的字符串的设置可以参见文档。如果只是简单的使用日志，你只需要指定 levelname（日志等级），message（事件描述，可以包含变量）以及当事件发生时需要在哪里显示消息，这部分将在接下来详细介绍。 在消息中显示时间如果需要在日志消息中显示时间，你需要在格式化字符串中放置 ‘%(asctime)s’： 123import logginglogging.basicConfig(format='%(asctime)s %(message)s')logging.warning('is when this event was logged.') 将打印如下消息： 2010-12-12 11:41:42,612 is when this event was logged.上面这种默认时间显示格式被称之为 ISO8601。如果你想要更加自由的控制时间格式，请提供一个 datefmt 参数给 basicConfig ： 123456import logginglogging.basicConfig( format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')logging.warning('is when this event was logged.') 将打印如下消息： 12/12/2010 11:46:36 AM is when this event was logged.datefmt 的格式控制原理与 time.strftime() 相同。 小结上面的作为基础教程，应该能够使你开始将日志功能添加到自己的代码中。事实上，logging包还提供了许多更为丰富的内容，如果你想更加熟练的应用日志功能，你需要花一点时间阅读高级教程(英)。 如果你对日志功能的需求十分简单，你可以阅读上面的例子，将简单的日志记录融入到自己的代码中去。如果遇到任何问题或者对某些东西不太理解欢迎光临论坛。 如果你看过了高级教程，也可以读读 Logging Cookbook。 注注注：本文手撸自官方文档 Author: Barwe(YinChen)Email: chenyinbarwe@qq.com]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[getpass内建模块]]></title>
    <url>%2Fpython%2F20180723-77b1.html</url>
    <content type="text"><![CDATA[getpass模块用于命令行程序获取密码，输入密码时不会回显在屏幕上 该模块主要包括两个可用的函数： getpass.getpass() getpass.getuser() getpass.getpass(提示字符串)getpass.getuser()该函数实际上会首先检索环境变量LOGNAME（Linux系统可通过echo $LOGNAME查看该值）及其他环境变量的值]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库-mysql-删除(DROP+TRUNCATE+DELETE)]]></title>
    <url>%2Fit%2F20180718-3284.html</url>
    <content type="text"><![CDATA[这里的删除分为： 删除表中的部分数据 删除表的所有数据：下次添加数据前不需要新建表 同时删除表的数据和表的定义：下次添加数据之前必须新建表 主要关键词有：TRUNCATE DROP DELETE 下面按删除类型介绍操作语句： 删除一行或部分行1.DELETE FROM 表 WHERE 字段 = 值：选择性地删除一行 有趣的是，MySQL对DELETE语句添加了标准SQL语句没有的一些功能： 2.DELETE FROM 表 WHERE 字段 = 值 LIMIT 数量：当WHERE过滤结果有多个时删除前面的几个，即删除过滤结果中的部分数据 如果上述语句2对删除部分结果的排序标准不明确可以使用ORDER BY指定排序字段，再使用LIMIT限制删除部分数据： 3.DELETE FROM 表 WHERE 字段 = 值 ORDER BY 排序字段 LIMIT 数量：由小到大 4.DELETE FROM 表 WHERE 字段 = 值 ORDER BY 排序字段 DESC LIMIT 数量：由大到小 删除一列UPDATE 删除所有行但保留表的定义1.DELETE FROM 表：返回被删除的记录数，自增字段起始值恢复为1 2.TRUNCATE TABLE 表：不返回被删除的记录数，自增字段起始值恢复为1 3.DELETE FROM 表 WHERE true：返回被删除的记录数，自增字段起始值不变 注意： 语句3由于加了WHERE将进行逐行扫描（尽管不进行WHERE判断），而语句1直接删除所有数据就好了，所以语句3的执行成本高于语句1 语句2相对于语句1虽然不能返回被删除的记录数，但是非常快 删除指定表（包括内容和定义）DROP TABLE 表 删除指定数据库的所有表数据（保留结构）首先要能遍历到所有表：SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA=’数据库名’ 然后我们使用内置连接语句CONCAT生成多个命令： SELECT CONCAT(‘TRUNCATE TABLE ‘, TABLE_NAME, ‘;’) FROM information_schema.TABLES WHERE TABLE_SCHEMA=’数据库名’：注意TRUNCATE TABLE后面留一个空格 其他DROP、DELETE、TRUNCATE的适用情况]]></content>
      <categories>
        <category>it</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[玻尔兹曼机和受限玻尔兹曼机]]></title>
    <url>%2Fdl%2F20180717-601f.html</url>
    <content type="text"><![CDATA[原文：A Tutorial on Energy-Based Learning 基于能量的模型(EBM, Energy Based Model)：在以计算概率为目标时我们需要将直接计算结果进行归一化和非负处理，完成这两个目标的方式有很多种，我们需要选择计算较为简单的，比如自然指数e。例如，对于离散型随机变量，x经过f变换后再经过自然指数的变换，最后归一化，结果可以被视为一组概率值： 其中 借用原博主的一句话，“哇，是不是很像softmax” 知乎回答 为什么称之为能量？按照原博主的介绍，“能量”的概念应该来自于统计物理学中相似的分布形式： 玻尔兹曼机(Boltzmann Machine)：由观察节点和隐藏节点组成，但是任意两个节点间都有无向边连接，这样的结构在应用中没有实际效果。 受限玻尔兹曼机(RBM, Restricted Boltzmann Machine)：顾名思义，在玻尔兹曼机的基础上去掉观察节点间和隐藏节点间的无向边，观察层和隐藏层由无向边进行连接。RBM定义了神经网络连续两层的形式，可以说是神经网络之砖瓦了。 受限玻尔兹曼机实际上是一个二分图：]]></content>
      <categories>
        <category>dl</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[统计模型估计-噪音对比估计NCE]]></title>
    <url>%2Fstat%2F20180716-beb2.html</url>
    <content type="text"><![CDATA[噪声对比估计(NCE, Noise-Contrastive Estimation) 指数有着十分好的性质，非负、求导快…但同时，指数概率分布的 配分函数 有时会很难计算。 $$p(x) = \frac {e^{G(x)}} Z \tag{1}$$ 配分函数$$Z = \sum_x{ e^{ G(x) } }$$配分函数难以计算主要原因有两个： 由于Z需要对所有样本求加和，当样本数庞大时计算量将超过预期 对于某些 $G(x)$，$Z​$ 实际上是不可计算的 如果我们要求 $p(x)$ 就必须计算配分函数，但是当配分函数实际不可计算时 $p(x)$ 也就不可求了。此时我们必须曲线救国，然后中间是看不太懂的巴拉巴拉…。总之，NCE的思想就是将概率生成问题转化为二分类问题：真实样本和从简单分布随机采样的错误样本进行对比，试图找到真实样本与错误样本的差异。$$p(1|x) = \sigma( G(x;\theta)-\gamma ) = \frac1 { 1+e^{ -G(x;\theta)+\gamma } } \tag{4}$$原文证明了为什么 $\gamma$ 能够替换配分函数 当负样本比正样本多的多时可以考虑NCE]]></content>
      <categories>
        <category>stat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python从requirements.txt安装包]]></title>
    <url>%2Fpython%2F20180423-48b5.html</url>
    <content type="text"><![CDATA[requirements.txt记录的是当前程序的依赖包和版本号用来在另一台电脑上重构运行环境。 生成requirements.txt文件查看包列表： pip freeze 保存只需要借助定向符即可： pip freeze &gt; requirements.txt 从requirements.txt文件进行安装pip install -r requirements.txt pip freeze保存的是当前环境下所有的包实际上我们可能只需要与项目有关的包这里建议使用另一个工具： pipreqs安装它： pip install pipreqs 使用它： pipreqs 项目路径 [选项] 默认保存为项目根路径下的requirements.txt]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10下在右键菜单中添加获取管理员权限入口]]></title>
    <url>%2Fos%2F20180420-f8ae.html</url>
    <content type="text"><![CDATA[右键菜单改造。 添加“管理员权限”效果如下： 新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 ;取得文件修改权限 [HKEY_CLASSES_ROOT\*\shell\runas] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\*\shell\runas\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas] @=&quot;管理员权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,102&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F /t&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /grant administrators:F /t&quot; 双击运行 添加“恢复原始权限” 新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 ;恢复原始权限 [HKEY_CLASSES_ROOT\*\shell\runas-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; ; &amp;&amp; takeown /f \&quot;%1\&quot; [HKEY_CLASSES_ROOT\*\shell\runas-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\exefile\shell\runas2-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas-] @=&quot;恢复原始权限&quot; &quot;Icon&quot;=&quot;C:\\Windows\\System32\\imageres.dll,101&quot; &quot;NoWorkingDirectory&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\Directory\shell\runas-\command] @=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; &quot;IsolatedCommand&quot;=&quot;cmd.exe /c takeown /f \&quot;%1\&quot; /r /d y &amp;&amp; icacls \&quot;%1\&quot; /reset &amp;&amp; cacls \&quot;%1\&quot; /e /r \&quot;%%USERNAME%%\&quot;&quot; 双击运行 取消上面两个选项取消之后就像从未发生过一样~~~新建REG文件，右键编辑内容 Windows Registry Editor Version 5.00 [-HKEY_CLASSES_ROOT\*\shell\runas] [-HKEY_CLASSES_ROOT\exefile\shell\runas2] [-HKEY_CLASSES_ROOT\Directory\shell\runas] [-HKEY_CLASSES_ROOT\*\shell\runas-] [-HKEY_CLASSES_ROOT\exefile\shell\runas2-] [-HKEY_CLASSES_ROOT\Directory\shell\runas-] 双击运行]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下给目录右键菜单添加cmd入口]]></title>
    <url>%2Fos%2F20180420-38fd.html</url>
    <content type="text"><![CDATA[你是否厌倦了在cmd中切换工作路径？现在教你设置在图形界面中通过右键菜单直接以某个目录作为工作路径即，在目录的右键菜单中添加“进入cmd”选项核心方法，还是修改注册表~~~ Ⅰ.打开注册表，路径栏搜索HKEY_CLASSES_ROOT\Folder\shell Ⅱ.右键shell新建项，名称为cmdPrompt实际上这个名字可以随便取，这就意味着我们可以添加多个项新建完成如下： Ⅲ.双击cmdPrompt下右侧的名称设置值，该字符串用于显示在右键菜单上，所以可以取个响亮的名字，比如“在cmd中打开” Ⅳ.右键cmdPrompt新建项，名称为command Ⅴ.双击command下右侧的名称设置值，值为cmd.exe /k cd %1 /k 表示执行后面的命令并保留终端 Ⅵ.保存退出即可 最终效果如下：]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下隐藏资源管理器中六大用户文件夹]]></title>
    <url>%2Fos%2F20180420-6264.html</url>
    <content type="text"><![CDATA[打开资源管理器或者我的电脑你会看到下面这六个用户文件夹如果你没有把文件存在这个目录下的打算，他们会显得特别碍眼这里我们可以隐藏这六个目录隐藏通过修改注册表实现：我们即可已直接修改注册表，也可以通过.reg脚本修改注册表因为涉及到的值较多，我们使用后一种方式修改注册表 隐藏在你喜欢的目录下新建reg文件，文件名随便取，但是必须以.reg结尾右键编辑，千万不能直接双击！！输入以下内容： Windows Registry Editor Version 5.00 [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{d3162b92-9365-467a-956b-92703aca08af}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{B4BFCC3A-DB2C-424C-B029-7FE99A87C641}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3dfdf296-dbec-4fb4-81d1-6a3438bcf4de}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{088e3905-0323-4b02-9826-5d99428e115f}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{24ad3ad4-a569-4530-98e1-ab02f9417aa8}] 保存后双击运行即可，弹出的一切提示均点击是即可 恢复恢复只要报上面语句的负号去掉即可新建另一个.reg文件，输入： Windows Registry Editor Version 5.00 [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{d3162b92-9365-467a-956b-92703aca08af}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{B4BFCC3A-DB2C-424C-B029-7FE99A87C641}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3dfdf296-dbec-4fb4-81d1-6a3438bcf4de}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{088e3905-0323-4b02-9826-5d99428e115f}] [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{24ad3ad4-a569-4530-98e1-ab02f9417aa8}] 保存后双击即可恢复那六个文件夹]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10下使用doskey在cmd中建立类似于linux中alias的宏]]></title>
    <url>%2Fos%2F20180420-1a3f.html</url>
    <content type="text"><![CDATA[在linux系统中可以通过修改.bashrc文件十分简便的设置alias宏命令（macro)在win10下cmd中实现相同的功能要复杂一点 新建宏文件首先你需要一个文件存放宏，假设我们在C盘根目录下建立了文件cmd-alias.bat 修改注册表然后你需要在启动cnd时自动加载文件中的宏，那么问题来了，怎么自动加载宏？修改注册表：1、摁下win+R输入regedit回车2、在菜单栏下的路径栏输入HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Command Processor回车3、右侧新建字符串值，数值名称填AutoRun，数值数据填C:\cmd-alias.bat（第一步那个文件的路径）4、关闭即可。。 修改宏文件右键第一步的文件，选择编辑（默认用记事本打开） ◎ 在windows系统下不是alias命令，而是doskey命令 doskey程序路径为C:\Windows\System32\doskey.exe ◎ 与.bashrc文件一样，一行一个doskey，语句以doskey开头 ◎ 以@doskey开头的宏在cmd打开时不会显示在屏幕上 单个命令的宏@doskey ls=dir：列出当前目录下的子文件/子目录信息 win10默认为dir，linux默认为ls，这里我们设置ls起到与dir相同的作用 @doskey ls=dir $*：$*表示后面可能还有其他参数，参考ls 多个命令的宏多个命令的宏用$t隔开，命令间不用加空格@doskey hexocgd=hexo clean$thexo g$thexo d：顺序执行hexo clean, hexo g, hexo d doskeydoskey /MACROS 可查看所有已经定义的宏命令]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[request.GET/POST属性值与QueryDict类]]></title>
    <url>%2FDjango%2F20180317-8d33.html</url>
    <content type="text"><![CDATA[django.http.QueryDict类在HttpRequest对象中，GET和POST属性的值都是QueryDict的实例 QueryDict用来处理单键对应多值的情况 qd = django.http.QueryDict(...) qd.__getitem__(key) 返回key对应值列表的最后一个值 qd.__setitem__(key, value_list) GET和POST属性值字典不允许被直接修改，因此此方法只能用于该字典的拷贝上 qd.get(key, IF_NONE) 如果key存在返回key对应值列表的最后一个值 qd.update(d) D为QD(查询字典)或者D(python字典)都可以,如果key存在，执行添加而不是替换 qd.items() 返回键值对，值是key对应的值列表的最后一个值（单值） qd.values() 跟items一样使用单值逻辑 //特有方法 qd.copy() 返回可更改的拷贝（比如可以使用__setitem__) qd.getlist(key) 返回key对应的python列表 qd.setlist(key, value_list) 无须拷贝直接修改？？？？？ qd.appendlist(key, value) 给已经存在的key的列表中添加一个值 qd.setlistdefault(ket, value_list) qd.lists() 作用与items类似，不执行单值逻辑，也就是说键值对的值是所有值的列表 qd.urlencode(key) 返回查询字符串格式的字符串，如&apos;a=3&amp;a=4&amp;a=5&apos;]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HttpRequest对象]]></title>
    <url>%2FDjango%2F20180317-f755.html</url>
    <content type="text"><![CDATA[每个view函数函数的第一个参数都是HttpRequest对象，包含当前请求URL的一些信息HttpRequest对象实例： //属性 request.path str类型，请求页面的全路径，不包括域名 request.method str类型，值为&apos;GET&apos;或者&apos;POST&apos; request.GET QueryDict实例 request.POST QueryDict实例，注意区分POST为空和POST的请求内容为空，判断是否为POST方法使用method属性 request.COOKIES 标准python字典对象(用{}表示)，{str:str} request.FILES 类字典对象（？），包含所有上传文件 - 形式为：XxxDict{name: {&apos;filename&apos;:..., &apos;content-type&apos;:..., &apos;content&apos;:...}} - 上面的name变量的值是&lt;input type=&quot;file&quot; name=&quot;...&quot;&gt;中name属性的值 - 只有POST请求并且啥啥啥的FILES属性才会有值，否则为空 request.META 可用的http头部信息字典 request.user django.contrib.auth.models.User对象实例，代表当前登录的用户 - 用户未登录则为django.contrib.auth.models.AnonymousUser对象实例 - 通过 request.user.is_authenticated() 判断用户是否登录 - 需要激活django的AuthenticationMidlleware属性 request.session 当前会话的字典对象，需要激活啥啥啥 request.raw_post_data 未解析的原始post数据 //方法 request.__getitem__(key) 取出GET/POST中的值，优先POST request.has_key(key) 检查GET/POST是否包含key request.get_full_path() 返回包含查询字符串的请求路径字符串 request.is_secure() 如果发出的是HTTPS安全请求返回True]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模板 - 扩展模板系统]]></title>
    <url>%2FDjango%2F20180317-717d.html</url>
    <content type="text"><![CDATA[【注】 下面两类语句不能正常书写，正确写法时需要去掉如下所示空格（空格的HTML字符实体码为 &amp; n b s p ;）： &gt; {&amp;nbsp;{ ... }&amp;nbsp;} &gt; {&amp;nbsp;% ... %&amp;nbsp;} 绝大部分的模板定制是以自定义标签/过滤器的方式来完成的 怎么编写自己的模板库文件？这里的自定义模板库指的就是自定义模板标签/过滤器。 模板库文件显然有他固定的书写格式，我们需要在模块开头写上： from django import template register = template.Library() 模块级变量register是自定义模板标签/过滤器的基础数据结构。 实际上我们在定义自己的模板库文件时可以参考官方的写法，查看django/template/defaultfilters.py文件或者django/template/defaulttags.py文件。 然后我们基于register来自定义模板标签/过滤器: 自定义模板过滤器：定义 + 注册过滤器本质上就是带参数的python函数（哈哈，没想到吧）。第一个参数应该传递管道符(|)入口的值，其他参数通过:进行传递。例如下面的这个过滤器： def cut(value, arg): return value.replace(arg, &apos;&apos;) 在模板中可以用来去掉模板变量值中的字符’A’: { { somevalue|cut:&quot;A&quot; } } 过滤器总是有可正常使用的返回值，不能触发异常（关于触发异常我的理解是：使用raise抛出一个异常，而不是使用try...except...进行异常捕获）。 不知道你发现没有，在上面我们定义模板过滤器的过程中还没有用到register，实际上当我们定义好模板过滤器后需要对他们进行注册才能正常使用，使用一下语句注册： register.filter(&apos;cut&apos;, cut) //(过滤器名称，函数本身) 实际上我们可以还可以使用@修饰器在定义的时候进行注册： //无参时直接@register.filter @register.filter(name=&apos;cut&apos;) def cut(value, arg): return value.replace(arg, &apos;&apos;) 自定义模板标签先略过,有需要再研究。。POTAL 模板库文件放在哪里好？模板库是Django能够导入的基本结构。 建议的目录结构如下： 为自定义的模板库单独建一个app并在INSTALLED_APPS中注册（只有注册的模板库才能被导入）。 在合适的app根目录下为模板们建一个单独的文件夹。 在这个文件夹下建立__init__.py文件和自定义模板库文件。 自定义的模板库怎么导入到模板中？ 这里注意对模板库和模板的概念进行区分 我们在编写模板时可以使用如下语句来导入我们自定义的模板库： { % load 模板库名 % } load模板标签会检查INSTALLED_APPS，只有已安装的app内的模板库才能被加载。这里模板库虽然是存放在某个特定的APP内的，但是load加载模板库时并没有涉及到这个APP。]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模板 - 自动转义]]></title>
    <url>%2FDjango%2F20180317-df12.html</url>
    <content type="text"><![CDATA[【注】 下面两类语句不能正常书写，正确写法时需要去掉如下所示空格（空格的HTML字符实体码为 &amp; n b s p ;）： &gt; {&amp;nbsp;{ ... }&amp;nbsp;} &gt; {&amp;nbsp;% ... %&amp;nbsp;} 自动转义的产生背景当对模板变量进行替换时，替换字符串中可能含有能产生非预期影响的元素。例如&quot;&lt;script&gt;alert(&#39;hellp&#39;)&lt;/script&gt;&quot;将弹出警告框用户利用这个特点做一些不可描述的事情 —— 跨域脚本（XSS）攻击 自动转义的详细操作为了防止这种情况出现，django引入自动转义机制，默认开启。以下5 个字符尤其重要： &lt;自动转为&amp;lt &gt;自动转为&amp;gt &#39;自动转为&amp;#39 &quot;自动转为&amp;quot &amp;自动转为&amp;amp 手动关闭自动转义有时候我们确实是希望模板变量被替换成一段HTML代码 来自用户的数据将进行自动转义 使用safe过滤器关闭单个模板变量的自动转义功能 { { data | safe } } 控制模板块的自动转义 { % autoescape off % } ... { % endautoescape % } 是否自动转义可根据模板间的继承进行转移 default过滤器的参数（字符串）的自动转义因为这个常亮字符串是由模板作者定义的，默认已经通过了safe过滤器。所以模板作者需要人工转义：定义{ { data|default:&quot;3 &amp;lt; 2&quot; } }而不是{ { data|default:&quot;3 &lt; 2&quot; } }。]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模板 - RequestContext和Context处理器]]></title>
    <url>%2FDjango%2F20180316-836c.html</url>
    <content type="text"><![CDATA[视图函数需要传递一个context给模板完成渲染，然后返回给用户完整页面。当向多个不同的模板中传入大量相同的键值对时会多写很多重复的代码，此时可以使用RequestContext 首先定义这些可复用的键值对 def common_items(request): return {...} 这个函数接受request纯粹是因为里面构造返回值字典时可能会用到换言之，它不是必须的：当你确定返回值字典用不着request完全可以不传参 构造视图函数 from django.template import RequestContext def view_1(request, *args, **kwargs): rcontext = RequestContext(request, d{...}, processors=[...]) ... 视图函数的第一个参数必须是request（HttpRequest对象实例）这里我们使用RequestContext对象代替Context对象构造RequestContext需要注意： 第一个参数时request 第二个参数是字典，代表非公用的键值对 processors是包含Context处理器的列表/元组 将RequestContext对象传递给render_to_response方法进行渲染与Context对象不同，使用RequestContext除了传递处理器外还需要传递一个额外的字典（见上第二个参数）使用关键字context_instance: return render_to_response(模板文件, 非公用字典, context_instance=RequestContext(request, processors=[…])) 注意： 这里在构建RequestContext对象的时候并没有传递非公用字典，而是将该字典传递给了render_to_response方法！！！ 但是频繁的键入processors还是会产生大量的代码（这个是真的懒。。）。。所以Django设计了全局context处理器：一般在settings.py的类似于’context_processors’的列表中声明，不同版本可能关键字不一样激活相应的处理器RequestContext将自动包含相对应的一部分变量到context中，具体如下： django.core.context_processors.auth: user：一个django.contrib.auth.models.User/AnonymousUser实例 message：当前登录用户的消息列表 perms：当前登录用户的权限 django.core.context_processors.debug: debug：settings.DEBUG值，检测是否处于debug模式，貌似一直为True？ sql_queries：顺序记录每个SQL查询以及耗费时间 这个处理器还需要满足一些其他的条件… django.core.context_processors.i18n: LANGUAGES LANGUAGE_CODE django.core.context_processors.request django.core.context_processors.messages]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法-动态规划-钢材切割经典问题]]></title>
    <url>%2Fml%2F20180313-6ba0.html</url>
    <content type="text"><![CDATA[钢条切割问题 什么时候用动态规划?动态规划（Dynamic Programming）问题的显著特征： 最优子结构 （optimal substructure） 如果一个问题的解结构包含其子问题的最优解，称此问题具有最优子结构问题。如上的钢条切割问题，长度为n的钢条显然存在最优解，而且最优解一定可以分为两组，假设为i和n-i，那么长度分别为i和n-i的钢条显然也存在最优解，而且可以对这两种钢条独立求解（对长度分别为i和n-i的钢条求解完全是另外的不同的问题）。当然，这里还存在另外一种情况，长度为n的钢条不一定非要切割才能产生最优解，事实上当pn足够大时最优解发生在不切割时，因此需要取不切割和切割成两组中较大的那个值。事实上所谓的切割成两组可能存在不止一种切法：长度为n的钢条存在n-1种切法。因此实际上的递归式可以写为： $$r_n=\max{(p_n,r_1+r_{n-1},r_2+r_{n-2},…,r_{n-1}+r_1)},\text{共n项}$$ 考虑到重复元素，可进一步精简为：$[\frac{n}2]$表示取整 $$r_n=max{(p_n,r_1+r_{n-1},r_2+r_{n-2},…,r_{[frac{n}2]}+r_{n-[\frac{n}2]})},\text{共$1+\frac{n}2$项}$$ 重叠子问题（overlapping subproblems） 当根据递归式自顶向底进行计算时，如果不运用任何技巧（如备忘录），将会浪费大量的资源重复计算相同的子问题，很明显这些子问题是重叠的。递归本来就需要额外的空间对中间状态进行存储，如果还要浪费大量时间重复计算重叠子问题，大大的不划算。解决重叠子问题大概有两种方法： 备忘录：自顶向下计算时将中间结果存下来，下次遇到时直接查询即可。备忘录解决了重叠子问题的计算。 自底向顶计算：这个就厉害了，连递归栈也省略了。利用递推式从最小的同类问题开始计算，你会发现意外的惊喜。在钢条切割问题中最简单的同类问题不外乎$r_0=0$或者$r_1=r_p$。 java中可以使用 Arrays.stream(ARRAY).max().getAsInt() 获取最大值 动态规划的经典模型线性模型状态的排布呈线性：？？ 【例题】在一个夜黑风高的晚上，有n（n &lt;= 50）个小朋友在桥的这边，现在他们需要过桥，但是由于桥很窄，每次只允许不大于两人通过，他们只有一个手电筒，所以每次过桥的两个人需要把手电筒带回来，i号小朋友过桥的时间为T[i]，两个人过桥的总时间为二者中时间长者。问所有小朋友过桥的总时间最短是多少。 区间模型背包模型]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[参数模型和非参数模型]]></title>
    <url>%2Fml%2F20180312-62c1.html</url>
    <content type="text"><![CDATA[任何模型的建立都会做出一定的假设 参数模型(parametric models)要先假设总体服从某个分布（不同的分布有自己的分布参数，比如高斯分布的参数是均值和方差），然后通过样本来估计总体分布的参数 参数模型通过估计出的分布进行检验和预测。 非参数模型(nonparametric models)有时候样本不足以支撑我们取估计总体，或者总体本身没有明显的特征。 非参数模型对总体的分布没有要求，也就是说在计算分析过程中不会用到总体的概率分布表达式。 但是数据必须可以进行排序，因此能影响排序的因素（数据中有相等值）对模型估计结果影响巨大。 非参数模型通过数据本身进行检验和预测。]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas模块-加载数据]]></title>
    <url>%2Fpython%2F20180308-ca22.html</url>
    <content type="text"><![CDATA[文本文件读取pd.read_csv(…)详细记录pandas读取表格型数据文件的方法：pd.read_csv() 其实pytohn内置了一个可以处理简单规整csv文件的模块：import csv给 csv.reader() 传入文件型对象即可什么？你不知道什么是文件型对象？ open(fp,&#39;r&#39;) 返回的就是文件型对象 重要参数解析： path: 这个参数不解释，可以是本地路径，也可以是URL sep: 行内字段(列)分隔符，csv文件默认为逗号，还可以为正则表达式 header: 标识列名的行号。默认为0，代表文件第一行是列名，没有列名时需要设为None //当header=None时: names: 字符串组成的列表，指定列名 index_col: 标识行索引 简单行索引：列编号（整数）或者列名（字符串） 层次化索引：列编号/列名zucheng的列表 什么是层次化索引？将多个列作为索引，列之间将产生层次关系 a a1 a2 b b1 b2 b3 ... skiprows: 如果传入整数，表示需要从文件开始出算起需要忽略的行数 如果传入整数列表，表示需要跳过的行号列表（从0开始） skip_footer: 整型，从文件末尾算起需要忽略的行数 nrows: 整型，需要读取的行数（从开始算起） na_value: 用于替换NaN的值 comment: 当行末有注释信息时指定注释信息标志字符 parse_dates: 尝试将数据解析为日期，默认为False 传入True：尝试将所有列都解析为日期（一般是行不通的） 传入列号/列名的列表：仅尝试将指定的（一些）列解析为日期 传入上一行列表的列表：尝试将多个列进行组合后进行解析（当日期不同部分分散在不同的列中时） keep_date_col: 当组合多列解析时是否保留原始列，默认为False dayfirst: 当日期出现歧义时（如7/6/2018究竟是6月还是7月？)指定day的位置，默认为False date_parser: 或者传入一个用于解析日期的函数 converters: 列值预处理，传入一个字典d={列号/列名: 函数, ...}，表示对相应地列的值将执行相应地函数 iterator: 用于逐块读取文件 chunksize: 文件块的大小 verbose: 是否打印日志 encoding: 文本编码格式 squeeze: 数据经过解析后只剩下一列，返回Series而不是DataFrame thousands: 千分位分隔符 逐块读取当文件特别大时我门可能只是想读取文件的一小部分或者逐块迭代 如果我们只是想读文件开始几行，使用关键字参数 nrows。（Q: 那中间几行呢？） 逐块迭代：read_csgv 指定 chunksize 后将返回一个TextParser用于后续迭代Q: 按固定字符数读取？If so, 貌似只适用于行字符数相同的文件? XML文件读取略 二进制文件读取pickle序列化首先得了解python内置的pickle序列化保存pandas对象到磁盘：PD_OBJ.save(FILE_NAME)从磁盘取回pandas对象：PD_OBJ.load(FILE_NAME) HDF5格式 高效读写磁盘上以二进制形式存储的科学数据HDF: Hierarchical Data Format高效分块读写python中有两个接口：PyTables + h5pypandas建立HDFStore类，通过PyTables存储对象 存储HDFStore类类似于字典，创建HDFStore对象后按照字典方式赋值即可。 //&lt;class &apos;pandas.io.pytables.HDFStore&apos;&gt; store = pd.HDFStore(OUTPUT_FILE) store[&apos;obj1&apos;] = dataframe store[&apos;obj1_col&apos;] = dataframe[&apos;a&apos;] 对于IO密集型数据（而非CPU密集型数据），使用HDF5能够大大提升效率 Excel格式panda中相关的类：ExcelFile依赖包：xlrd, openpyxl 创建实例：xls_file = pd.ExcelFile(XLS_FILE) 读取数据：table = xls_file.parse(&#39;Sheet1&#39;) 使用HTML和Web API使用数据库存取MongoDB中的数据]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas模块-Series类和DataFrame类]]></title>
    <url>%2Fpython%2F20180307-3057.html</url>
    <content type="text"><![CDATA[pandas是基于numpy构建的高级数据结构和操作工具。 pandas的数据结构Series类似于一维数组的对象，索引即标签导入： from pandas import Series初始化： obj = Series([-1, 3, 1, 4]) obj = Series([-1, 3, 1, 4], index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])pandas可以被当做字典使用~ obj = Series({&#39;a&#39;:-1, &#39;b&#39;:3, &#39;c&#39;:1})：不指定index使用传入字典的键 obj = Series({&#39;a&#39;:-1, &#39;b&#39;:3, &#39;c&#39;:1}, index=[&#39;a&#39;,&#39;b&#39;])：使用index时将按照index去字典中挑选，如果该字典中键值对不存在则设为NaN（缺失值）。 使用 obj.isnull() 和 obj.notnull() 检测是否为缺失值 obj.isnull() 等价于 pd.isnull(obj) , notnull() 类似 基本属性： obj.values 查看值 obj.index 查看索引 obj.name obj.index.name DataFrame表格型数据结构数据以二维块存放，行列操作比较平衡，因为存放结构并不是简单的复合一维数据结构（如原生的嵌套列表，行的操作比列简单）导入：from pandas import DataFrame初始化： 传入字典：键为列的名称，值为等长的列表或者ndarray此时索引默认为自然数0到N-1；或者通过 index 传入索引值。索引值对应的列如果没有值将设为NaN ↓↓↓ 未指定列的顺序，将按照列名称自动排序：传入的列名是name age, 实际存储的顺序是age nameframe = DataFrame({&#39;name&#39;:[&#39;aa&#39;,&#39;cc&#39;,&#39;bb&#39;], &#39;age&#39;:[21, 24, 18]}) ↓↓↓ 或者通过 columns 指定列的顺序：按照columnszhiding的顺序进行存储frame = DataFrame({&#39;name&#39;:[&#39;aa&#39;,&#39;cc&#39;,&#39;bb&#39;], &#39;age&#39;:[21, 24, 18]}, columns=[&#39;name&#39;, &#39;age&#39;]) 传入嵌套字典：外键被解释为列名(columns)，内键被解释为索引(index)，自动填充缺失数据内键将会被重新排序形成有序的索引，可以使用index=显示指定索引的顺序，同时还能起到筛选的作用 实际上可以传入由列表、元组、ndarray、Series、字典复合而成的多种数据结构 基本操作： 获取列：frame[&#39;name&#39;] or frame.name，实际上是一个Series索引获取的列是原数据的视图，修改Series将反馈到原数据，复制请使用Series.copy() 修改列：直接给列赋值标量或者向量 赋标量将自动广播 赋列表或者数组一定要保证长度一致 赋Series会对索引进行精确匹配，空位填上NaN 添加列：用frame[&quot;new_column_name&quot;]=...进行赋值将自动添加新列，frame.new_column_name不可以 删除列：del frame[&quot;existed_column_name&quot;]，del frame.existed_column_name不可以 获取列名：frame.columns 表的转置：frame.T，Q:是否是视图？ 获取ndarray数组：frame.values当列的数据类型不一致时返回数组的数据类型将使用能兼容所有列的数据类型的类型 索引对象管理轴标签、轴名称等额外数据字符串索引：&lt;class &#39;pandas.core.indexes.base.Index&#39;&gt;自然数索引：&lt;class &#39;pandas.core.indexes.range.RangeIndex&#39;&gt; index可以整体替换掉，但不能单个修改(immutable) → 安全共享 。。。。。。 操作Seies和DataFrame中的数据重新索引 obj.reindex(…) 如果新索引中包含原索引中没有的值将产生缺失值，可以使用关键字fill_value指定缺失默认值 时间序列的前向插值：时间轴上缺失的索引对应的值与比它大的原来存在的索引对应的值的相同（前向插值） 使用前向差值：obj.reindex(NEW_INDEX, method=&#39;ffill&#39;) 使用后向差值：obj.reindex(NEW_INDEX, method=&#39;bfill&#39;) 实际上还有更精确的插值方式… DataFrame的重新索引 行重新索引: frame.reindex(NEW_INDEX) 列重新索引: frame.reindex(columns=NEW_INDEX) 行列同时重新索引：frame.reindex(NEW_ROW_INDEX, columns=NEW_COLUMN_INDEX) 插值：只能按行插，即按行索引插 丢弃指定轴上的项 obj.drop(…)将返回一个新对象！！ 原来的索引：[‘a’,’b’,’c’,’d’,’e’]丢弃：obj.drop(&#39;c&#39;)，新索引：[‘a’,’b’,’d’,’e’]丢弃：obj.drop([&#39;c&#39;,&#39;d&#39;])，新索引：[‘a’,’b’,’e’] 索引、选取、过滤 直接索引：obj[&#39;c&#39;] 切片包含末端！！！ 结合列的切片和布尔运算来选取行 frame[frame[&#39;three&#39;] &gt; 3]将以”three”列为依据选取”three”列中所有值大于3的行 → 所以实际选取的是特定的某些行 frame[frame &lt; 5 = 0]给表中所有小于5的位置重新赋值为0 frame &lt; 5 本身应该是布尔值（逻辑运算），所以会得到一个由布尔值组成的新DataFrame布尔运算的特性实际上应该继承自numpy 索引关键字ix 算数运算和数据对齐数据对齐：Series或者DataFrame对象间的加法会自动合并相同轴上的索引并自动填充NaN → 取并集 DataFrame对象的算数运算（以加法为例）* `df = df1 + df2` 以df1为主表，将df2的行、列索引加到df1里面去：df1中存在的将执行加法，不存在的为NaN * `df = df1.add(df2, fill_value=0)` 可以这样理解： * 先将df2的行列索引合并到df1中得到并集表df（暂时假设将所有值设为NaN） * 再将主表df1中的存在的值填入df（去掉一部分NaN） * 再将df中剩余的NaN位置改成`fill_value`（去掉所有NaN） * 再将df2中所有元素加到df的相关位置（df2中所有索引必定已经存在于df中） * （括号内的内容纯属方便理解，尚未验证，算不得数） * 其他运算类似： * 减法 sub - * 乘法 mul * * 除法 div / DataFrame和Series间的运算涉及广播机制（broadcasting），熟悉numpy的话应该也熟悉广播机制。。。 * 最简单的广播：Series的索引与DataFrame的行/列索引相同 * 上述索引不完全相同时，先对索引取并集，再进行广播 函数应用和映射* numpy中操纵数组元素的方法（如np.abs)也可以用来操纵pandas对象 * 操纵pandas对象的行/列数组使用 `obj.apply(lambda x: x.min(), axis=1)`（一般用不上） * 使用元素级的python函数操纵pandas元素：例如`obj.applymap(lambda x: &quot;{:.2f}&quot;.format(x))` 排序* Series按index排序：`obj.sort_index()` * Series按values排序：`obj.order()`,缺失值将按index排序后丢到末尾 * DataFrame按指定轴的index排序：`df.sort_index(axis=0)`,默认为0 * DataFrame按某列值排序：例如 `df.sort_index(by=&apos;a&apos;)` * DataFrame按多列值排序：例如 `df.sort_index(by=[&apos;a&apos;,&apos;b&apos;])` * 默认升序，降序设置 `ascending=False` 排名(ranking)根据某种规则破坏评级关系。原著是这么说的，不过很费解不是吗 以 obj = Series([7, -5, 7, 4, 2, 0, 4]) 为例，obj.rank() 返回的实际上是 Series([6.5, 1.0, 6.5, 4.5, 3.0, 2.0, 4.5])。首先我们可以获取一个类似于argsort的序列[6, 1，7, 4, 3, 2, 5]，然后考虑相同的值：两个‘7’的位置分别为7和6，因此都改为均值(7+6)/2=6.5，两个‘4’的位置分别为7和6，因此都改为均值(5+4)/2=4.5，因此算出结果。（至于有什么用，现在我还不知道。。。） 关键词 method : {&#39;average&#39;, &#39;min&#39;, &#39;max&#39;, &#39;first&#39;, &#39;dense&#39;} 指定排名方法： average：默认值，如上取相同值索引的平均值以保证每个相同的元素具有相同的排名 return Series([6.5, 1.0, 6.5, 4.5, 3.0, 2.0, 4.5]) first：与argsort功能相同，按值出现顺序排名，结果应该是连续的正整数 return Series([6.0, 1.0, 7.0, 4.0, 3.0, 2.0, 5.0]) min：相同值的排名全部取最小的那个 return Series([6.5, 1.0, 6.0, 4.0, 3.0, 2.0, 4.0]) max：相同值的排名全部取最大的那个 return Series([7.0, 1.0, 7.0, 5.0, 3.0, 2.0, 5.0]) dense：min和max会跳值，dense不会 return Series([5.0, 1.0, 5.0, 4.0, 3.0, 2.0, 4.0]) 带有重复值的轴索引一般情况下轴索引的值唯一（类似于“键”的概念），但这并不是硬性要求。也就是说，轴索引可以重复@_@。（一般应该也用不上吧？） 判断pandas对象的索引值是否唯一：obj.index.is_unique: bool。（pandas对象指的是Series对象或者DataFrame对象） 对不唯一的索引进行访问将返回所有符合值构成的pandas对象，访问唯一索引返回标量 常用的数学和统计方法约简方法 求和：默认按列求和，自动排除NaN值：df.sum(axis=0, skipna=True)。按行求和指定 axis=1;禁用自动跳过NaN指定 skipna=False count,min,max,argmin,argmax,idxmin,idxmax,sum,mean,media,mad,var,std,… 累计方法 累计求和：df.cumsum() 特殊方法 df.describe()：一次性产生多个统计结果（统计量) 有趣的统计量 corr、corrwith：相关系数 cov：协方差（矩阵） 。。。。 改天再看，顺便练习盲打。。。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回归模型结果度量指标]]></title>
    <url>%2Fml%2F20180306-c444.html</url>
    <content type="text"><![CDATA[回归模型因为预测值连续，很难用‘准确度’这类指标去衡量。回归模型评估方法主要有： MAE (Mean Absolute Error, 平均绝对误差)sklearn API: from sklearn.metrics import mean_absolute_errorformula:$$MAE=\frac1n\sum_{i=1}^n{|f_i-y_i|}$$ 这个很矬，一般不用！ MSE (Mean Squre Error, 均方误差)sklearn API: from sklearn.metrics import mean_squared_errorformula:$$MSE=\frac1n\sum_{i=1}^n{(f_i-y_i)^2}$$ 这个很流行！ RMSE (Rooted Mean Square Error, 均方根误差)formula:$$RMSE=\sqrt{MSE}$$ 与上面的MSE没有区别，一般使用MSE即可！注意：如果是在迭代过程中作为loss函数，需要合理选择MSE/RMSE R2 (R-Squared)sklearn API: from sklearn.metrics import r2_scoreformula:$$y_m=\frac1n\sum_{i=1}^n{y_{t_i}}$$$$R^2=1-\frac{\sum_{i=1}^n{(y_{p_i}-y_{t_i})^2}}{\sum_{i=1}^n{(y_{t_i}-y_m)^2}}$$$y_p$ means prediction while $y_t$ means truth. 将预测值与只使用均值的情况进行比较 当 $y_{p_i}=y_m$时，$R^2=0$ $\rightarrow$ 完全不沾边，还不如使用均值 当 $y_{p_i}=y_{t_i}$ 时，$R^2=1$ $\rightarrow$ 完全匹配（实际上应该是达不到的） 据说 $0{\le}R^2{\le}1$，尚未亲身验证]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[聚类结果评估]]></title>
    <url>%2Fml%2F20180302-90e6.html</url>
    <content type="text"><![CDATA[一个好的聚类结果要求较高的簇内相似度 (intra-) 和较低的簇间相似度 (inter-) 。根据模型比较的对象不同，聚类结果评估指标可以分为外部指标 (external) 和内部指标 (internal) 两类： 外部指标：需要参考模型，比较样本在不同模型中的分类情况； 内部指标：基于自己的聚类结果，不借助模型间的比较。 外部指标外部指标一般基于样本对的分类一致性情况导出。 集合表示 $n$ 个样本的集合共有 $\mathrm{C}_n^2$ 个样本对，按照每个样本对在本模型和参考模型中的分类一致性可以将所有样本对分为四类：$$a=|SS|, \quad SS = \left\lbrace{ \left({\boldsymbol{x}_i,\boldsymbol{x}_j}\right)\;|\; \lambda_i = \lambda_j, \; \lambda_i^\ast = \lambda_j^\ast, \; i &lt; j}\right\rbrace \\b=|SD|, \quad SD = \left\lbrace{ \left({\boldsymbol{x}_i,\boldsymbol{x}_j}\right)\;|\; \lambda_i = \lambda_j, \; \lambda_i^\ast \neq \lambda_j^\ast, \; i &lt; j}\right\rbrace \\c=|DS|, \quad SD = \left\lbrace{ \left({\boldsymbol{x}_i,\boldsymbol{x}_j}\right)\;|\; \lambda_i \neq \lambda_j, \; \lambda_i^\ast = \lambda_j^\ast, \; i &lt; j}\right\rbrace \\b=|DD|, \quad DD = \left\lbrace{ \left({\boldsymbol{x}_i,\boldsymbol{x}_j}\right)\;|\; \lambda_i \neq \lambda_j, \; \lambda_i^\ast \neq \lambda_j^\ast, \; i &lt; j}\right\rbrace$$上面的定义中，$\lambda_i$ 表示样本 $i$ 在本模型中的类别，$\lambda_i^\ast$ 表示样本 $i$ 在参考模型中的类别。 因此直观理解上面的定义： $SS$ 是在本模型中属于同一类、在参考模型中也属于同一类的样本对的集合，$a$ 是集合 $SS$ 的元素个数； $SD$ 是在本模型中属于同一类、在参考模型中不属于同一类的样本对的集合，$b$ 是集合 $SD$ 的元素个数； $DS$ 是在本模型中不属于同一类、在参考模型中属于同一类的样本对的集合，$c$ 是集合 $DS$ 的元素个数； $DD$ 是在本模型中不属于同一类、在参考模型中也不属于同一类的样本对的集合，$d$ 是集合 $DD$ 的元素个数 当参考分布是真实分布时，此时参考模型是理想模型，即$$SS = TP \; \text{(真阳性)； } SD = FP \; \text{(假阳性)； }DS = FN \; \text{(假阴性)； } SS = TN \; \text{(真阴性)； }$$矩阵表示 样本对的分类一致性也可以通过两个矩阵表示$$X_{pair}=\left[\begin{matrix}I(x_1,x_1) &amp; I(x_1,x_2) &amp; \cdots &amp; I(x_1,x_n) \\I(x_2,x_1) &amp; I(x_2,x_2) &amp; \cdots &amp; I(x_2,x_n) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\I(x_n,x_1) &amp; I(x_n,x_2) &amp; \cdots &amp; I(x_n,x_n)\end{matrix}\right]$$ $$Y_{pair}=\left[\begin{matrix}I^\ast(x_1,x_1) &amp; I^\ast(x_1,x_2) &amp; \cdots &amp; I^\ast(x_1,x_n) \\I^\ast(x_2,x_1) &amp; I^\ast(x_2,x_2) &amp; \cdots &amp; I^\ast(x_2,x_n) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\I^\ast(x_n,x_1) &amp; I^\ast(x_n,x_2) &amp; \cdots &amp; I^\ast(x_n,x_n)\end{matrix}\right]$$ $I(x_i,x_j) \in \lbrace 0, \; 1 \rbrace$ ：0表示本模型将 $x_i$ 和 $x_j$ 两个样本分为同一类，1表示分为不同类； $I^\ast(x_i,x_j) \in \lbrace 0, \; 1 \rbrace$ ：0表示参考模型将 $x_i$ 和 $x_j$ 两个样本分为同一类，1表示分为不同类； 所以 $X_{pair}(n \times n)$ 和 $Y_{pair}(n \times n)$ 是二值矩阵。 从矩阵可以导出四个集合（矩阵中1对应的样本对属于该集合）：$$\begin{split}SS &amp;= 1 - X_{pair} \; | \; Y_{pair} \\SD &amp;= \frac{Y_{pair} - X_{pair} + 1}2 \\DS &amp;= \frac{X_{pair} - Y_{pair} + 1}2 \\DD &amp;= X_{pair} \; \&amp; \; Y_{pair}\end{split}$$ Jaccard系数杰卡德系数 (Jaccard Coefficient, JC) 用于比较 有限 集合$X$和$Y$之间的相似性与差异性。$$\mathrm{JC} = \frac{|DD|}{|SD|+|DS|+|DD|} = \frac{|DD|}{1-|SS|} = \frac{X_{pair} \; \&amp; \; Y_{pair}}{X_{pair} \; | \; Y_{pair}}$$适用于 稀疏 &amp; 二值 矩阵。 系数 &amp; 多值矩阵可用Tanimoto系数。 Tanimoto系数Tanimoto系数又叫广义Jaccard系数，用来计算两个稀疏向量间的相似性，向量各分量取值0到1。$$\mathrm{EJ}(A,B) = \frac{A \cdot B}{||A|| + ||B|| - A \cdot B}$$$\mathrm{EJ}$ 系数常见的计算方式还有另一种 假设 $k$ 个特征，即 $A=(a_1,\;a_2,\;\cdots,\;a_k)$，$B=(b_1,\;b_2,\;\cdots,\;b_k)$$$\mathrm{EJ}(A,B) = \frac{\sum_{i=1}^k \min{(a_i,b_i)}}{\sum_{i=1}^k \max{(a_i,b_i)}}$$连续情况定义如下$$\mathrm{EJ}(f,g) = \frac{\int \min(f,g)dx}{\int \max(f,g)dx}$$ FM指数FM指数 (Fowlkes and Mallows lndex，FMI) 多用于衡量两个层次聚类间或者与基准参考聚类模型间的相似度。 与基准聚类模型比较$$\mathrm{FMI}=\sqrt{\frac{|TP|}{|TP|+|FP|} \times \frac{|TP|}{|TP|+|FN|}}$$ 层次聚类模型比较… RI 和 ARI给定 $n$ 个样本的集合 $S$，$X={X_1,X_2,{\cdots},X_r}$ 和 $Y={Y_1,Y_2,{\cdots},Y_s}$ 是关于 $S$ 的两个的划分。因为聚类的过程就是再对数据集进行划分，所以一个划分就等效于一个聚类模型。X和Y同时也意味着两个聚类模型 (clustering)。 RI定义兰德指数 (Rand Index, RI)：$$RI = \frac{|SS|+|DD|}{|SS|+|SD|+|DS|+|DD|} = \frac{||SS|+|DD|}{C_n^2}$$直观上理解，分子是X和Y认可一致的配对，分母是所有可能的配对。当Y是基准划分时，RI表示真阳性数据和真阴性数据所占的比重。 ARI为了实现“在聚类结果随机产生的情况下，指标应该接近零”提出了 调整兰德指数 (Adjusted Rand Index, ARI) 。 构建列联表 (contingency table):其中 $n_{ij}$ 表示X的第i簇与Y的第j簇共有的元素，即 $n_{ij}=|X_i{\cap}Y_j|$。 ARI 定义如下：$$\mathrm{ARI} = \frac { \mathrm{RI} - E[\mathrm{RI}] } { \max {(\mathrm{RI})} - E[\mathrm{RI}] }= \frac{ \overbrace{ \sum_{ij}{C_{n_{ij}}^2} }^{\text{Index}} - \overbrace{ \frac{\sum_i C_{a_i}^2\sum_j C_{b_i}^2}{C_n^2} }^{\text{Expected Index}}}{ \underbrace{ \frac{\sum_i C_{a_i}^2+\sum_j C_{b_i}^2}2 }_{\text{Max Index}} - \underbrace{ \frac{\sum_i C_{a_i}^2\sum_j C_{b_i}^2}{C_n^2} }_{\text{Expected Index}}}$$ARI 的取值范围扩展到了[-1, 1]，起到了衡量 两组数据分布的吻合程度 的作用。 MI 和 NMI互信息 (Mutual Information, MI)$$MI(X,Y)=\sum_{x,y}{ p(x,y)\log{ \frac{p(x,y)}{p(x)p(y)} } }$$ 标准互信息 (Normalized Mutual Information, NMI)$$U(X,Y)=2R=2\frac{I(X;Y)}{H(X)+H(Y)}$$ $$H(X)= \sum_{i=1}^{n}{p(x_i)I(x_i)} = \sum_{i=1}^{n}{p(x_i)\log_b{\frac1{p(x_i)}}} = -\sum_{i=1}^{n}{p(x_i)\log_b{p(x_i)}}$$ 内部指标所谓的内部指标就是基于本模型的聚类结果进行计算。 为了衡量样本之间的相似性和差异性，我们需要先定义一个距离公式 dist(a, b) ，距离的定义需要满足下面四个基本性质： $dist(a, b) {\ge} 0$ 恒成立 当且仅当 a = b 时 $dist(a, b) = 0$ 对称：$dist(a, b) = dist(b, a)$ 三角不等式： $dist(a, b) {\le} dist(a, c) + dist(c, b)$ 当然也可以使用经典距离，例如欧式距离、余弦距离、Hamming距离等等。基于距离，我们定义以下量： $avg(C)$：簇内两点间的平均距离 $diam(C)$：簇内样本间的最大距离 $d_{min}(C_i,C_j)$：两个簇最近样本间的距离 $d_{cen}(C_i,C_j)$：两个簇中心点（簇的重心）间的距离 Davies-Bouldin Index戴维森堡丁指数 (Davies-Bouldin Index, DBI)$$\mathrm{DB} = \frac1k \sum_{i=1}^k \max \limits_{j \neq i}\left ( \frac { \overline{C_i} + \overline{C_j} } { || w_i - w_j ||_2 }\right )$$ 邓恩指数邓恩指数 (Dunn Validity Index, DVI) 对环状等奇葩分布效果很差$$\mathrm{DVI} = \frac{\min \limits_{0&lt;m \neq n&lt;K}\left\lbrace \min \limits_{\forall x_i \in \Omega_m \\ \forall x_j \in \Omega_n } \left\lbrace || x_i - x_j || \right\rbrace\right\rbrace}{\max \limits_{0&lt;m \le K} \max \limits_{\forall x_i,x_j \in \Omega_m}\left\lbrace ||x_i - x_j|| \right\rbrace}$$ 轮廓系数 (Silhouette coefficient)]]></content>
      <categories>
        <category>ml</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-基于TFRecord加载图像数据]]></title>
    <url>%2Fdl%2F20180301-e69e.html</url>
    <content type="text"><![CDATA[什么是TFRecord文件？将一组图像数据和它们对应的标签存在一个二进制文件里，这个文件就是TFRecord文件。 创建自定义的牙骨项目数据加载器 核心步骤： 声明一个用于生成TFRecord文件的writer with tf.python_io,TFRecordWriter(tfr_fp) as writer: …# tfr_fp是生成文件路径 获取图像的二进制数据：例如 PIL::Image 对象 img 可以使用 img.tobytes() 获取二进制信息 然后套用tensorflow提供的API： 红色下划线为关键APIA：标识图像标签的key，加载TFRecord文件的时候会用到B：注意类型对应：图像标签为int类型，图像内容为bytes类型形参 value 的值注意加 [] 使用1.老规矩，先定义队列123q = tf.train.string_input_producer([tfr_fp])这里传入的是 `[tfr_fp]` ，代表建立的实际上是一个文件名队列 2.定义读取TFRecord文件的阅读器1reader = tf.TFRecordReader() 3.用这个reader读取TFRecord文件的文件名队列得到序列化的样例：1_, serialized_example = reader.read(q) 4.从样例中解析出图像数据和图像标签1234567features = tf.parse_single_example( serialized_example, features = &#123; 'label': tf.FixedLenFeature([], tf.int64), 'img_raw': tf.FixedLenFeature([], tf.string) &#125;) 5.从二进制数据中解码图像12img = tf.decode_raw(features[&apos;img_raw&apos;], tf.uint8)img = tf.reshape(img, SHAPE) 6.千万别忘了在session中开启线程，被坑傻了传送门]]></content>
      <categories>
        <category>dl</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[假设检验原理2-卡方检验]]></title>
    <url>%2Fstat%2F20180228-5746.html</url>
    <content type="text"><![CDATA[什么是$\chi^2$分布？若k个独立随机变量 $Z_1,Z_2,…,Z_k$ 服从标准正态分布N(0,1)，则这k个随机变量的平方和$X=\sum{Z_i^2}$ 服从自由度为k的卡方分布。记作 $X\sim\chi^2(k)$。卡方分布的期望为n，方差为2n。 $\chi^2$适合性检验适用范围：比较观测数与理论数是否符合 检验假设：$H_0$: O - E = 0 (观测数与理论数间没有差异) $\chi^2$独立性检验适用范围：常用来判定两个变量之间是独立的还是相互影响的 列联表的独立性检验2x2列联表检验假设：$H_0$: 事件A和时间B无关$H_1$: 事件A和时间B有关 连续性矫正 $r{\times}c$ 列联表这是一个 $2{\times}3$ 列联表：上表中一共有两个变量： 性别：有2个取值（男、女） 肥胖程度：有3个取值（不肥胖、轻度肥胖、中/重度肥胖） $\chi^2$检验验证的是两个变量是否独立，即”性别和肥胖程度无关”]]></content>
      <categories>
        <category>stat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python自用进度条]]></title>
    <url>%2Fpython%2F20180201-ffef.html</url>
    <content type="text"><![CDATA[自定义一个简单的进度条。 123456789101112def progress_bar(x, max, msg='', std_max=100, new_line=False, interval=1, done_mark='█', undone_mark='░'): assert isinstance(new_line, bool) if new_line: interval = 1 x = int(x * std_max / max) done_marks = done_mark * x undone_marks = undone_mark * (std_max-x) r_mark = '' if new_line else '\r' end_mark = '' if not new_line and ( x - std_max ) else '\n' if x % interval == 0 or x - std_max == 0: string = "&#123;&#125;&#123;&#125; &#123;&#125;&#123;&#125; &#123;:&gt;3&#125;%&#123;&#125;" string = string.format(r_mark, msg, done_marks, undone_marks, x*100/std_max, end_mark) print(string, end='')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>青铜派森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-基于队列和多线程加载数据]]></title>
    <url>%2Fdl%2F20180126-f29a.html</url>
    <content type="text"><![CDATA[建文件建议采用 csv格式，第一列是图像路径，第二列是标签（建议标签预处理成数字），分隔符为 , 按行加载文件12with open('fileslist.csv','r') as fin: imgNames = fin.read().strip().split('\n') 建立文件名队列建队 1234567fn_queue = tf.train.string_input_producer(imgNames)#Other opt-parameters# num_epochs: 读取文件一次为一个epoch，默认无限次读取# shuffle: 读取文件时是否乱序# seed: 当shuffle为True时设置随机数种子(可选)# capacity: 队列容量，默认为32(很显然需要大于batch-size)# share_name, name, cancel_op 出队 1item = fn_queue.dequeue() 注： 出队的只是一个符号(参考tensorflow计算图的概念)，真正的值要在Session中获取 此时 item 相当于 fileslist.csv 中的一行（一个记录），每个记录具有相同的形式！ 显然 item 不是标准形式，还需要在后面进行进一步加工 单个记录(item)数据标准化：根据图像 路径字符串 解析出 数组对记录 item 解码得到路径 img_path 和标签 img_label1img_path, img_label = tf.decode_csv(item, [[''],[1]], field_delim=',') 参数解释 records : 必须参数，字符串类型的张量，这里就是 fn_queue 出队的结果 item record_defaults : 必须参数，指定返回值的类型(这里返回值类型的定义很有意思~~)这里直观返回值类型为 str, int , 因此形参赋值 [[&#39;&#39;], [1]] (惊奇)第一个 [] 内可以是任意 str 对象，第二个 [] 内可以是任意 int 对象 field_delim : 建议显示声明，csv文件列间分隔符，默认为 , use_quote_delim : unknown, unimportant name na_value : NA/NaN格式选择，没啥用 解析 img_path 的内容1img_str = tf.read_file(img_path) img_dtr 实际上还是一个字符串，与包含文件名内容的 img_path 不同，img_str 包含的是文件内容。 对图像内容字符串 img_str 进行解析，获取图像的数字信息1img_data = tf.image.decode_jpeg(img_str, channels=3) 参数解释 content : 必须，字符串张量，此处为 img_str channels=0 ： 必须指定，常见图像通道数可能取值有 1, 3, 4 ratio=1 fancy_upscaling=True try_recover_truncated=False acceptable_fraction=1 dct_method=&#39;&#39; name=None 由flatten化的数字信息确定图像矩阵尺寸（说白了就是reshape）1img_array = tf.image.resize_images(img_data, [HEIGHT， WIDTH]) 可选参数 method=ResizeMethod.BILINEAR : 变形方法 align_corners=False 获取批次数据（batch subset）123456images, labels = tf.train.shuffle_batch( [img_array, img_label], batch_size=8, capacity=100, min_after_dequeue=20) 完整参数解释 tensors : 必须，待打包的记录列表，此处为 [img_array, img_label]因为 img_array 和 img_label 在这里都只是个符号，也就不难理解上述赋值了~ batch_size : 必须，与训练过程的 batch_size 相同因此在使用队列训练模型时，batch_size需要作为超参进行预赋值 capacity : 建议，队列容量注意这里的队列与前面的文件名队列不同 min_after_dequeue : 建议，每次出队后队列中剩余的记录此参数可用来衡量shuffle水平，剩余记录数量越大，shuffle程度越高 num_threads=1 ： 可选，入队线程数，通常情况下1个就够用了 seed=None : 可选，设置shuffle随机种子 enqueue_many=False : 可选，一个记录一个记录的入队还是多个一起入队 shapes=None allow_smaller_final_batch=False : 当每个epoch最后一个batch数目不足时是否允许灵活调整batch大小 share_name=None : 多个Session共用数据加载队列时使用，一般用不上 name=None 然而，以上都是在定义队列加载的 图 。。。 Session里面如何使用开启队列？12345678with tf.Session(...) as sess: coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) sess.run(tf.global_variables_initializer()) X, y = sess.run([images, labels]) # 利用feed_dict传入X,y进行训练... coord.request_stop() coord.join(threads=threads)]]></content>
      <categories>
        <category>dl</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[win10系统将C盘的用户文件夹迁移至D盘]]></title>
    <url>%2Fos%2F20180125-f59f.html</url>
    <content type="text"><![CDATA[C盘终极清理！ 激活管理员账户win10默认禁用管理员账户。 打开“计算机管理” 右键“我的电脑”- 管理 cmd - 键入“计算机管理”- 打开 激活管理员账户： 右键“计算机管理（本地）&gt;系统工具&gt;本地用户和组&gt;用户&gt;Administrator”选择属性 去掉“账户已禁用”的对勾 切换至管理员 注销当前用户，切换至Administrator用户 打开cmd，执行 1`robocopy &quot;C:\Users\USERNAME&quot; &quot;D:\Users\USERNAME&quot; /E /COPYALL /XJ` 可能需要安装文件解锁工具 LockHunter：程序接口位于右键菜单中！ 解锁 “C:\Users\USERNAME”：右键菜单选择API进入解锁界面 删除”C:\Users\USERNAME”：cmd中执行 1rmdir &quot;C:\Users\USERNAME&quot; /S /Q 建立软连接：cmd中执行 1mklink /J &quot;C:\Users\USERNAME&quot; &quot;D:\Users\USERNAME&quot;]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow-计算资源控制]]></title>
    <url>%2Fdl%2F20180125-ff54.html</url>
    <content type="text"><![CDATA[控制GPU和CPU的使用。 tf.ConfigProto()的参数使用CPU计算时指定核数1device_count = &#123;'CPU': 4&#125; 使用GPU进行计算:123gpu_options = tf.GPUOptions( per_process_gpu_memory_fraction = 0.25, allow_growth = True) per_process_gpu_memory_fraction 指定每个显存占用比例 allow_growth 允许显存占用自适应增长 并行计算相关设置独立op间并行计算1inter_op_parallelism_threads = 1 op内部（如矩阵乘法）并行计算1intra_op_parallelism_threads = 1 其他参数是否打印设备分配日志1log_device_placement = True 如果你指定的设备不存在，允许TF自动分配设备1allow_soft_placement = True 构建config选择必要参数123config = tf.ConfigProto( device_count = ... 使用CPU核心，可选 gpu_options = ... GPU显存控制，可选 将config传递给Session1session = tf.Session(config=config)]]></content>
      <categories>
        <category>dl</category>
      </categories>
  </entry>
</search>
