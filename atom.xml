<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阔落煮酒</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://chenyin.top/"/>
  <updated>2019-09-15T02:46:48.323Z</updated>
  <id>http://chenyin.top/</id>
  
  <author>
    <name>Barwe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>screen-SSH终端离线持久化工具</title>
    <link href="http://chenyin.top/os/20190911-89a2.html"/>
    <id>http://chenyin.top/os/20190911-89a2.html</id>
    <published>2019-09-11T03:03:26.000Z</published>
    <updated>2019-09-15T02:46:48.323Z</updated>
    
    <content type="html"><![CDATA[<p>有时候我们使用自己的电脑远程连接服务器（例如SSH）进行工作，某些任务我们希望放在<strong>前台</strong>运行但是其运行时间可能很长，如果程序运行期间我们需要断开连接，一般情况下这个前台任务也会随之中断。例如R交互式环境中的程序。screen工具就是为了解决这样一个问题。</p><a id="more"></a><p>screen工具不仅可以保证在断开远程连接的情况下继续运行当前任务，还可以实现单个<strong>实际窗口</strong>中操纵多个<strong>工作窗口</strong>。简单来说，新建一个<strong>screen会话</strong>会创建一个主进程，这个主进程对应着一个会话窗口。这个主进程是存储在服务器上的，它不受我们连接服务器的SSH进程的影响，因此当我们断开SSH连接时这个进程依旧存在，在下次重新连接服务器时依然可以恢复。而我们连接服务器时的工作环境中的任务实际上受SSH连接进程的影响，当我们断开连接时相关联的任务自然而然也就停止了。（PS: 下面提到的<strong>会话</strong>就是指一个虚拟窗口）</p><p>screen的用法如下：</p><ul><li><strong>怎么查询当前服务器中建立了哪些会话？</strong> <code>screen -ls</code> 即可</li><li><strong>怎么建立一个会话？</strong> <code>screen -S &lt;SOCKNAME&gt;</code> 可以建立一个虚拟会话，查看会话信息如下：<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/09/01.png" alt="screen_info_1"><br>由上可知，一个虚拟窗口的ID标识由<strong>进程号PID</strong>和<strong>会话名（SOCKNAME）</strong>组成，我们可以通过这两个信息恢复会话。除了 <code>screen -s ...</code> 之外，<code>screen -R ...</code> 也能建立一个新的虚拟窗口，与<code>-S</code>不同的是，<code>-R</code>是去尝试着恢复一个已有的会话，如果在已有会话中没有找到，他就会建立一个新的会话，跟“若目录不存在则创建目录”是一个意思。</li><li><strong>怎么恢复一个会话？</strong> <code>screen -r ...</code>可以恢复一个会话，当会话不存在时会报错；<code>screen -R ...</code>也能恢复一个会话，但是当会话不存在时会创建一个新的会话。注意：同一时间一个会话只能在一个实际窗口中打开（例如你可能会在不同的电脑上连接服务器或者在一台电脑上打开多个SSH会话）。当会话被挂起时（意味着此时没有窗口打开这个会话），会话信息中每个ID后面会标识出<code>(Detached)</code>，此时意味着你可以在当前窗口中打开这个会话继续工作；如果标识的是<code>(Attached)</code>，那么标识这个虚拟窗口已经在其它的地方被打开了，你将不能打开这个会话。如果你确定这属于异常情况，你可以使用<code>screen -d ...</code>强制挂起一个会话，此时状态会变成<code>(Detached)</code>，这表示你单方面终止了在某个未知地方打开的虚拟窗口。<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/09/02.png" alt="screen_info_2"><br>screen对你命令传入的会话名与会话ID进行匹配，例如你可以：<ul><li>只指定进程号PID，例如<code>screen -r 23288</code></li><li>在不引起歧义的情况下只指定会话名的首字母或前几个字母，例如<code>screen -r n</code>将恢复<code>23288.net</code></li><li>当然你也可以传入完整的<code>&lt;SOCKNAME&gt;</code>或者<code>&lt;PID&gt;.&lt;SOCKNAME&gt;</code></li></ul></li><li><strong>怎么退出并挂起当前会话？</strong> 依次摁下<code>Ctrl A D</code>三个键即可退出并挂起当前会话</li><li><strong>怎么挂起当前窗口中创建的指定会话？</strong>上面已经提到了，<code>screen -d ...</code>可强制挂起指定会话</li><li><strong>怎么删除一个会话？</strong>请确认你的虚拟窗口完成使命后再删除它，不然追悔莫及。<ul><li>直接<code>kill &lt;PID&gt;</code>可删除相应的会话</li><li>摁下<code>Ctrl A K D</code>可删除当前会话中的所有任务并退出当前会话</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候我们使用自己的电脑远程连接服务器（例如SSH）进行工作，某些任务我们希望放在&lt;strong&gt;前台&lt;/strong&gt;运行但是其运行时间可能很长，如果程序运行期间我们需要断开连接，一般情况下这个前台任务也会随之中断。例如R交互式环境中的程序。screen工具就是为了解决这样一个问题。&lt;/p&gt;
    
    </summary>
    
      <category term="os" scheme="http://chenyin.top/categories/os/"/>
    
    
      <category term="Linux实用命令" scheme="http://chenyin.top/tags/Linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>C++中与号和星号的再理解</title>
    <link href="http://chenyin.top/C/20190713-1536.html"/>
    <id>http://chenyin.top/C/20190713-1536.html</id>
    <published>2019-07-13T07:36:39.000Z</published>
    <updated>2019-09-15T02:40:27.026Z</updated>
    
    <content type="html"><![CDATA[<p>大学初识<strong>指针</strong>和<strong>引用</strong>，一知半解。最近想重新学习C++，发现自己对指针和引用的了解透彻了许多，遂记之以温习。</p><a id="more"></a><p><code>&amp;</code>和<code>*</code>实际上各有两种含义：出现在<strong>定义</strong>中时，作为<strong>运算符</strong>时</p><ul><li><code>&amp;</code>用于定义往往只在函数形参中使用，表示<strong>引用</strong>，如<code>void func (int&amp; x, int&amp; y) ...</code></li><li><code>&amp;</code>作为运算符表示<strong>取地址</strong>操作，例如<code>int* p = &amp;x</code></li><li><code>*</code>用于定义中时表示定义一个指针型变量，例如<code>int* p</code>或者<code>int *p</code></li><li><code>*</code>作为运算符时表示<strong>间接取值</strong>，即通过地址间接而不是通过变量名直接获取值，如<code>(*p)++</code></li></ul><h1 id="作为运算符"><a href="#作为运算符" class="headerlink" title="作为运算符"></a>作为运算符</h1><p><code>&amp;</code>和<code>*</code>分别称之为<strong>取地址运算符</strong>和<strong>间接取值运算符</strong>。</p><p><code>&amp;</code>是一个<strong>一元运算符</strong>，是一个<strong>运算符</strong>，它的作用是取出<strong>变量的地址</strong>，取出的是<strong>地址</strong>，地址的类型会带上<code>*</code> ，例如<code>int</code>型的变量<code>x</code>的地址（例如<code>0x61fe44</code>）的<strong>值</strong>的类型就是<code>int*</code>，这个值不能被保存到<code>int</code>类型中（因为<code>int</code>和<code>int*</code>是两种不同的数据类型），那么这个地址值怎么保存呢？这里定义了<strong>指针</strong>类型用于保存地址的值。上面例子中我们可以定义<strong>int指针类型</strong>用于保存int类型的变量的地址（通过<code>&amp;</code>获取），即<code>int *p = &amp;x</code>。其实<code>int *p</code>这种写法容易引起误解，<code>int* p</code>就明显一点，但是前者似乎更通用。上面的定义已经表明了：<code>p</code> 是一个<code>int*</code>类型（即int指针类型）,它保存的是变量<code>x</code>的内存地址，因此可以直接通过<strong>指针</strong>访问变量<code>x</code>存储的值，方法是<code>*p</code>，所以实际上<code>*p</code>和<code>x</code>都能访问到相应地址块上的值，这里<code>*</code>使得我们可以间接通过地址<code>p</code>访问值，所以叫<strong>间接寻址运算符</strong>。这里注意体会<code>int*</code>中的<code>*</code>和<code>*p</code>中的<code>*</code>意义是不完全相同的：前者表明这是变量（<code>p</code>）是一个指针型变量，后者是一种运算符，所谓<strong>运算</strong>，就从一个状态出发得到新的状态，这里就是指从<strong>地址</strong>出发计算得到了<strong>值</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">int</span> *p; <span class="comment">// 写作 int* p可能更好理解点</span></span><br><span class="line">p = &amp;x; <span class="comment">// 这两步可以直接写作 int *p = &amp;x，写成 int* p = &amp;x 可能更加容易理解</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 1000</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p; <span class="comment">// 类似于 0x61fe44 这样的表示地址的值，其类型为 int*</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p; <span class="comment">// 1000，对地址进行*运算得到值</span></span><br></pre></td></tr></table></figure><h1 id="用于参数定义"><a href="#用于参数定义" class="headerlink" title="用于参数定义"></a>用于参数定义</h1><p><code>*</code>用于参数定义时表示定义一个指针型变量，这十分容易理解。</p><p>那么<code>&amp;</code>用于参数定义时表示什么意思呢？<strong>引用</strong>！</p><p>下面以常见的三种函数参数传递方式为例说说<strong>引用</strong>的用法。</p><p><strong>参数传递</strong>有三种常见方式：值传递、指针传递、引用传递</p><p>不得不说时隔这么多年，我总算理解了三种参数传递的方式的基本区别，真笨。</p><p>假设我们现在需要对一个整型变量<code>x</code>进行加1操作，三种参数传递的解决方案如下：</p><ol><li><p><strong>值传递</strong>：值传递即按照普通方式定义形参，此时会对变量进行复制，以保证函数内部不会修改实参的任何信息，这样的参数传递方式当然不能直接修改实参的值，它返回的实际上是一块新的内存区域</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span> <span class="params">(<span class="keyword">int</span> v)</span> </span>&#123;<span class="keyword">return</span> ++v;&#125;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> y = f(x);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 1</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; y; <span class="comment">// 2</span></span><br></pre></td></tr></table></figure></li><li><p><strong>指针传递</strong>：指针传递实际上传递的不是变量名称，而是变量指向的内存区域，这个区域怎么表示呢？当然用一个地址来表示啦。这个地址怎么获取呢？当然是用 <strong>取地址运算符&amp;</strong> 啦。对于指针传递来说，形参定义为一个<strong>指针型</strong>变量，而实参也是一个指针型变量（地址），所以指针传递将会直接修改原内存块的值，这导致函数执行后我们再通过相同的方式取访问变量（实际上是访问变量指向的内存块的值），它的值已经发生了改变</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addone</span> <span class="params">(<span class="keyword">int</span>* p)</span> </span>&#123;(*p)++;&#125;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 1</span></span><br><span class="line">addone(&amp;x);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 2</span></span><br></pre></td></tr></table></figure></li><li><p><strong>引用传递</strong>：指针传递实际上并不优雅，它直接让程序接触到了内存地址。与其如此，我们为什么不直接定义一个类似于<strong>值传递</strong>中的但是又不用对实参进行复制的形参呢？这个形参本质上还是一个<strong>非指针数据类型</strong>，但是它能直接修改实参的值，就相当于给实参定义了一个<strong>别名</strong>，不论通过实参本身修改值还是通过这个别名修改值都是等价的。这种方式称之为<strong>引用</strong>，通过<code>&amp;</code>实现，<code>&amp;</code>作为<strong>引用</strong>含义时常常出现在函数形参中（<code>&amp;</code>还可作为<strong>运算符</strong>使用），它表示形参实际上是实参的一个别名，对形参的任何修改都将等价于对实参的修改，例如形参<code>int&amp; x</code>定义的就是对某个整形实参的<strong>引用</strong>，此时传递的实参也不需要像<strong>指针传递</strong>那样需要先获取地址，程序本身规避了对地址进行直接操作</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addone</span> <span class="params">(<span class="keyword">int</span>&amp; v)</span> </span>&#123;++v;&#125;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 1</span></span><br><span class="line">addone(x);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x; <span class="comment">// 2</span></span><br></pre></td></tr></table></figure></li></ol><p><code>*</code>和<code>&amp;</code>的其它高级用法以后遇到了再温习吧~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大学初识&lt;strong&gt;指针&lt;/strong&gt;和&lt;strong&gt;引用&lt;/strong&gt;，一知半解。最近想重新学习C++，发现自己对指针和引用的了解透彻了许多，遂记之以温习。&lt;/p&gt;
    
    </summary>
    
      <category term="C++" scheme="http://chenyin.top/categories/C/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux三个常见的压缩打包命令</title>
    <link href="http://chenyin.top/os/20190515-feae.html"/>
    <id>http://chenyin.top/os/20190515-feae.html</id>
    <published>2019-05-15T01:32:02.000Z</published>
    <updated>2019-09-15T02:52:16.040Z</updated>
    
    <content type="html"><![CDATA[<p>常见压缩文件扩展名</p><table><thead><tr><th style="text-align:center">扩展名</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">*.gz</td><td style="text-align:center">gzip压缩文件</td></tr><tr><td style="text-align:center">*.bz2</td><td style="text-align:center">bzip2压缩文件</td></tr><tr><td style="text-align:center">*.tar</td><td style="text-align:center">tar打包的文件，没有压缩</td></tr><tr><td style="text-align:center">*.tar.gz</td><td style="text-align:center">tar打包文件。经过gzip压缩</td></tr><tr><td style="text-align:center">*.tar.bz2</td><td style="text-align:center">tar打包文件，经过bzip2压缩</td></tr></tbody></table><a id="more"></a><h1 id="gzip"><a href="#gzip" class="headerlink" title="gzip"></a>gzip</h1><p>压缩并<strong>删除</strong>本地文件：<code>gzip -v SOURCE</code></p><p>压缩但<strong>保留</strong>本地文件：<code>gzip -c SOURCE &gt; TARGET</code></p><p>解压缩：<code>gzip -d SOURCE</code></p><p>不解压缩但查看文件内容：<code>zcat SOURCE</code></p><h1 id="bzip2"><a href="#bzip2" class="headerlink" title="bzip2"></a>bzip2</h1><p>取代gzip，压缩比例高于gzip</p><p>压缩并<strong>删除</strong>本地文件：<code>bzip2 -zv SOURCE</code></p><p>压缩但<strong>保留</strong>本地文件：<code>bzip2 -kv SOURCE</code></p><p>解压缩：<code>bzip2 -d SOURCE</code></p><p>不解压缩但查看文件内容：<code>bzcat SOURCE</code></p><h1 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h1><p>将（多个）文件打包（压缩）成一个文件</p><p>压缩 <code>-c</code>：</p><ul><li>创建bzip2压缩文件：<code>tar -jcvf TARGET.tar.bz2 SOURCE</code></li><li>创建gzip压缩文件：<code>tar -zcvf TARGET.tar.gz SOURCE</code></li></ul><p>不解压缩查看文件列表 <code>-t</code>：</p><ul><li><code>tar -jtvf SOUTCE.tar.bz2</code></li><li><code>tar -ztvf SOURCE.tar.gz</code></li></ul><p>解压缩 <code>-x</code>：</p><ul><li><code>tar -jxvf SOURCE.tar.bz2</code></li><li><code>tar -zxvf SOURCE.tar.gz</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;常见压缩文件扩展名&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;扩展名&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;*.gz&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;gzip压缩文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;*.bz2&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;bzip2压缩文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;*.tar&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;tar打包的文件，没有压缩&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;*.tar.gz&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;tar打包文件。经过gzip压缩&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;*.tar.bz2&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;tar打包文件，经过bzip2压缩&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="os" scheme="http://chenyin.top/categories/os/"/>
    
    
      <category term="Linux实用命令" scheme="http://chenyin.top/tags/Linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>markdown奇淫技巧</title>
    <link href="http://chenyin.top/cargo/20190514-71e.html"/>
    <id>http://chenyin.top/cargo/20190514-71e.html</id>
    <published>2019-05-14T03:13:16.000Z</published>
    <updated>2019-05-14T03:19:02.166Z</updated>
    
    <content type="html"><![CDATA[<h1 id="原生表格中插入包含-的代码"><a href="#原生表格中插入包含-的代码" class="headerlink" title="原生表格中插入包含 | 的代码"></a>原生表格中插入包含 <code>|</code> 的代码</h1><p>由于markdown原生表格使用”|”分隔列，在表格中直接输入”|”会使表格结构解析错误。</p><p>下面这种写法可以使表格正常解析：</p><table><thead><tr><th style="text-align:center">样式</th><th style="text-align:center">代码</th></tr></thead><tbody><tr><td style="text-align:center"><code>A &#124; B</code></td><td style="text-align:center"><code>&lt;code&gt;A &amp;#124; B&lt;/code&gt;</code></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;原生表格中插入包含-的代码&quot;&gt;&lt;a href=&quot;#原生表格中插入包含-的代码&quot; class=&quot;headerlink&quot; title=&quot;原生表格中插入包含 | 的代码&quot;&gt;&lt;/a&gt;原生表格中插入包含 &lt;code&gt;|&lt;/code&gt; 的代码&lt;/h1&gt;&lt;p&gt;由于markdow
      
    
    </summary>
    
      <category term="cargo" scheme="http://chenyin.top/categories/cargo/"/>
    
    
  </entry>
  
  <entry>
    <title>R模型公式</title>
    <link href="http://chenyin.top/R/20190514-95ad.html"/>
    <id>http://chenyin.top/R/20190514-95ad.html</id>
    <published>2019-05-14T03:00:25.000Z</published>
    <updated>2019-05-14T03:10:26.650Z</updated>
    
    <content type="html"><![CDATA[<p>在进行 <strong>方差分析</strong>（ANOVA）或者 <strong>回归分析</strong> 时我们常常会遇到 <strong>~</strong> 操作符，这对R新手来说实在是难以理解，遂查查<a href="http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html" target="_blank" rel="noopener">文档</a>整理整理。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/tildeBig.jpg" alt=""></p><a id="more"></a><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aov(formula, data = <span class="literal">NULL</span>, projections = <span class="literal">FALSE</span>, qr = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, <span class="keyword">...</span>)</span><br><span class="line"></span><br><span class="line">lm(formula, data, subset, weights, na.action, method = <span class="string">"qr"</span>, model = <span class="literal">TRUE</span>, x = <span class="literal">FALSE</span>, y = <span class="literal">FALSE</span>, qr = <span class="literal">TRUE</span>, singular.ok = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, offset, <span class="keyword">...</span>)</span><br><span class="line"></span><br><span class="line">glm(formula, family = gaussian, data, weights, subset, na.action, start = <span class="literal">NULL</span>, etastart, mustart, offset, control = list(<span class="keyword">...</span>), model = <span class="literal">TRUE</span>, method = <span class="string">"glm.fit"</span>, x = <span class="literal">FALSE</span>, y = <span class="literal">TRUE</span>, singular.ok = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, <span class="keyword">...</span>)</span><br></pre></td></tr></table></figure><p>R函数例如 <code>aov()</code>, <code>lm()</code>, <code>glm()</code> 都提供了 <code>formula</code> 参数供用户定义将要进行的分析中涉及到的变量（反应变量、解释变量）。这个 <code>formula</code> 参数直接决定了R构建和测试的模型结构。<code>formula</code> 参数的基本格式为：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response_variable[反应变量] ~ explanatory_variables[解释变量]</span><br></pre></td></tr></table></figure><p>上式中的波浪号（tilde）可以理解为“通过……建模”或者“是……的函数”。</p><p>上式的技巧多在于，如何书写解释变量。</p><p>最基础的回归分析的 <code>formula</code> 参数格式如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y ~ x</span><br></pre></td></tr></table></figure><p>上式中的”x” 称之为 <strong>解释变量</strong>（Explanatory Variable）或者 <strong>自变量</strong>（Independent Variable, IV）,”y”称之为 <strong>反应变量</strong>（Response Variable）或者 <strong>因变量</strong>（Dependent Variable, DV）。</p><p>如果还有其它的解释变量添加到表达式后面即可。下式表示构建两变量的多回归模型：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y ~ x + z</span><br></pre></td></tr></table></figure><p>类似于上面的式子我们称之为 <strong>模型公式</strong>（Model Formula），它将传递给 <code>formula</code> 参数。</p><p>如何书写正确优雅的模型公式是一件很有意思的事情……</p><p>尤其注意，我们在其它地方使用的数学运算符（例如四则运算符）在模型公式里都有新的含义，也就是说我们不能像读一般数学公式那样去阅读模型公式。</p><p>下面这张表列出了模型公式中常用符号的意义，我们可以直观地感受到它们与一般意义地显著差别：</p><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">例子</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center"><code>+</code></td><td style="text-align:center"><code>+ x</code></td><td style="text-align:center">包含了解释变量x</td></tr><tr><td style="text-align:center"><code>-</code></td><td style="text-align:center"><code>- x</code></td><td style="text-align:center">删除解释变量x（没太理解）</td></tr><tr><td style="text-align:center"><code>:</code></td><td style="text-align:center"><code>x : z</code></td><td style="text-align:center">包含了解释变量x和z间的互作</td></tr><tr><td style="text-align:center"><code>*</code></td><td style="text-align:center"><code>x * z</code></td><td style="text-align:center">包含了解释变量x和z，以及它们之间的互作</td></tr><tr><td style="text-align:center"><code>/</code></td><td style="text-align:center"><code>x / z</code></td><td style="text-align:center">嵌套：包含了嵌套在x中的z</td></tr><tr><td style="text-align:center"><code>&#124;</code></td><td style="text-align:center"><code>x &#124; z</code></td><td style="text-align:center">条件：包含了给定z时的x</td></tr><tr><td style="text-align:center"><code>^</code></td><td style="text-align:center"><code>(u + v + w + z) ^ 3</code></td><td style="text-align:center">包含了四个变量，以及它们之间最多三个变量间的互作</td></tr><tr><td style="text-align:center"><code>poly</code></td><td style="text-align:center"><code>poly(x, 3)</code></td><td style="text-align:center">多项式回归：正交多项式</td></tr><tr><td style="text-align:center"><code>Error</code></td><td style="text-align:center"><code>Error(a/b)</code></td><td style="text-align:center">指定误差项</td></tr><tr><td style="text-align:center"><code>I</code></td><td style="text-align:center"><code>I(x*z)</code></td><td style="text-align:center"><code>I()</code> 中的表达式保留一般的数学意义，表示包含了x乘以z这个新变量</td></tr><tr><td style="text-align:center"><code>1</code> (数字)</td><td style="text-align:center"><code>- 1</code></td><td style="text-align:center">截距：删除截距，即通过原点回归</td></tr></tbody></table><p>同一个模型可以通过不同的公式表达：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model 1</span></span><br><span class="line">y ~ u + v + w + u:v + u:w + v:w + u:v:w</span><br><span class="line">y ~ u * v * w</span><br><span class="line">y ~ (u + v + w)^<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model 2</span></span><br><span class="line">y ~ u + v + w + u:v + v:w + v:w</span><br><span class="line">y ~ u * v * w - u:v:w</span><br><span class="line">y ~ (u + v + w)^<span class="number">2</span></span><br></pre></td></tr></table></figure><p>解释变量的属性（例如二值变量、分类变量、数值变量……）决定了模型的特性，例如对公式 <code>y ~ x + z</code>：</p><ul><li>如果x和y是两个分类变量，该公式表示方差分析[?]</li><li>如果x和y是两个数值变量，该公式表示多回归[?]</li><li>如果一个是数值变量一个是分类变量，则表示相关性分析[?]</li></ul><p><strong>关于 <code>Error()</code> 的一点见解</strong></p><p>略</p><!-- url -->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在进行 &lt;strong&gt;方差分析&lt;/strong&gt;（ANOVA）或者 &lt;strong&gt;回归分析&lt;/strong&gt; 时我们常常会遇到 &lt;strong&gt;~&lt;/strong&gt; 操作符，这对R新手来说实在是难以理解，遂查查&lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文档&lt;/a&gt;整理整理。&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/tildeBig.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="R" scheme="http://chenyin.top/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>Basset：CNN学习新的染色体开放位点</title>
    <link href="http://chenyin.top/bioinfo/20190423-1702.html"/>
    <id>http://chenyin.top/bioinfo/20190423-1702.html</id>
    <published>2019-04-23T07:07:47.000Z</published>
    <updated>2019-04-23T07:37:11.922Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/25.jpg" alt=""></p><p>尝试着将神经网络的元件与生物学意义联系起来。大胆假设，小心求证！</p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568/" target="_blank" rel="noopener">PMC</a>   |  <a href="https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract" target="_blank" rel="noopener">Genome Res.</a>  |  <a href="https://github.com/davek44/Basset" target="_blank" rel="noopener">GitHub</a></p><a id="more"></a><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从<a href="https://www.encodeproject.org/" target="_blank" rel="noopener">ENCODE Project Consortium</a>下载125种细胞类型的数据。<br>从<a href="http://www.roadmapepigenomics.org/" target="_blank" rel="noopener">Roadmap Epigenomics Consortium</a>下载39种细胞类型的数据。<br>数据形式为DNase-seq的peak信息，保存在<a href="https://vip.biotrainee.com/d/167-bed" target="_blank" rel="noopener">BED格式</a>的文件中。<br>使用未去重叠（overlap）的peak数据。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><ol><li>以1%的FDR使用模拟方法修改原始数据集——robustness</li><li>归并重叠的peaks共 $2,071,886$ 个峰，比对到hg19参考基因组<ul><li><strong>标准输入</strong>为每个位点600bp的DNA序列长度</li><li><strong>标准标签</strong>为一个164维的二值向量，该向量表示这个峰（位点）在164种细胞类型中的开放情况（1为开放，0为不开放）</li></ul></li><li>将数据集切分为训练集、测试集和验证集<ul><li>training data：训练模型参数</li><li>testing data：计算序列特异性参数</li><li>validation data：用于early stopping</li></ul></li><li>应模型后续分析需要，使用GENOME v18 reference catalog将位点分为promoter（转录起始位点周围2kb区域）、intragenic（与基因区域发生重叠）、intergenic（位于基因间区域内）三类</li></ol><hr><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>训练模型和测试模型的用途有细微差别。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练模型是一个3CNN+2FC的简单神经网络，此项目的亮点不在神经网络的设计，而在于将神经网络的元件与生物学意义结合起来，这也是以往项目中我所想不透的地方。训练网络如下：<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/22.jpg" alt=""></p><p>模型的输入是一个one-hot编码的 $4 \times 600$ 的序列（与处理中已经将所有位点的序列长度截取到了600bp）。</p><p>the first CNN是本模型的重点，它包括了一个卷积层、一个激活层和一个池化层。</p><p>卷积层使用的是300个 $4 \times M$ 的一维卷积核（filter，滤波器），其中 $M$ 长度跟motif长度相当（这里取19b）。作者在这里给每个filter赋予了生命力，认为它们不仅是一种网络元件。因为<u>每个卷积核是基于所有序列优化得来的</u>，所以我们认为卷积核代表了所有序列共有的一种信息，即模式。这是符合思考逻辑的。</p><p>简而言之，每个卷积核可能代表了一种motif，这个motif具体的生物学意义未知。此项目试图从蛋白质结合motif出发去验证卷积核中是否存在相应的结构与之对应。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/23.jpg" alt=""></p><p>这是一种不错的假设-验证思想。</p><p>模型的最后一层也就是第二个全连接层的输出是164个节点，分别代表了该600bp的序列在164种类型细胞种的开放情况。参考标准标签计算binary loss进行参数优化。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试阶段作者做了很有意思的事情。</p><p>为了计算卷积核（motif）对（预测的）开放性打分结果的影响程度，将第一CNN的输出结果（每个卷积核会将 $4 \times 600$ 的序列转化为（600-19+1）维的向量）全部用其平均值替换，这将消除卷积核的特异性影响。用替换结果进行后续计算，将计算结果与未经替换的预测结果进行比较。计算差异的平方和，这个值作为该卷积核的influence值。</p><p>IC（Information Content）值得原文计算方法如下：<br>$$<br>\mathrm{IC} = \sum_{i,j}{m_{ij}\log_2 m_{ij}} - \sum_{i,j}{ b_j \log_{2}b_{j}}<br>$$<br>写成这样似乎更清楚点：<br>$$<br>\mathrm{IC} = \sum_{i=1}^{19} \sum_{j=1}^4 m_i^{(j)} \log_2 m_i^{(j)} - 19\sum_{j=1}^4 b^{(j)} \log_2 b^{(j)}<br>$$</p><p>其中向量 $b$ 是四个碱基分布的背景值，向量 $m\;(4 \times 19)$ 是motif各个位置的碱基概率分布，等效于一个卷积核。</p><p>有趣的是，除了对卷积核进行上述操作计算IC值influence值，作者还直接从数据库中（CIS-BP数据库）下载已知的蛋白质motif代替卷积核进行计算，最后一同进行比较分析。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/24.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/25.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;尝试着将神经网络的元件与生物学意义联系起来。大胆假设，小心求证！&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PMC&lt;/a&gt;   |  &lt;a href=&quot;https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Genome Res.&lt;/a&gt;  |  &lt;a href=&quot;https://github.com/davek44/Basset&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="深度学习" scheme="http://chenyin.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://chenyin.top/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="基因组开放位点" scheme="http://chenyin.top/tags/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%BC%80%E6%94%BE%E4%BD%8D%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>采样5 - M-H采样解决接受率α过小的问题</title>
    <link href="http://chenyin.top/stat/20190422-551c.html"/>
    <id>http://chenyin.top/stat/20190422-551c.html</id>
    <published>2019-04-22T02:06:51.000Z</published>
    <updated>2019-04-22T03:13:44.323Z</updated>
    
    <content type="html"><![CDATA[<p><strong>M-H采样</strong> 是 <strong>Metropolis-Hastings采样</strong> 的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。</p><p>M-H采样解决了<a href="/stat/20190417-ea8c.html">原始MCMC采样</a>中接受率过小的问题。</p><a id="more"></a><p>对于细致平稳条件<br>$$<br>\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji}<br>$$<br>既然 $\alpha_{ij}​$ 太小，我们就把它扩大到1，多么简单粗暴：<br>$$<br>\alpha(i, j)=\min \left \lbrace \frac{\pi(j) Q(j, i)}{\pi(i) Q(i, j)},\; 1 \right \rbrace<br>$$<br>M-H采样过程如下：</p><ol><li>已知平稳分布 $\pi(x)​$，预设状态转移次数阈值 $n​$ 和采样样本数 $m​$，选定任意的某个马氏链对应的状态转移矩阵 $Q(i,\;j)​$</li><li>从任意简单概率采样得 $x_0$</li><li>从 $t=1$ 到 $t=n+m-1$ 循环采样：<ol><li>从 $q(x|x_{t-1})$ 中采样得到状态 $\ast$ 的采样值 $x^\ast$</li><li>从均匀分布中采样得到 $u \sim uniform(0,\;1)$</li><li>如果 $u \lt \alpha(t-1,\;\ast)=\min \left \lbrace \frac{\pi(\ast) Q(\ast, \;t-1)}{\pi(t-1) Q(t-1,\;\ast)},\; 1 \right \rbrace$ ，则接受采样值 $x^\ast$，这意味着我们接受了状态 $t-1$ 到状态 $\ast$ 的转移，即 $x_t=x^\ast$；否则拒绝转移，即 $x_t=x_{t-1}$</li></ol></li><li>$\left \lbrace x_n,\; x_{n+1},\; \cdots,\; x_{n+m-1} \right \rbrace$ 即为平稳分布对应的采样集</li></ol><p>一般强化先验知识有助于简化计算。如果我们预设的 $Q​$ 是对称矩阵，即 $Q(i,\;j)=Q(j,\;i)​$，那么此时 $\alpha​$ 可以简化为<br>$$<br>\alpha(i,\;j) = \min \left \lbrace \frac{\pi_j}{\pi_i},\; 1 \right \rbrace<br>$$<br>尽管M-H采样解决了[原始MCMMC采样][]过小的问题，它还是有很多缺陷，特别是在特征维度很大时：</p><ol><li>接受概率 $\alpha$ 难以计算，或进行了许多的无用计算（拒绝采样）；</li><li>联合分布难以计算，相比之下条件概率分布计算更容易</li></ol><p><a href="/stat/20190422-dcdd.html">Gibbs采样</a>解决了上面的问题。</p><!-- URL -->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;M-H采样&lt;/strong&gt; 是 &lt;strong&gt;Metropolis-Hastings采样&lt;/strong&gt; 的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。&lt;/p&gt;
&lt;p&gt;M-H采样解决了&lt;a href=&quot;/stat/20190417-ea8c.html&quot;&gt;原始MCMC采样&lt;/a&gt;中接受率过小的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
  </entry>
  
  <entry>
    <title>采样4 - 细致平稳条件和MCMC采样</title>
    <link href="http://chenyin.top/stat/20190417-ea8c.html"/>
    <id>http://chenyin.top/stat/20190417-ea8c.html</id>
    <published>2019-04-17T03:07:54.000Z</published>
    <updated>2019-07-09T01:07:01.007Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/07/01.jpg" alt=""><br><a id="more"></a></p><h1 id="细致平稳条件"><a href="#细致平稳条件" class="headerlink" title="细致平稳条件"></a>细致平稳条件</h1><p><strong>&gt; 细致平稳条件（Detailed Banlance Condition）</strong></p><p>当一个系统微观上达到平衡时满足 <strong>细致平稳条件</strong>：$\pi_i​$ 表示系统处于状态 $i​$ 的概率，$P_{i{\to}j}​$ 表示系统从状态 $i​$ 转移到状态 $j​$ 的概率，一个反应（转移）应该与它的逆反应（逆转移）达到平衡，即<br>$$<br>\pi_i P_{i \to j} = \pi_j P_{j \to i} \tag{1}<br>$$<br><a href="https://en.wikipedia.org/wiki/Detailed_balance" target="_blank" rel="noopener">细致平稳条件</a> 是 Ludwig Boltzmann 于1872年提出来的。</p><p><strong>&gt; 马尔可夫链具有平稳分布的充分条件</strong></p><p>假设马尔可夫链的状态分布为 $\pi$，状态转移矩阵为 $P$，如果存在某一个时间节点使得对于任意的状态 $i$ 和 $j$ 有<br>$$<br>\pi_i P_{ij} = \pi_j P_{ji} \tag{2}<br>$$<br>上述细致平稳条件成立，则该马尔可夫链具有<strong>平稳分布</strong> $\pi$（Stationary Distribution）。</p><p>细致平稳条件是马氏链收敛的充分条件，而不是必要条件。</p><blockquote><p>充分性证明：</p><p>因为 $\sum_i \pi_i P_{ij} = \sum_i \pi_j P_{ji} = \pi_j \sum_i P_{ji} = \pi_j$</p><p>所以 $ \sum_i \pi_i P_{ij} = \pi P = \pi $</p></blockquote><p>仅在二状态系统下，细致平稳条件也是马氏链收敛的必要条件，即充要条件。</p><p><strong>&gt; 从目标平稳分布 $\pi(x)$ 出发寻找转移矩阵 $P$</strong></p><p>假设目标平稳分布 $\pi(x)$ 已知，根据细致平稳条件，我们只需要确定一个矩阵 $P$ 作为相应的状态转移概率矩阵，就能确定这样一个马尔可夫链：转移矩阵为 $P$，平稳分布为 $\pi(x)$。</p><p>但是，矩阵 $P$ 的确定并不是一件很容易的事情，MCMC采样算法很巧妙的解决了这个问题。</p><h1 id="MCMC采样"><a href="#MCMC采样" class="headerlink" title="MCMC采样"></a>MCMC采样</h1><p><strong>&gt; 马尔可夫蒙特卡洛采样（Markov Chain Monte Carlo，MCMC）的基本思想</strong></p><p>我们随机确定一个马尔可夫链的状态转移矩阵 $Q$，它并不满足细致平衡条件，即<br>$$<br>\pi_i Q_{ij} \ne \pi_j Q_{ji} \tag{3}<br>$$<br>为了使等式成立，引入非零 $\alpha_{ij}$ 项，即<br>$$<br>\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji} \tag{4}<br>$$<br>取<br>$$<br>\begin{cases}<br>\alpha_{ij} &amp;= \pi_j Q_{ji} \<br>\alpha_{ji} &amp;= \pi_i Q_{ij}<br>\end{cases} \tag{5}<br>$$<br>此时目标平稳分布 $\pi(x)$ 对应的马氏链的状态转移概率矩阵 $P$ 可表示为<br>$$<br>P_{ij} = Q_{ij} \alpha_{ij} \tag{6}<br>$$<br>由(5)式可知，$0 \le \alpha_{ij}^{(k)} \le 1$，对于特定的 $Q_{ij}$，$\alpha_{ij}$ 越小，则相应的 $P_{ij}$ 越小：这可以解释为 $\alpha_{ij}$ 是对 $Q_{ij}$ 的接受率，目标状态转移矩阵 $P$ 可以通过任意一个马尔可夫链的状态转移矩阵 $Q$ 以一定概率 $\alpha$ 接受获得。</p><p><strong>&gt; MCMC采样算法过程</strong></p><blockquote><ol><li>初始化状态分布 $\pi_0$ 并采样 $x_0$</li><li>循环采样直到获得 $m$ 个样本：<ol><li>$t$ 时刻马氏链的状态分布为 $\pi_t$（未知，我们也不关系纯向量的值），从 $Q(x|x_t)$ 采样得样本 $x^*$</li><li>从均匀分布中采样 $u \sim uniform(0,\;1)​$</li><li>如果 $u &lt; \alpha_{x_t \to x^<em>} = \pi_(x^</em>)Q(x^<em>,\;x_t)$ 则接受转移 $x_t \to x^</em>$，即 $x_{t+1}=x^*$；否则不接受转移，即 $x_{t+1}=x_t$</li><li>舍弃第 $0$ 到第 $n-1$ 共 $n$ 个样本，保留第 $n$ 到第 $n+m-1$ 共 $m$ 个样本作为采样样本 </li></ol></li></ol></blockquote><p>上面的 $\alpha_{ij}$ 实际上很小，这导致大部分采样都会被拒绝，这意味着我们的 $n$ 要预设的非常大。</p><p>基于这个困境，<strong>M-H采样</strong> 对 <strong>MCMC采样</strong> 进行了优化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/07/01.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样3 - 离散马尔可夫链采样</title>
    <link href="http://chenyin.top/stat/20190416-1f84.html"/>
    <id>http://chenyin.top/stat/20190416-1f84.html</id>
    <published>2019-04-16T08:03:14.000Z</published>
    <updated>2019-04-17T05:45:25.680Z</updated>
    
    <content type="html"><![CDATA[<p>如果我们的目标分布是<strong>简单</strong>、<strong>离散</strong>的分布，这个目标分布可以当作某个马尔可夫链的<strong>平稳分布</strong>，这个平稳分布对应着一个<strong>状态转移矩阵</strong>。如果这个状态转移矩阵已知，我们就可以十分容易的得到目标分布的采样集。</p><p>离散马尔可夫链采样只是理论基础，实用价值不大。</p><a id="more"></a><h1 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h1><p>下面直接以例子说明采样过程：</p><p>假设一个质量分布不均匀的六面骰子掷出六个点数的概率服从一个简单、离散的概率分布，记为 $\pi(x)$，$x=\lbrace 1\;2\;3\;4\;5\;6 \rbrace$。</p><p>$\pi(x)$ 可以通过大量独立重复实验用频率进行估计，如果知道了状态转移矩阵 $P$ 就可以很容易的进行采样。</p><p>先假设转移概率矩阵 $P$ 已知，这里模拟一个状态转移矩阵作为示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">36</span>).reshape([<span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">p = p/p.sum(axis=<span class="number">1</span>).reshape([<span class="number">-1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>$$<br>p = \left[\begin{matrix}<br>0.18206229 &amp; 0.1764297  &amp; 0.23015706 &amp; 0.13247823 &amp; 0.23143627 &amp;0.04743645 \\<br>0.03751516 &amp; 0.33564335 &amp; 0.39400589 &amp; 0.12847041 &amp; 0.07526389 &amp;0.0291013  \\<br>0.12921795 &amp; 0.25957537 &amp; 0.01825006 &amp; 0.32088546 &amp; 0.26899888 &amp;0.00307228 \\<br>0.22883623 &amp; 0.07873173 &amp; 0.23422034 &amp; 0.23744235 &amp; 0.08045652 &amp;0.14031283 \\<br>0.14770889 &amp; 0.20742323 &amp; 0.19634175 &amp; 0.07048638 &amp; 0.16482596 &amp;0.21321379 \\<br>0.07080479 &amp; 0.24460837 &amp; 0.22450135 &amp; 0.21595409 &amp; 0.11542076 &amp;0.12871064<br>\end{matrix}\right]<br>$$<br><strong>&gt; 1. 指定初始分布</strong></p><p>因为马尔可夫链的平稳分布只与状态转移矩阵相关，而与初始状态分布无关。</p><p>所以我们可以从一个从任意初始化状态分布出发进行采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pi = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">6</span>)</span><br><span class="line">pi = pi / pi.sum()</span><br></pre></td></tr></table></figure><p>$$<br>\pi_0 = [0.22081522,0.11290264,0.23593329,0.0524546,0.1917839,0.18610888]<br>$$</p><p><strong>&gt; 2. 指定记录采样值时的状态转移次数 $n$ 和需要记录的采样数 $m$</strong></p><p>整个采样过程中需要进行的状态转移次数为<br>$$<br>N= n+m-1<br>$$<br>将要记录下的采样值集合为<br>$$<br>X = \lbrace \underbrace{x_n,\;x_{n+1},\;\cdots,\;x_{n+m-1}}_{\text{共 $$m 项}} \rbrace<br>$$<br>采样集合 $X$ 即为平稳分布 $\pi(x)$ 对应的样本集。</p><p>实际应用中，$n$ 凭经验确定。</p><p><strong>&gt; 3. 从初始分布采样</strong></p><p>从初始分布采样的意思是：从简单分布（例如均匀分布）采样，以初始分布的确定采样值（常见的方法是以初始分布的累积分布函数确定采样值）。</p><p>计算累积分布函数（python中调用 <code>np.cumsum(arr)</code>）：<br>$$<br>F_0 = [0.22081522,0.33371785,0.56965314,0.62210774,0.81389112,1]<br>$$<br>从[0,1]均匀分布采样一次（<code>np.random.uniform(0, 1)</code>）得 $p_0 = 0.27364499172190815​$</p><p>因为 $F_0^{(1)}&lt;p_0&lt;F_0^{(2)}​$，我们认为本次采样值 $x_1=2​$，即本次骰子之出来的点数是2。</p><p><strong>&gt; 4. 基于 $P(x|x_1)​$ 进行第二次采样</strong></p><p>$P(x|x_1=2)$ 即从状态转移矩阵 $P$ 的第二行采样<br>$$<br>P(x|x_1) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]<br>$$<br>从[0,1]均匀分布采样一次得 $p_1 = 0.23181961317728317$，$x_2=2$。</p><p><strong>&gt; 5. 基于 $P(x|x_2)​$ 进行第三次采样</strong></p><p>$P(x|x_2=2)$ 即从状态转移矩阵 $P$ 的第二行采样<br>$$<br>P(x|x_2) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]<br>$$<br>从[0,1]均匀分布采样一次得 $p_2= 0.5880393198555685​$，$x_3=3​$。</p><p><strong>&gt; 基于 $P(x|x_{n-1})$ 进行第 $n$ 次采样</strong> </p><p>$P(x|x_{n-1})$ 即从状态转移矩阵 $P$ 的第 $x_{n-1}$ 行采样，得到第 $n$ 个采样值 $x_n$，这个采样值将作为我们第一个目标样本。也就是说前面的 $n-1$ 个采样会被舍弃掉。</p><p>继续采样，直到获取第 $n+m-1$ 个采样值。</p><h1 id="挠头"><a href="#挠头" class="headerlink" title="挠头"></a>挠头</h1><p><strong>Q1 由上知是从矩阵 $P$ 而不是每个状态分布 $\pi$ 中采样，因为不论状态转移多少次，矩阵 $P$ 总是不变的，为什么不直接从矩阵 $P$ 中采样，而是从初始分布采样舍弃 $n-1$ 个样本？</strong></p><p>这个问题看似复杂，其实很简单：因为总是选取矩阵 $P$ 的某一行采样，至于要选取哪一行取决于上一次的采样结果。那么，<strong>第一次使用矩阵 $P$ 进行采样</strong>（区别于<strong>第一次采样</strong>）应该选取哪一行呢？这是不确定的。因此第一次采样我们从指定的初始分布采样，第二次采样（即第一次使用矩阵 $P$ 进行采样）基于第一次采样结果进行。</p><p><strong>Q2 为什么要舍弃前面的 $n-1$ 个采样值？</strong></p><p>因为初始分布是我们自己定义的，而采样值服从的分布应该是平稳分布而不是初始分布或者达到平稳分布前的任何一个分布。</p><p>$n$ 一般是人为设定的阈值，代表第 $n$ 次转移时已经达到平稳分布，显然这不是一个十分精确的阈值，但是在实际应用过程中影响不大。</p><p>状态的分布伴随着采样过程而改变，这是一个隐藏的过程。当状态分布收敛时的采样值集为平稳分布的采样值。（这句话很玄学，暂时没太理解！）</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上面的马尔可夫链采样的基础是状态概率矩阵 $P$ 已知，但是实际上 $P$ 是很难求出来的，即我们知道的只有平稳分布 $\pi(x)$。这似乎是一个死循环~</p><p>MCMC采样巧妙地解决了这个问题。</p><p>M-H采样和Gibbs采样是MCMC采样地改进版。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果我们的目标分布是&lt;strong&gt;简单&lt;/strong&gt;、&lt;strong&gt;离散&lt;/strong&gt;的分布，这个目标分布可以当作某个马尔可夫链的&lt;strong&gt;平稳分布&lt;/strong&gt;，这个平稳分布对应着一个&lt;strong&gt;状态转移矩阵&lt;/strong&gt;。如果这个状态转移矩阵已知，我们就可以十分容易的得到目标分布的采样集。&lt;/p&gt;
&lt;p&gt;离散马尔可夫链采样只是理论基础，实用价值不大。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样2 - 离散马尔可夫链的几个性质</title>
    <link href="http://chenyin.top/stat/20190416-ea6c.html"/>
    <id>http://chenyin.top/stat/20190416-ea6c.html</id>
    <published>2019-04-16T03:05:25.000Z</published>
    <updated>2019-07-09T01:12:19.268Z</updated>
    
    <content type="html"><![CDATA[<p>互通、可约、周期、常返、遍历<br><a id="more"></a></p><h1 id="状态间的互通性"><a href="#状态间的互通性" class="headerlink" title="状态间的互通性"></a>状态间的互通性</h1><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/15.jpg" alt=""></p><p>互通性的三个性质：</p><ol><li>自返律（假设状态有<strong>自环</strong>）：$\mathbf{i} \leftrightarrow \mathbf{i}​$</li><li>对称律：$\mathbf{i} \leftrightarrow \mathbf{j}​$当且仅当$\mathbf{j} \leftrightarrow \mathbf{i}​$</li><li>传递律：如果$\mathbf{i} \leftrightarrow \mathbf{k}$且$\mathbf{k} \leftrightarrow \mathbf{j}$，那么$\mathbf{i} \leftrightarrow \mathbf{j}$ </li></ol><h1 id="链的可约性"><a href="#链的可约性" class="headerlink" title="链的可约性"></a>链的可约性</h1><p><strong>不可约性</strong>：如果一个马氏链的任意两个状态都互通，则这个马氏链不可约；否则可约。</p><p>不可约的马氏链：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/16.jpg" alt=""></p><p>可约的马氏链：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/17.jpg" alt=""></p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/18.jpg" alt=""></p><h1 id="状态的周期性"><a href="#状态的周期性" class="headerlink" title="状态的周期性"></a>状态的周期性</h1><p>状态的周期性有个挠头的定义（$from$ 张波《应用随机过程》）：</p><blockquote><p>记$d_i$为数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$的最大公约数，则称它为状态$i$的周期。</p><p>若对一切$n{\ge}1$有$p_{i i}^{(n)}=0$，则约定$d_{i}=\infty$。</p><p>当$d_i&gt;1​$时，称$i​$是有周期的状态；当$d_i=1​$时，称$i​$是非周期的状态。</p></blockquote><p>数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$指的是从状态$i$出发再次回到状态$i​$的<strong>步数</strong>的集合。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/19.jpg" alt=""></p><p>上图状态1再次回到状态1的方式有：</p><ol><li>$1{\to}1$：步数为1</li><li>$1\to2\to1$：步数为2</li><li>重复方式1，或者重复方式2，或者方式1和方式2的组合，例如$1\to1\to1$，$1\to2\to1\to1$</li></ol><p>因此状态1回到状态1的步数集合是{1, 2, 3, 4, 5, …}，$d_i=1$，状态1是非周期的；</p><p>状态2回到状态2的基本方式是{$2\to1\to2$：2步，$2\to1\to1\to2$：3步}，步数集合是{2，3，4，5，…}，最大公约数$d_i=1$，因此状态2是非周期的。</p><p>再看下图：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/21.jpg" alt=""></p><p>状态1自返的基本方式是：</p><ul><li>$1\to2\to3\to4\to1$，步数为4</li><li>$1\to5\to6\to7\to8\to9\to1$，步数为6</li></ul><p>状态1自返的步数集合是{4，6，8，10，…}，最大公约数$d_i=2$，所以状态1是周期的。</p><p>状态2，3，4的步数集合是{4，8，10，12，14，…}，都是周期的。</p><p>状态5，6，7，8，9的步数集合是{6，10，12，18，…}，都是周期的。</p><p>如何简单判断一个状态是<strong>周期/非周期</strong>的？</p><ul><li>带有<strong>自环</strong>的状态一定是非周期的（因为$d_i=1$），但不是所有非周期的状态都有自环，如下条</li><li>与非周期状态互通的状态一定是非周期的</li></ul><h1 id="状态的常返性"><a href="#状态的常返性" class="headerlink" title="状态的常返性"></a>状态的常返性</h1><p><strong>常返性</strong>即：马氏链由一个状态出发之后能否再次回归到本状态的特性。</p><p>常返性分为三种：</p><ul><li><strong>正</strong>常返（必定会返回，平均返回时间为有限值）</li><li><strong>零</strong>常返（必定会返回，平均返回时间为 $\infty​$ ）</li><li><strong>非</strong>常返（可能不再返回）</li></ul><p>定义略</p><p><strong>不可约马氏链的状态一致性</strong>定理：不可约马氏链的状态集全为正常返，或者全为零常返，或者全为非常返，并且每个状态的周期相同。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/20.jpg" alt=""></p><p>上图不可约马氏链中：</p><ul><li>如果 $p&lt;q$，那么全为正常返；</li><li>如果 $p=q$，那么全为零常返；</li><li>如果 $p&gt;q$，那么全为非常返。</li></ul><h1 id="状态-链的遍历性"><a href="#状态-链的遍历性" class="headerlink" title="状态/链的遍历性"></a>状态/链的遍历性</h1><p>如果齐次马氏链中的某个状态是非周期、正常返状态，称这个状态是<strong>可遍历</strong>的。</p><p>如果马氏链所有状态全互通（不可约）、可遍历（非周期、正常返），称这个马氏链为<strong>遍历链</strong>。</p><p>遍历链存在一个<strong>平稳分布</strong>：</p><ul><li>平稳分布与初始状态无关</li><li>平稳分布是唯一的</li><li>平稳分布全部大于0</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;互通、可约、周期、常返、遍历&lt;br&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样1 - 逆变换采样和拒绝采样</title>
    <link href="http://chenyin.top/stat/20190415-a232.html"/>
    <id>http://chenyin.top/stat/20190415-a232.html</id>
    <published>2019-04-15T09:27:14.000Z</published>
    <updated>2019-04-22T01:40:45.196Z</updated>
    
    <content type="html"><![CDATA[<p><strong>蒙特卡洛方法（Monte Carlo Method）</strong>尝试利用计算机模拟随机数（伪随机数）解决一类问题，这类问题通常是：1. 所求解的问题本身具有内在随机性，例如中子与原子核的相互作用受量子力学规律的制约；2. 所求解问题可以转化为某种随机分布的特征数，例如通过撒豆子的方式计算不规则图形的面积。蒙特卡洛法是一种以概率统计理论为指导的数值计算方法。</p><p><strong>抽样</strong>（<strong>采样</strong>）指从总体中抽取一部分作为样本。计算机模拟中，抽样意味着从一个概率分布中生成一个观察值，这涉及到一个随机的过程。一般认为计算机只能进行均匀分布的采样，对于复杂的概率分布，需要进行采样方法设计。</p><a id="more"></a><h1 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h1><p>从图像上理解连续型随机变量的采样是个什么玩意儿十分形象：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/10.jpg" alt=""><br>左图是正态分布的<strong>概率密度函数（Probability Density Function，PDF）</strong>记为$f(x)​$，右图是正态分布的<strong>累积分布函数（Cumulative Distribute Function，CDF）</strong>记为$F(x)​$。随机变量的采样就是在$[0,1]​$均匀分布采样的基础上，选取尽可能分散的点，使这些点尽可能地拟合CDF曲线。</p><!-- 当$F(x)$的反函数$F^{-1}(u)$的定义域始终是$[0,1]$。如果我们在$F^{-1}(u)$的定义域上多次均匀采样，此时可以认为相邻采样点的横坐标之差相同，反函数曲线变化趋势只与相邻点的纵坐标相关。这样，我们基于$[0,1]$的均匀采样产生了一组数（或者说一组点），这组点很好的拟合了反函数$F^{-1}(u)$，这就意味着这组点也能很好拟合$F(x)$。相应的，这组点的纵坐标取值集合就是我们针对$F(x)$的一组采样。--><h1 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h1><p>逆变换采样虽然简单有效，但是其应用场景十分有限：当累积分布函数或者反函数难求时，而实际情况往往是这样。</p><p>下图中的$f(x)$是我们采样的目标PDF，当其CDF或者CDF的反函数不容易求的时就不能直接对$f(x)$进行采样。<strong>拒绝采样（Rejection Sampling）</strong>的基本思想是借助这样一个参考概率密度函数$f_r(x)$即下图中的$Mg(x)$：</p><ul><li>$f_r(x)$十分容易进行采样，例如取均匀分布意味着参考PDF可以直接进行逆变换采样</li><li>$f_r(x)$位于$f(x)$上方，即对任意$x$有$f_r(x){\ge}f(x)$</li><li>$Mg(x)$表示将均匀分布$g(x)$向上移动，此时以$f(x)$的极大值确定$M$的值效果比较好</li></ul><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/11.jpg" width="50%"></p><p>从图上来看，参考PDF“罩住”了目标PDF：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/12.jpg" width="50%"></p><p>拒绝采样的过程如下：</p><ol><li><p>从$f_r(x)$进行一次采样$x_i$</p></li><li><p>计算$x_i$的<strong>接受概率</strong>$\alpha$（Acceptance Probability）：<br>$$<br>\alpha=\frac{f\left(x_{i}\right)}{f_r\left(x_{i}\right)}<br>$$</p></li><li><p>从$(0,1)$均匀分布中进行一次采样$u$</p></li><li>如果$\alpha{\ge}u$，接受$x_i$作为一个来自$f(x)$的采样；否则，重复第1步</li></ol><p>显然对于特定的目标PDF，参考PDF不止一个，不同PDF的$\max(\alpha)$不同。以均匀分布采样为例，当参考PDF从上面越靠近目标PDF采样效率越高，相应的寻找这样的参考PDF的难度就越大。采样效率高意味着对于那些概率密度较小的区域有更大的几率能够采样到。</p><p>为了平衡采样效率和参考PDF的确定难度，提出了<strong>自适应拒绝采样</strong>。</p><h1 id="自适应拒绝采样"><a href="#自适应拒绝采样" class="headerlink" title="自适应拒绝采样"></a>自适应拒绝采样</h1><p>当参考PDF不能很好的“罩住”目标PDF时，那些未罩住区域内的采样点被拒绝的概率就会很大，采样效率低。所以如果能够找到一个跟目标PDF非常接近的参考PDF，即参考PDF计划能够完全从上面贴合目标PDF，此时能够达到较好的采样效率。</p><p>当目标PDF是<strong>log-concave函数</strong>时可以采用<strong>自适应拒绝采样（Adaptive Rejection Sampling，ARS）</strong>。</p><blockquote><p><strong>log-concave函数</strong>：当概率密度函数$f(x)$是凹函数（concave）且$\log{f(x)}$仍然是凹函数时，$f(x)$称之为log-concave函数：<br>$$<br>f(\theta x+(1-\theta) y) \geq \theta f(x)+(1-\theta) f(y) \<br>\log f(\theta x+(1-\theta) y) \geq \theta \log f(x)+(1-\theta) \log f(y)<br>$$</p></blockquote><p>在log-concave函数上随机选取一些点做切线：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/13.jpg" width="50%"></p><p>将log-concave函数变换回原来的PDF，此时上图的切线将变成曲线（取指数），它！们！弯！了！</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/14.jpg" width="50%"></p><p>将这组弯了的“切线”组成成一个分段函数，这个分段函数将会很好的贴合目标PDF。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;蒙特卡洛方法（Monte Carlo Method）&lt;/strong&gt;尝试利用计算机模拟随机数（伪随机数）解决一类问题，这类问题通常是：1. 所求解的问题本身具有内在随机性，例如中子与原子核的相互作用受量子力学规律的制约；2. 所求解问题可以转化为某种随机分布的特征数，例如通过撒豆子的方式计算不规则图形的面积。蒙特卡洛法是一种以概率统计理论为指导的数值计算方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;抽样&lt;/strong&gt;（&lt;strong&gt;采样&lt;/strong&gt;）指从总体中抽取一部分作为样本。计算机模拟中，抽样意味着从一个概率分布中生成一个观察值，这涉及到一个随机的过程。一般认为计算机只能进行均匀分布的采样，对于复杂的概率分布，需要进行采样方法设计。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>git疑难杂症</title>
    <link href="http://chenyin.top/bug/20190415-bb05.html"/>
    <id>http://chenyin.top/bug/20190415-bb05.html</id>
    <published>2019-04-15T08:14:20.000Z</published>
    <updated>2019-04-15T08:18:08.016Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/09.jpg" alt=""></p><p><strong>报错关键词</strong>：Updates were rejected because a pushed branch tip is behind its remote counterpart.</p><p><strong>原因分析</strong>：直接在远程master分支进行修改或者有其他人修改后已经提交到了远程master，而本地使用test（非本地master分支）分支进行再次修改后直接push到远程master分支，此时本地master分支的版本还是远程master分支修改之前的版本，即本地master的版本落后于远程master的版本，因此导致push失败。</p><p><strong>解决办法</strong>：先 <code>git checkout master</code> 到本地master分支，再 <code>git pull 远程仓库 master</code> 拉取最新版本，再 <code>git checkout test</code> 回到本地工作分支，再 <code>git push 远程仓库 master</code> 推送最新版本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/09.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;报错关键词&lt;/strong&gt;：Updates were rejected b
      
    
    </summary>
    
      <category term="bug" scheme="http://chenyin.top/categories/bug/"/>
    
    
  </entry>
  
  <entry>
    <title>python3中的字符串与编码问题</title>
    <link href="http://chenyin.top/python/20190411-d7e5.html"/>
    <id>http://chenyin.top/python/20190411-d7e5.html</id>
    <published>2019-04-11T06:33:52.000Z</published>
    <updated>2019-09-15T02:22:19.728Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/05.jpg" alt=""></p><p>python3的编码方式比python2已经简单很多了，不过还是让我等菜鸟头疼。廖老师的教程写的着实通俗易懂，恍然大悟。</p><a id="more"></a><h1 id="ASCII、Unicode和UTF-8"><a href="#ASCII、Unicode和UTF-8" class="headerlink" title="ASCII、Unicode和UTF-8"></a>ASCII、Unicode和UTF-8</h1><p>所有的字符串都会拆分成单个字符进行传输和存储。对字符串编码不仅要考虑字符集的完整性、不同语言字符集的兼容性，还要考虑传输存储空间大小。</p><p><strong>ASCII</strong></p><p>ASCII全称是American Standard Code for Information Interchange (美国信息交换标准代码)。</p><p>计算机是美国人发明的，因此早期的ASCII编码只考虑到26个英文字母和一些简单的字符。由于数量较少，我们只需要用<strong>一个字节</strong>就可以完成编码，所以ASCII编码对应的整数值从0到255。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/ascii.png" alt=""></p><p><strong>Unicode</strong></p><p>ASCII编码不能用来编码中文字符，当然也不能用来编码日文字符、韩文字符和阿拉伯文字符等等等等。如果每个国家都为了是计算机能识别自己特有的字符而使用自己的编码方式，当处理多国语言文本时就会出现许多问题。所以大家统一规定了Unicode编码。换句话说，Unicode编码兼容地球上所有国家的语言符号！真是一个令人振奋的消息~</p><p>一般情况下，Unicode使用<strong>两个字节</strong>对字符进行编码（对于某些十分偏僻的字符可能要用到更多的字节）。对于原来的ASCII编码的字符，在它们的ASCII编码（二进制编码）前加上一个字节的0（即8个0）即可完美转化为Unicode编码。</p><table><thead><tr><th style="text-align:center">字符</th><th style="text-align:center">ASCII编码</th><th style="text-align:center">Unicode编码</th><th style="text-align:center">UTF-8编码</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center"><code>01000001</code></td><td style="text-align:center"><code>00000000 01000001</code></td><td style="text-align:center"><code>01000001</code></td></tr><tr><td style="text-align:center">中</td><td style="text-align:center">不能编码</td><td style="text-align:center"><code>01001110 00101101</code></td><td style="text-align:center"><code>11100100 10111000 10101101</code></td></tr></tbody></table><p>好了，现在通过Unicode能够兼容所有语言的字符集了。但是又出现了新的问题：当我的文本（几乎）是全英文是，如果直接使用Unicode编码文件，其传输的字节数量和占用的磁盘空间都是ASCII编码的两倍，显然十分多余。为了解决这个问题，又出现了UTF-8编码。</p><p><strong>UTF-8</strong></p><p>UTF-8编码是不定长的编码方式：那些可以用ASCII编码的字符转化成UTF-8编码时仍然只有一个字节，UTF-8编码的汉字一般是三个字节，少数特殊字符所用的字节数更多。所以，UTF-8是完美兼容ASCII编码的，只支持ASCII编码的应用仍然可以在UTF-8编码上运行。</p><p><strong>总结</strong></p><p>总结以下：</p><ol><li>ASCII编码只适用于英文字符，固定单字节；</li><li>Unicode编码是内存中的编码方式，固定多字节；</li><li>UTF-8编码优化了Unicode编码，不定字节；</li></ol><h1 id="计算机编码工作方式"><a href="#计算机编码工作方式" class="headerlink" title="计算机编码工作方式"></a>计算机编码工作方式</h1><p>字符串所在的场景不外乎两种：</p><ol><li>当我们在python程序中处理字符串时，它们存储在内存中，此时使用Unicode编码字符串；</li><li>当我们存储字符串时往往使用UTF-8等编码方式（建议只适用UTF-8编码存储文件）。</li></ol><p><strong>场景1：记事本编辑文件</strong></p><p>文本文件通常以UTF-8编码存储在磁盘上，记事本应用读取文件到内存中，此时字符串改用Unicode编码，当我们编辑完成之后需要使用UTF-8编码文件保存到磁盘。</p><p><strong>场景2：网页浏览</strong></p><p>当我们浏览某些网页的时候，服务器会自动生成信息，这些字符串是Unicode编码；服务器使用UTF-8编码这些字符串传输至浏览器并显示。</p><h1 id="python3中的字符串"><a href="#python3中的字符串" class="headerlink" title="python3中的字符串"></a>python3中的字符串</h1><p>python中字符串类型是<code>str</code>，它们以Unicode方式编码。</p><p><strong>字符</strong>通过Unicode编码可以与数字（二进制、十进制、十六进制…）进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取字符的Unicode编码的十进制整数值</span></span><br><span class="line">ord(<span class="string">'A'</span>) <span class="comment"># 65</span></span><br><span class="line">ord(<span class="string">'中'</span>) <span class="comment"># 20013</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 将十进制整数通过Unicode编码方式转化为字符</span></span><br><span class="line">chr(<span class="number">65</span>) <span class="comment"># 'A'</span></span><br><span class="line">chr(<span class="number">20013</span>) <span class="comment"># '中'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 注意：</span></span><br><span class="line"><span class="comment"># ord()只能转换单个字符，传递字符串会报错</span></span><br><span class="line"><span class="comment"># 传入chr()接收的整数如果超过Unicode编码的范围也会报错</span></span><br></pre></td></tr></table></figure><p>我们在程序中处理的字符串都是Unicode编码的，一个字符占据了多个字节。</p><p>当进行网络传输或者本地存储时，<code>str类型</code>的字符串需要被转换为字节数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单单一个'中'是Unicode编码的，直观上就没有反映出字节信息</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad'</span></span><br><span class="line"><span class="comment"># 从encode结果我们可以看到'中'包含了三个字节</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="string">b'\xd6\xd0'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面两种方式军会报错LookupError</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'unicode'</span>) <span class="comment"># 因为'中'本来就是Unicode编码了</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'ascii'</span>) <span class="comment"># ASCII不能编码汉字字符</span></span><br></pre></td></tr></table></figure><p>python中</p><ol><li><p><code>str.encode(CODING_TYPE)</code>方法可以将Unicode字符串转换成CODING_TYPE编码的字符串；</p></li><li><p><code>bytes.decode(ORIGIN_TYPE)</code>方法将ORIGIN_TYPE编码的字节串解码成Unicode字符串。</p></li><li><p>encode的参数是目标编码方式，decode的参数是源字节串的编码方式。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad'</span>.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'中'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xd6\xd0'</span>.decode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="string">'中'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## .decode(..., errors='ignore')可忽略掉部分不能正确解码的字节</span></span><br><span class="line"><span class="comment"># \xe4\xb8\xad 被忽略</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xd6\xd0'</span>.decode(<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># \xd6\xd0 被忽略</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xd6\xd0'</span>.decode(<span class="string">'gbk'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 下面两种是错误的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad'</span>.decode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xd6\xd0'</span>.decode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p><code>len(...)</code>函数接收<strong>字符串</strong>时计算的是字符串的<strong>字符数目</strong>，接收<strong>字节串</strong>时计算的是字节串的<strong>字节数</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">len(<span class="string">'中国'</span>) <span class="comment">#=&gt; 2</span></span><br><span class="line">len(<span class="string">'中国'</span>.encode(<span class="string">'utf-8'</span>)) <span class="comment">#=&gt; 6，等价于 len(b'\xe4\xb8\xad\xe5\x9b\xbd')</span></span><br><span class="line">len(<span class="string">'中国'</span>.encode(<span class="string">'gbk'</span>)) <span class="comment">#=&gt; 4，等价于 len(b'\xd6\xd0\xb9\xfa')</span></span><br></pre></td></tr></table></figure><p><strong>.py源文件</strong>也是文本，我们在保存是应该保存为UTF-8编码。为了使python解释器能正确读取源文件，我们需要指定解释器读取源文件的编码方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure><p>notepad++应该选择 <strong>Encoding in UTF-8 without BOM</strong> 编码文件！</p><h1 id="中文文件的读写"><a href="#中文文件的读写" class="headerlink" title="中文文件的读写"></a>中文文件的读写</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding参数指定原文件的编码格式，可设置errors='ignore'</span></span><br><span class="line"><span class="keyword">with</span> open(INPUT_FP, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    OUTPUT_STRING = ...</span><br><span class="line">    </span><br><span class="line"><span class="comment"># encoding指定新文件的编码格式</span></span><br><span class="line"><span class="keyword">with</span> open(OUPUT_FP, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    writer.write(OUTPUT_STRING)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 不要将这里的encoding参数和上面的.encode/.decode混淆：</span></span><br><span class="line"><span class="comment"># |- encoding参数指定输入文件/输出文件的编码类型，一般在读文件（本地/网络）的时候才用到</span></span><br><span class="line"><span class="comment"># |- .encode/.decode方法指定unicode编码字符串与其它编码字符串间的转换方式</span></span><br></pre></td></tr></table></figure><p>参考自<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431664106267f12e9bef7ee14cf6a8776a479bdec9b9000" target="_blank" rel="noopener">廖雪峰的python教程-python基础-字符串和编码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/05.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;python3的编码方式比python2已经简单很多了，不过还是让我等菜鸟头疼。廖老师的教程写的着实通俗易懂，恍然大悟。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://chenyin.top/categories/python/"/>
    
    
  </entry>
  
  <entry>
    <title>灵感源自macaron让人十分舒适的十种颜色</title>
    <link href="http://chenyin.top/cargo/20190410-3bf3.html"/>
    <id>http://chenyin.top/cargo/20190410-3bf3.html</id>
    <published>2019-04-10T03:09:17.000Z</published>
    <updated>2019-04-14T16:02:20.846Z</updated>
    
    <content type="html"><![CDATA[<p>Macaron是一种用蛋白、杏仁粉、白砂糖和糖霜制作，并夹有水果酱或奶油的法式甜点。口感丰富，外脆内柔，外观五彩缤纷，精致小巧。Macaron 10 色因为它们的视觉舒适而广为流传。</p><p><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/01.jpg" alt=""></p><a id="more"></a><table><thead><tr><th style="text-align:center">英文名</th><th style="text-align:center">代码</th><th style="text-align:center">视觉改释</th></tr></thead><tbody><tr><td style="text-align:center">bewitched tree</td><td style="text-align:center"><code>#19CAAD</code> 或 <code>rgb(19,202,173)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#19CAAD"></div></td></tr><tr><td style="text-align:center">mystical green</td><td style="text-align:center"><code>#8CC7B5</code> 或 <code>rgb(140,199,181)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#8CC7B5"></div></td></tr><tr><td style="text-align:center">light heart blue</td><td style="text-align:center"><code>#A0EEE1</code> 或 <code>rgb(160,238,225)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#A0EEE1"></div></td></tr><tr><td style="text-align:center">glass gall</td><td style="text-align:center"><code>#BEE7E9</code> 或 <code>rgb(190,231,233)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#BEE7E9"></div></td></tr><tr><td style="text-align:center">silly fizz</td><td style="text-align:center"><code>#BEEDC7</code> 或 <code>rgb(190,237,199)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#BEEDC7"></div></td></tr><tr><td style="text-align:center">brain sand</td><td style="text-align:center"><code>#D6D5B7</code> 或 <code>rgb(214,213,183)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#D6D5B7"></div></td></tr><tr><td style="text-align:center">mustard addicted</td><td style="text-align:center"><code>#D1BA74</code> 或 <code>rgb(209,186,116)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#D1BA74"></div></td></tr><tr><td style="text-align:center">magic powder</td><td style="text-align:center"><code>#E6CEAC</code> 或 <code>rgb(230,206,172)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#E6CEAC"></div></td></tr><tr><td style="text-align:center">true blush</td><td style="text-align:center"><code>#ECAD9E</code> 或 <code>rgb(236,173,158)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#ECAD9E"></div></td></tr><tr><td style="text-align:center">merry cranesbill</td><td style="text-align:center"><code>#F4606C</code> 或 <code>rgb(244,96,108)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#F4606C"></div></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Macaron是一种用蛋白、杏仁粉、白砂糖和糖霜制作，并夹有水果酱或奶油的法式甜点。口感丰富，外脆内柔，外观五彩缤纷，精致小巧。Macaron 10 色因为它们的视觉舒适而广为流传。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/01.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="cargo" scheme="http://chenyin.top/categories/cargo/"/>
    
    
  </entry>
  
  <entry>
    <title>降维04 - TSNE引领时尚</title>
    <link href="http://chenyin.top/ml/20190328-acd8.html"/>
    <id>http://chenyin.top/ml/20190328-acd8.html</id>
    <published>2019-03-28T03:12:08.000Z</published>
    <updated>2019-04-23T07:23:06.304Z</updated>
    
    <content type="html"><![CDATA[<div style="display: inline-block; width: 670px; height: 456px; overflow: hidden; border: 1px solid #ddd"><br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.gif" style="margin-top: -114px !important; border: none"><br></div><p><strong>t-SNE</strong> (<strong>t-distributed Stochastic Neighbor Embedding</strong>) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。</p><a id="more"></a><p><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/02.png" alt=""></p><p>相比于SNE，t-SNE的主要优化有：联合概率替代条件概率、低维空间下使用t分布代替高斯分布。<a href="#ref1">[1]</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>早期的<strong>可视化 (Visualization)</strong> 工具<u>不负责解释数据</u>，这就限制了这些工具在真实世界数据上的应用，因为我们要想解释数据，我们还是只能靠人眼看。相比于能解释数据的监督学习而言，可视化只需要展示训练数据，而不需要训练模型使它能够拟合到测试数据集。可视化的任务简单许多。<a href="#ref2">[2]</a></p><h2 id="线性降维"><a href="#线性降维" class="headerlink" title="线性降维"></a>线性降维</h2><p> 将数据从高位空间映射到低维空间的过程我们称之为 <strong>map</strong>，相应的，低维空间中的映射点被称之为 <strong>map points</strong>。降维算法已经注意到，要将高维空间中的数据结构问题尽可能的保留到低维空间。</p><p> 但是传统的<strong>线性降维 (Linear dimentionality reduction)</strong>  算法，例如PCA、MDS，更加侧重<u>在低维空间中保持高维空间中的<strong>差异性</strong></u>，即尽可能地分开数据。同时它们更加关注数据地<strong>全局特征</strong>，这点与非线性降维算法显著不同。</p><h2 id="非线性降维"><a href="#非线性降维" class="headerlink" title="非线性降维"></a>非线性降维</h2><p>大部分<strong>非线性降维 (non-linear dimentionality reduction)</strong> 算法关注的是 <u>在低维空间中保持高维空间地<strong>局部特征</strong></u>。这就意味着，它们不能同时关注数据的全局特征和局部特征。全局特征就是基于所有数据进行的解释，比如聚类结果就是基于所有数据进行的，理想情况下每个数据点都能找到它自己所属的类；局部特征只是基于部分数据点进行的推导，比如在SNE算法中，总是计算离中心点欧式距离小的部分点进行下降，它关注的是以中心点为圆心，以有限长度为半径的（超）球体内的点。</p><p>下面7个常见的非线性降维算法，它们在局部特征提取上都是很优秀的：Sammon mapping <a href="#ref3">[3]</a>, CCA <a href="#ref4">[4]</a>, SNE <a href="#ref5">[5]</a>, Isomap <a href="#ref6">[6]</a>, MVU <a href="#ref7">[7]</a>, LLE <a href="#ref8">[8]</a>, Laplacian Eigenmaps <a href="#ref9">[9]</a>。</p><h2 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h2><p>t-SNE继承自SNE算法，同样是非线性降维，它的优势在于：能够保持大部分局部特征到低维空间，同时不丢失全局特征（例如聚类）。</p><p>与SNE一样，t-SNE的思想还是计算两个点间的<strong>相似度</strong> (similarity)。</p><h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1><p>SNE尽管能得到比较好的可视化结果，但是它的损失函数难以优化，并且还存在 <strong>crowding problem (拥挤问题)</strong> 。相比之下，t-SNE能缓和上面提到的所有问题（优化问题和拥挤问题），与SNE相比，t-SNE主要在两个方面进行改进：<br>1.使用<strong>对称</strong>的损失函数，新的损失函数求导会更加容易。<a href="#ref10">[10]</a><br>2.计算低维空间中两点的相似度使用<strong>t分布</strong>而不是高斯分布，t分布是一种<a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution" target="_blank" rel="noopener"><strong>重尾分布 (heavy-tailed distribution)</strong></a>，它能够有效缓解拥挤问题和优化问题，后面将会详细介绍。</p><h2 id="优化SNE成对称结构"><a href="#优化SNE成对称结构" class="headerlink" title="优化SNE成对称结构"></a>优化SNE成对称结构</h2><p><strong>联合概率替换条件概率</strong></p><p>在SNE中我们通过<strong>条件概率</strong>分别计算高维空间和低维空间中<strong>点对</strong>间的相似度：<br>$$\begin{cases}<br>&amp; p_{ij}=p(x_j|x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{k{\ne}i}{\exp(-||x_i-x_k||^2)}} \\<br>&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}}<br>\end{cases}$$</p><p>然后在t-SNE中我们将条件概率换成<strong>联合概率</strong>：<br>$$\begin{cases}<br>&amp; p_{ij}=p(x_j,x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{m{\ne}n}{\exp(-||x_m-x_n||^2)}} \\<br>&amp; q_{ij}=q(y_j,y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}<br>\end{cases}$$</p><p>注意上面两种表述方式的分母的差异：</p><ul><li>条件概率的分母是中心点 $x_i$ 与其它所有点的相似度之和;</li><li>联合概率的分母没有中心点一说，计算的是所有点对（n个数据点有 $C_n^2$ 个点对）的相似度之和。</li><li>条件概率中 $p_{ij}{\ne}p_{ji}$，而联合概率中 $p_{ij}=p_{ji}$（q同理），这正好也与分母的这种差异吻合。</li></ul><p><strong>注意到联合概率算法会产生一个条件概率算法不会遇到的问题：离群点。</strong></p><p>观察上面的联合概率公式，对于离群点 $x_i$，所有与它配对计算出来的 $p_{ij}$ 或者 $p_{ji}$ 的 $||x_i-x_j||^2$ 将会特别大，这导致 $p_{ij}$ 或者 $p_{ji}$ 总是特别的小，即与 $x_i$ 相关的 $p_{ij}$ 或者 $p_{ji}$ 在对损失函数的贡献总是特别小。这相当于自动减小了那些低密度区域的点在损失函数中的权重，使得通过相似性确定离群点在低维空间中的位置更加困难。<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/03.png" alt=""></p><p>所以呢，必须想办法消除这种效应，增大离群点在损失函数中的比重，文中用用条件概率公式代替上述 $p_{ij}$ 的定义，即：<br>$$\begin{cases}<br>&amp; p_{ij}=\frac{p(x_j|x_i)+p(x_i|x_j)}{2n}  \\<br>&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}<br>\end{cases}$$</p><p>这样每个点 $x_i$ 对损失函数的贡献度 $p(x_i)=\sum_jp_{ij}&gt;\frac1{2n}$，这就保证了离群点的贡献不会太少。</p><p><strong>KL散度作为损失函数</strong></p><p>t-SNE仍然使用KL散度作为损失函数，所不同的是，这里求的是两个联合概率分布之间的散度：<br>$$C=KL(P||Q)=\sum_i\sum_jp_{ij}\log{\frac{p_{ij}}{q_{ij}}}$$</p><p>此时KL损失函数求导的结果更加简洁：<br>$$\frac{\partial{C}}{\partial{y_i}}=4\sum_k(p_{ik}-q_{ik})(y_i-y_k)$$</p><p>SNE求导结果为：<br>$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$</p><h2 id="解决SNE的拥挤问题"><a href="#解决SNE的拥挤问题" class="headerlink" title="解决SNE的拥挤问题"></a>解决SNE的拥挤问题</h2><h3 id="什么是拥挤问题"><a href="#什么是拥挤问题" class="headerlink" title="什么是拥挤问题"></a>什么是拥挤问题</h3><h4 id="流形的直观理解"><a href="#流形的直观理解" class="headerlink" title="流形的直观理解"></a>流形的直观理解</h4><p>manifold的<a href="https://en.wikipedia.org/wiki/Manifold" target="_blank" rel="noopener">Wiki解释</a>：</p><blockquote><p>In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. </p></blockquote><p>而中文概念“流形”是由北大已故数学教授江泽涵先生提出来。江老的堂姐夫是胡适？… … 不过“流形：这个词真的很艺术，我初次见到时就感叹其形象而不能自已。流形的<a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E5%BD%A2" target="_blank" rel="noopener">Wiki中文解释</a>：</p><blockquote><p>是局部具有欧几里得空间性质的空间，是欧几里得空间中的曲线、曲面等概念的推广。欧几里得空间就是最简单的流形的实例。地球表面这样的球面则是一个稍微复杂的例子。一般的流形可以通过把许多平直的片折弯并粘连而成。</p></blockquote><p><strong>为什么说二维流形面上的点距容易建模 (model)？</strong></p><p>这个问题直观上理解最是简单。首先对于欧几里得空间，我们普通人类最多能直观感受到三维。换算成黎曼空间，就意味着我们只能在三维空间中直观感不超过二维流形曲面的存在，二维流形曲面上的距离就是曲面内连接它们的最短曲线长度。经典的二维流形曲面如下（Swiss Roll 流形, <a href="http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html%29" target="_blank" rel="noopener">Swiss Roll dataset</a>）：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/04.png" alt=""></p><h4 id="为什么存在拥挤"><a href="#为什么存在拥挤" class="headerlink" title="为什么存在拥挤"></a>为什么存在拥挤</h4><p>为了便于可视化，我们会将高维流形上的点映射到二维空间，同时最大程度的保留它们的相对位置（这种每个点相对于整体数据点的定位就是一种全局特征）。然而这种映射是很难完美实现的，举个例子，十维空间（欧几里得空间或者黎曼空间）中可以很容易找到11个相互等距的点（就好比二维空间中能轻易找到三个相互等距的点一样），然而映射到二维空间是不可能找到11个相互等距的点的，势必会有一些点会相互靠近挤在一起，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/05.png" alt=""><br>以二维相互等距的三个点映射到一维空间为例，无论怎么努力，三点都不可能再等距。归根揭底，不同维度空间内的距离分布是不同的，降维映射难免尽如人意。</p><p>再以球内区域为例解释crowding现象：以数据点 $x_i$ 为中心的球的体积与 $r^m$ 直接相关（ $r$ 是半径，$m$ 是球所在空间的维度）。如果在十维流形曲面上数据点均匀分布在这个球中，我们试图在二维流形曲面上以 $y_i$ 为中心对与 $x_i$ 相关的两两距离进行建模。此时我们就会遇到传说中的拥挤问题：与容纳中心点附近数据点的区域相比，容纳适中距离数据点的区域显得不够用。</p><p>在均匀分布的条件下，等距点的数量与半径相关，距离越大数量越多，这意味着映射到低维空间就会越“挤”。因此如果我们想要较为准确的在二维流形曲面中对以 $x_i$ 为中心的两两距离建模，我们就必须把距离 $x_i$ 适中位置的那些点往更远的地方推置（因为太挤了）。</p><!--在SNE中，数据点 $y_i$ 与其它点之间存在一个微弱的引力，所有的力相互作用最终使所有的点**分散在**了自己的收敛位置，这使得点的分布具有连续性质，在聚类时也不会出现断层的现象。--><p>不只是SNE，其它局部特征提取算法例如Sammon mapping等也都面临着拥挤难题。</p><h3 id="怎么解决拥挤问题"><a href="#怎么解决拥挤问题" class="headerlink" title="怎么解决拥挤问题"></a>怎么解决拥挤问题</h3><p>一种叫做<strong>UNI-SNE</strong>的改良算法<a href="#ref10">[10]</a>提出了一种解决办法：给每一个两两相似度添加一个背景值，背景值采样自均匀分布并以一定的比例 $\rho$ 进行混合。由于每个点对之间都引进了背景值，因此不管低维空间中两个映射点离的多么远，$q_{ij}$ 永远不会小于 $\frac{\rho}{n(n-1)/2}$（n个数据点可组合成 $C_n^2$ 个点对）。</p><p>引进背景值导致对于高维空间中相距很远的两个数据点总有$q_{ij}&gt;p_{ij}$（$p_{ij}{\rightarrow}0$ 时 $q_{ij}{\rightarrow}\frac{\rho}{n(n-1)/2}$），这表示低维空间中点对并没有完全拟合高维空间的点对相似度，映射后相似度变小。</p><p>尽管UNI-SNE的效果比SNE好，但是其损失函数却很难优化。目前较好的优化UNI-SNE的方法是：开始的时候将背景值混合比例设为0，这实际上等效于运行SNE；当SNE开始使用<strong>模拟退火</strong>策略时增大背景值的混合比例，促进自然分类间的gaps形成。</p><p>直接优化UNI-SNE并不可行，因为低维空间中两个相距很远的映射点的 $q_{ij}$ 几乎全部来自于背景值，即高维空间中相应两点间（即使他们的 $p_{ij}$ 很大）的距离对 $q_{ij}$ 的影响很小，这使得映射后的两点间的 $q_{ij}$ 没有什么实际意义。这表示，如果一个自然类的两部分在优化早期就分开了，就再也不会再聚合在一起了。</p><h2 id="低维空间采用柯西分布表达联合概率"><a href="#低维空间采用柯西分布表达联合概率" class="headerlink" title="低维空间采用柯西分布表达联合概率"></a>低维空间采用柯西分布表达联合概率</h2><p>UNI-SNE通过添加背景值使低维空间中相距甚远的 $q_{ij}$ 不至于趋近于0。</p><p>本文提出了一种新的解决办法，采用与高斯分布性质极其相似的重尾分布计算联合概率。右重尾分布使得当随机变量取值很大时其对应的概率值高斯分布要大，典型的重尾分布是t分布，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/06.png" alt=""><br>t分布实际上是不同方差的高斯分布的混合分布，它的性质与高斯分布十分接近，而且更加容易计算：因为高斯分布涉及到指数运算，而t分布只需要求倒数。</p><p>这里采用的是自由度 $\nu=1$ 的t、分布，又叫做<strong>柯西分布</strong>，其概率密度函数如下：<br>$$f(x;x_0,\gamma)=\frac1{\pi\gamma[1+(\frac{x-x_o}{\gamma})]^2}$$</p><p>取 $x_0=0, \gamma=1$ 得标准柯西分布：<br>$$f(x;0,1)=\frac1{\pi[1+(x-x_0)]^2}$$</p><p>用标准柯西分布表示联合概率：<br>$$q_{ij}=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum_{m{\ne}n}{(1+||y_m-y_n||^2)^{-1}}}=\frac{\sum_{m{\ne}n}(1+||y_m-y_n||^2)}{1+||y_i-y_j||^2}$$</p><p>求导结果如下：<br>$$\frac{\partial{C}}{\partial{y_i}}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}}$$</p><h2 id="Pseudo-code"><a href="#Pseudo-code" class="headerlink" title="Pseudo code"></a>Pseudo code</h2><p>下面是<strong>精简版t-SNE算法</strong>伪代码，非常简洁：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/07.png" alt=""></p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p><strong>添加微小的动量项可以减少到达最优解的迭代次数</strong></p><p><strong>精简版的t-SNE算法采用适应性学习率加速训练</strong>：在梯度较稳定的方向上增大学习率。<a href="#ref11">[11]</a></p><p>尽管精简版算法已经可以吊打其它非参数降维技术了，还是可以继续优化，文中提出了两个技巧：</p><p><strong>1. early compression</strong>：优化起始的时候将所有的映射点初始化在原点附近，有利于映射点移动、形成自然类。early compression通过给损失函数加上一个L2惩罚项实现。<br><strong>2. early exaggeration</strong>：在优化的初始阶段将所有的 $p_{ij}$ 扩大指定倍数加快收敛速度。</p><p>总结一下模型优化的参数配置：</p><ul><li>起始的50次迭代中将所有的 $p_{ij}$ 乘以4（这个步骤在精简版算法的伪代码中没有写出来）；</li><li>梯度下降的迭代轮数T设为1000；</li><li>动量项 $\alpha^{(t)}$ 当 $t&lt;250$ 时设为0.5，当 $t{\ge}250$ 时设为0.8；</li><li>学习率初始值设为100，每次迭代都将进行适应性更新 <a href="#ref11">[11]</a>。</li></ul><p><a href="http://ticc. uvt.nl/˜lvdrmaaten/tsne" target="_blank" rel="noopener">算法的Matlab实现</a></p><h1 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h1><p>作者分析了三个不足之处。</p><p><strong>1. 不能用于低维空间超过三维的情况</strong></p><p>因为t-SNE算法在映射空间利用了<strong>柯西分布</strong>的重尾特性解决拥挤问题，柯西分布是自由度为1的t分布，这种特性在二维空间中表现十分优异。但如果需要降到三维以上的映射空间，1自由度的t分布不能很好的保留局部特征，我们可能需要使用更多自由度的t分布。</p><p><strong>2. 本征维度诅咒</strong></p><p>t-SNE虽然能够保留全局特征，但是总体上还是基于局部特征进行的降维，这表示t-SNE对原始数据的 <strong>本征维度 (intrinsic dimentionality)</strong> 十分敏感，因为本征维度过高，我们就不能再把流形曲面的局部区域当欧几里得空间处理了，数据点间的局部特征更加复杂 <a href="#ref12">[12]</a>。不仅t-SNE，其它主流的基于局部特征提取的降维算法（如Isomap，LLe）都面临着这个诅咒。</p><p>作者提出了一种可行的办法：先用 <strong>自编码器 (autoencoder)</strong><a href="#ref13">[13]</a> 对数据进行压缩，这类模型可以大大降低原始数据的维度，同时最大保留高维数据的特征。经过编码的数据再进行t-SNE降维。</p><p><strong>3. 损失函数不凸~</strong></p><p>不幸的是当前主流降维算法使用的损失函数都是凸函数，而t-SNE优化的超参更多，这使得其损失函数是非凸的。这意味着，不同的超参取值、不同的初始化都可能收敛到不同的（局部最优）解。但是作者表示，如果固定这些超参，t-SNE就可以应用于不同的可视化任务，优化结果不会随着不同批次而发生变化。</p><p>t-SNE降维结果中点间的距离是没有实际意义的。原始的t-SNE训练很慢，后面有许多改进，比如 </p><ul><li>multiple maps of t-SNE</li><li>parametric t-SNE</li><li>… …</li></ul><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><span id="ref1">[1]</span> <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="noopener">Maaten, L. V. D., &amp; Hinton, G. (2008). <strong>Visualizing data using t-SNE</strong>. Journal of machine learning research, 9(Nov), 2579-2605.</a><br><span id="ref2">[2]</span> M.C. Ferreira de Oliveira and H. Levkowitz. <strong>From visual data exploration to visual data mining: A survey</strong>. IEEE Transactions on Visualization and Computer Graphics, 9(3):378–394, 2003.<br><span id="ref3">[3]</span> J.W. Sammon. <strong>A nonlinear mapping for data structure analysis</strong>. IEEE Transactions on Computers, 18(5):401–409, 1969.<br><span id="ref4">[4]</span> P. Demartines and J. Herault. <strong>´ Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets</strong>. IEEE Transactions on Neural Networks, 8(1):148–154, 1997<br><span id="ref5">[5]</span> G.E. Hinton and S.T. Roweis. <strong>Stochastic Neighbor Embedding</strong>. In Advances in Neural Information Processing Systems, volume 15, pages 833–840, Cambridge, MA, USA, 2002. The MIT Press.<br><span id="ref6">[6]</span> J.B. Tenenbaum, V. de Silva, and J.C. Langford. <strong>A global geometric framework for nonlinear dimensionality reduction</strong>. Science, 290(5500):2319–2323, 2000.<br><span id="ref7">[7]</span> K.Q. Weinberger, F. Sha, and L.K. Saul. <strong>Learning a kernel matrix for nonlinear dimensionality reduction</strong>. In Proceedings of the 21st International Confernence on Machine Learning, 2004.<br><span id="ref8">[8]</span> S.T. Roweis and L.K. Saul. <strong>Nonlinear dimensionality reduction by Locally Linear Embedding</strong>. Science, 290(5500):2323–2326, 2000.<br><span id="ref9">[9]</span> M. Belkin and P. Niyogi. <strong>Laplacian Eigenmaps and spectral techniques for embedding and clustering</strong>. In Advances in Neural Information Processing Systems, volume 14, pages 585–591, Cambridge, MA, USA, 2002. The MIT Press.<br><span id="ref10">[10]</span> J.A. Cook, I. Sutskever, A. Mnih, and G.E. Hinton. <strong>Visualizing similarity data with a mixture of maps</strong>. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, volume 2, pages 67–74, 2007.<br><span id="ref11">[11]</span> R.A. Jacobs. <strong>Increased rates of convergence through learning rate adaptation</strong>. Neural Networks, 1: 295–307, 1988.<br><span id="ref12">[12]</span> Y. Bengio. <strong>Learning deep architectures for AI</strong>. Technical Report 1312, Universite´ de Montreal, ´ 2007.<br><span id="ref13">[13]</span> G.E. Hinton and R.R. Salakhutdinov. <strong>Reducing the dimensionality of data with neural networks</strong>. Science, 313(5786):504–507, 2006.</p>]]></content>
    
    <summary type="html">
    
      &lt;div style=&quot;display: inline-block; width: 670px; height: 456px; overflow: hidden; border: 1px solid #ddd&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.gif&quot; style=&quot;margin-top: -114px !important; border: none&quot;&gt;&lt;br&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;t-SNE&lt;/strong&gt; (&lt;strong&gt;t-distributed Stochastic Neighbor Embedding&lt;/strong&gt;) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。&lt;/p&gt;
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="t-SNE" scheme="http://chenyin.top/tags/t-SNE/"/>
    
  </entry>
  
  <entry>
    <title>降维03 - SNE原理</title>
    <link href="http://chenyin.top/ml/20190325-80ae.html"/>
    <id>http://chenyin.top/ml/20190325-80ae.html</id>
    <published>2019-03-25T12:44:14.000Z</published>
    <updated>2019-04-14T16:02:20.120Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.png" width="100%"></p><blockquote><p><a href="http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf" target="_blank" rel="noopener">Hinton, G. E., &amp; Roweis, S. T. (2003). <strong>Stochastic neighbor embedding</strong>. In Advances in neural information processing systems (pp. 857-864).</a></p></blockquote><p><strong>随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE)</strong> 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 </p><a id="more"></a><h1 id="定义条件概率"><a href="#定义条件概率" class="headerlink" title="定义条件概率"></a>定义条件概率</h1><p>部分情况下，高维空间中两个点间的相似性可以用基于欧式距离的<strong>不相似度</strong> $d_{ij}$ 来衡量：<br>$$d_{ij}^2=\frac{||x_i-x_j||^2}{2\sigma_i^2}$$</p><p>SNE用<strong>条件概率</strong>来代替欧式距离度量两个高维数据点间的相似性。即：以点 $x_i$ 为中心，点 $x_i$ 选择 $x_j$ 作为自己邻居的概率记为 $p(x_j|x_i)$，定义<br>$$p_{ij}=p(x_j|x_i)=\frac{\exp(-d_{ij}^2)}{\sum_{k{\ne}i}{\exp(-d_{ik}^2)}} $$<br>注意这里对于 $p(x_i|x_i)$ 的理解有点怪异：它表示点 $x_i$ 选择自己作为邻居的概率，显然自己永远不可能是自己的邻居，所以 $p(x_i|x_i)=0$，而不是1。</p><h1 id="确定方差的确定"><a href="#确定方差的确定" class="headerlink" title="确定方差的确定"></a>确定方差的确定</h1><p>上式中的 $\sigma_i^2$ 是以 $x_i$ 为中心的高斯分布的<strong>方差</strong>：不同点周围的点的密度是不一样的，所以每个点的高斯分布对应的方差 $\sigma_i^2$ 也不相同，周围点密度大的中心点对应的方差应该较小。作者定义了<strong>困惑度 k (perplexity)</strong> ：手动指定的超参，代表某个点的有效邻居数，这个值对所有点都是常数。$\sigma_i^2$ 的取值将使得以点 $x_i$ 为中心选择其它所有点作为邻居的分布对应的<strong>熵</strong>等于 $\log{k}$，即<br>$$H(P_i)=-\sum_{j{\ne}i}{p(x_j|x_i)\log_2{p(x_j|x_i)}}=\log_2k$$<br>理论上可以通过上面的式子可以针对每个点 $x_i$ 解出对应的 $\sigma_i^2$。</p><p><strong>熵 $H(P_i)$ 的理解</strong></p><ul><li><strong>不确定性</strong>：熵本身就意味着不确定性，当区域点密集时，中心点位置的不确定性就小；</li><li><strong>能量</strong>：不确定性大意味着能量大，拉不住中心点，它要到处跑；</li><li><strong>有效邻居数</strong>：点密集时中心点的有效邻居就多。</li></ul><h1 id="映射到低维空间"><a href="#映射到低维空间" class="headerlink" title="映射到低维空间"></a>映射到低维空间</h1><p>在低维空间（二维或者三维）确定一点 $y_i$，它与高维空间的点 $x_i$ 对应，我们手动设置点 $y_i$ 的条件概率分布，即固定以 $y_i$ 为中心点的高斯分布对应的方差为 $\frac12$，当 $j{\ne}i$ 时：<br>$$ q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}} $$<br>当j=i时 $q(y_j|y_i)=0$ 。</p><p>此时，如果低维点 $y_i$ 能够正确表示高维点 $x_i$，意味着 $q(y_j|y_i)=p(x_j|x_i)$。为了使两个概率（近似）相等，我们可以最小化<strong>KL散度</strong>。损失函数如下：<br>$$C=\sum_iKL(P_i|Q_i)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$</p><ul><li>$P_i$ 表示：给定点 $x_i$，其它所有点的条件概率分布；</li><li>$Q_i$ 表示：给定低维空间映射点 $y_i$，其它所有低维映射点的条件概率分布。</li></ul><p>KL散度又叫<strong>相对熵</strong>，用来度量两个分布间的距离。假设P是真实分布，Q是模型分布，$KL(P|Q)$ 表示用Q表示P分布的数据所需的额外信息。   </p><p><strong>KL散度是不对称的</strong> </p><p>KL散度中包含 $\log\frac{p_{ij}}{q_{ij}}$ 意味着这个映射不是对称的，即：</p><ul><li>使用距离较小的低维点表示距离较大的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于小于0，则损失C较小；</li><li>使用距离较大的低维点表示距离较小的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于大于0，则损失C较大。</li></ul><p>这里就存在一个问题：当两个高维点距离很远，而我构造两个距离很近的低维点能够使损失函数更小，却与实际的目的不相符！所以，SNE算法使得高维空间中距离近的点在低维空间中距离仍然很近，但是远的点就嘿嘿嘿了。</p><h1 id="最小化损失函数"><a href="#最小化损失函数" class="headerlink" title="最小化损失函数"></a>最小化损失函数</h1><p>从 $q_{ij}$ 的定义式的分母部分可知，低维空间中点 $y_i$ 选择点 $y_j$ 的概率 $q_{ij}$ 与低维空间中的每一个映射点都有关系（分母起到了normalization的作用），但是求导结果却十分简洁：<br>$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$</p><p>想沿着所有点以最陡梯度下降是不现实的，不仅低效，还可能陷入糟糕费解的局部最优。上面的梯度公式右边理论上是针对所有低维映射点进行迭代，但是实际上，相距较远的一堆点间的影响十分小（抽象），在计算时往往可以忽略不计，也就是说：仅仅计算与中心点相距较近的一部分点，即<strong>邻居</strong> ，这也是为什么算法中含有单词neighbour的原因了吧。</p><p>选择中心点的部分较近的邻居参与计算表示，我们只保留了中心点附近区域的特性，而忽略了整体局势，所以说SNE是保留局部特征而非全局特征的算法。这个局部特性主要反应为 $\sigma_i^2$：局部点密集方差小，局部点稀疏方法大。方差确定的方法前面已经陈述了~。</p><h2 id="带动量项的梯度更新"><a href="#带动量项的梯度更新" class="headerlink" title="带动量项的梯度更新"></a>带动量项的梯度更新</h2><p>为了加速优化过程、避免很一般的局部解，可以在每次下降时添加动量项。<strong>动量 (momentum)</strong> 的作用就是在下降到局部最优时，小球仍然具有沿切线方向的分量，这使得小球将继续朝前运动，这会有两种结果：</p><ul><li>小球越过障碍，继续前行；</li><li>小球回退，返回局部最优解；<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/08.png" alt=""><br>具体的，更新公式如下：<br>$$\gamma^{(t)}=\gamma^{(t-1)}-{\eta}\frac{\partial{C}}{\partial{\gamma}}+\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$$</li></ul><p>式中 $\alpha(t)$ 即动量，动量项 ($\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$) 还与上一次运动幅度有关，直观的看，上一次运动越剧烈，下一次就越刹不住车。</p><h2 id="随机抖动"><a href="#随机抖动" class="headerlink" title="随机抖动"></a>随机抖动</h2><p><strong>随机抖动</strong> (random jitter) 是一种初始化技巧，即将低维空间中的所有数据点初始化在离坐标原点极近的地方。在迭代的过程中，它们将抖抖抖抖抖动直至收敛。尽管还是很慢，不过在节约时间和选择更优局部解时还是有明显提升的。</p><h2 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h2><p>在优化早期给每一步迭代加上高斯噪音，这可以帮助避免不好的局部最优解。随着迭代次数变多，噪音方差将逐渐减小。当方差变化非常慢时，这表明开始形成全局结构。（玄学）</p><p>然而，高斯噪音的数量和衰减速率是十分难以确定的，它们不仅与动量相关，也受学习率的影响。怎么办？多算几次！666</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.png&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hinton, G. E., &amp;amp; Roweis, S. T. (2003). &lt;strong&gt;Stochastic neighbor embedding&lt;/strong&gt;. In Advances in neural information processing systems (pp. 857-864).&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE)&lt;/strong&gt; 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 &lt;/p&gt;
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="非线性降维" scheme="http://chenyin.top/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4/"/>
    
      <category term="SNE" scheme="http://chenyin.top/tags/SNE/"/>
    
      <category term="t-SNE" scheme="http://chenyin.top/tags/t-SNE/"/>
    
  </entry>
  
  <entry>
    <title>降维02 - 主成分分析 (PCA)</title>
    <link href="http://chenyin.top/ml/20190325-c024.html"/>
    <id>http://chenyin.top/ml/20190325-c024.html</id>
    <published>2019-03-25T07:13:19.000Z</published>
    <updated>2019-04-14T16:02:20.109Z</updated>
    
    <content type="html"><![CDATA[<p>PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了<strong>线性代数</strong>之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。<a id="more"></a></p><p>PCA是最易于理解的<strong>特征提取</strong>过程：通过对原始特征的<strong>线性组合</strong>构造新的“特征”，这些特征不同于原始特征，但是又能与原始特征一样表达原始数据的信息。</p><p><strong>PCA (Primary Component Analysis, 主成分分析)</strong> 为什么叫做主成分分析呢？因为PCA构造出的新特征地位并不是等同的，即这些新特征的重要程度存在差异：</p><ul><li><strong>第一主成分 (the first component)</strong> 是新特征中最重要的特征，它在所有新特征中方差最大，这意味着它对数据变异的贡献是最大的；</li><li><strong>第二主成分 (the second component)</strong> 在保证不影响第一主成分的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异）；</li><li><strong>第三主成分 (the third component)</strong> 在不保证第一和第二主成分呢的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异 - 第二主成分引起的变异）;</li><li>依次类推……</li></ul><h1 id="线代原理"><a href="#线代原理" class="headerlink" title="线代原理"></a>线代原理</h1><p>预备知识：线性代数（矩阵运算、特征值&amp;特征向量、特征值分解）</p><h2 id="可对角化"><a href="#可对角化" class="headerlink" title="可对角化"></a>可对角化</h2><p>如果一个n阶方阵A相似于对角矩阵，即存在可逆矩阵$P$使得$P^{-1}AP$是对角矩阵，则称方阵A是<strong>可对角化</strong>的。</p><p><strong>n阶方阵A可对角化的充要条件是A每个特征值的几何重数与代数重数相等</strong>：<strong>代数重数</strong>指<strong>特征多项式</strong>中该特征值的幂次，<strong>几何重数</strong>指特征值对应的线性无关的特征向量的个数。</p><p><strong>n阶方阵A可对角化的充要条件是A有n个线性无关的特征向量</strong>：几何重数与代数重数相等意味着n个线性无关的特征向量。</p><p>即使方阵A可逆也不能保证每个特征值的代数重数与几何重数相等，因此A可逆不是A可对角化的充要条件！</p><h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>如果矩阵A是一个<strong>可对角化</strong>的方阵，它就可以进行特征值分解，即A可表示为：<br>$$A=Q{\Lambda}Q^{-1}$$<br>其中</p><ul><li>$Q$ 是n阶方阵，它的n个列向量是方阵A的n个特征向量</li><li>$\Lambda$ 是对角方阵，对角线元素是方阵A的特征值，其位置与 $Q$ 中的特征向量位置相对应</li></ul><p>特征值分解的应用？求逆。<br>如果方阵A是<strong>非奇异矩阵</strong>（即可以进行特征值分解且特征值不含0），则 $A^{-1}=Q{\Lambda}^{-1}Q^{-1}$，其中 $[{\Lambda}^{-1}]_{ii}=\frac1{\lambda_i}$。</p><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>特征值分解对A的要求格外严格：可逆、特征值不含0、方阵……<br>放松特征值分解的限制，将A扩展到任意 $m{\times}n$ 的矩阵即得到 <strong>奇异值分解 (Singular Value Decomposition, SVD)</strong> 。</p><p>假设M是定义在<strong>实数域</strong>或者<strong>复数域</strong>上的 $m{\times}n$ 阶的矩阵：<br>$$M=U{\Sigma}V^\ast$$<br>其中</p><ul><li>U是 $m{\times}m$ 阶<strong>酉矩阵</strong>：U的m个列向量实际上是 $M^{\ast}M$ 的特征向量，称为M的<strong>左奇异向量</strong>。</li><li>$\Sigma$ 是 $m{\times}n$ 阶<strong>非负实数对角矩阵</strong>：对角线元素称为M的<strong>奇异值</strong>，一般情况下奇异值按<strong>从大到小</strong>的顺序排列！</li><li>$V^\ast$ 是 $V$ 的<strong>共轭转置</strong>，是 $n{\times}n$ 阶<strong>酉矩阵</strong>：V的n个列向量实际上是 $MM^\ast$ 的特征向量，称为M的<strong>右奇异向量</strong>。</li></ul><blockquote><p><strong>共轭转置</strong>：共轭转置与转置是两个概念，当矩阵定义在实数域上时二者结果相同，矩阵A的共轭转置记为 $A^\ast$，定义如下：<br>$$A^\ast=(\overline{A})^T=\overline{A^T}$$<br>其中，$\overline{A}$ 表示对A的元素<strong>复共轭</strong>，当A定义在实数域时 $\overline{A}=A$。</p></blockquote><p>当矩阵M定义在实数域时有：<br>$$M=U{\Sigma}V^T$$<br>我们在应用SVD时一般都是定义在实数域上的哟~</p><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>上面提到了对于任意 $m{\times}n$ 阶矩阵M的SVD分解：<br>$$M_{m{\times}n}=U_{m{\times}m}{\Sigma_{m{\times}n}}V_{n{\times}n}^T$$<br>直观图如下（这里假设样本数量m多于特征数量n，这意味着M有n个奇异值）：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/19.png" alt=""><br>其中 $\Sigma$ 矩阵很有意思，当m&gt;n时，矩阵 $\Sigma_{m{\times}n}$ 中只有子矩阵 $\Sigma_{n{\times}n}$ 的对角线上的值不为0，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/20.png" alt=""></p><p>以scRNA测序为例：假设在表达谱矩阵中，一行表示一个细胞中不同基因的表达量，一列表示一个基因在不同细胞中的表达量。这与我们的习惯（一列表示一个细胞，一行表示一个基因）有所不同！</p><p>对应到上述SVD分解式我们发现，n表示细胞数量，m表示基因数量。我们降维的结果肯定是要保证细胞总数m不变，而将基因数目从n减小到k。</p><p>具体的，取 $\Sigma$ 中最大的k个奇异值，即取 $\Sigma_{k{\times}k}$ 子矩阵，相应的取U的前k列和V的前k列（即$V^\ast$的前k行），即：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/21.png" alt=""><br>此时<br>$$M_{m{\times}n}=U_{m{\times}k}{\Sigma_{k{\times}k}}V_{k{\times}n}^T$$<br>上式中的 $U_{m{\times}k}$ 就是 $M_{m{\times}n}$ 从n维特征空间降到k维特征空间的结果。注意矩阵 $U_{m{\times}k}$ 的k个列向量并不在矩阵  $M_{m{\times}n}$ 中，而是M中的n个列向量线性组合的结果。</p><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了&lt;strong&gt;线性代数&lt;/strong&gt;之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="PCA" scheme="http://chenyin.top/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>降维01 - 特征选择和特征提取</title>
    <link href="http://chenyin.top/ml/20190325-d2ce.html"/>
    <id>http://chenyin.top/ml/20190325-d2ce.html</id>
    <published>2019-03-25T07:13:10.000Z</published>
    <updated>2019-04-14T16:02:20.106Z</updated>
    
    <content type="html"><![CDATA[<p>大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的<strong>降维</strong>浓缩技术应运而生。<a id="more"></a></p><p>降维的好处有哪些？</p><ol><li>减少数据维度，存储数据需要的空间也会减少（盘霸可忽略~）；</li><li>低维数据可以减少计算量，缩短模型训练时间；</li><li>很多算法在高维数据上的表现远远没有在低维数据上好；</li><li>去掉冗余特征（强相关特征），提高数据的质量；</li><li>有助于可视化，我们只能形象观察三维及以下的数据！</li></ol><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>降维总是围绕着减少特征数进行的，根据对特征的操作可分为：</p><ul><li><strong>特征选择</strong>：保留原始特征集的子集，即选取部分原始特征；</li><li><strong>特征提取</strong>：构造不同于原始特征的新特征，新特征往往是原始特征的组合，替代原始特征表达原始数据想表达的信息。<br>特征提取是降维算法研究的核心内容。</li></ul><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择只是对每个特征进行评估，去掉不重要的或者选出重要的：</p><ol><li>缺失值比率：按缺失值比率删除特征；</li><li>低方差滤波：删除方差小的特征；</li><li>高相关滤波：只保留高相关特征中的一个；</li><li>随机森林：计算每个特征的重要性；</li><li>前向特征选择：依次增加特征数检验模型性能；</li><li>反向特征消除：依次减少特征数检验模型性能。</li></ol><h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>特征提取才是降维思想的核心内容，降维算法家族枝繁叶茂，先做一个总体分类：<img src="" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的&lt;strong&gt;降维&lt;/strong&gt;浓缩技术应运而生。
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>批次效应校正与RUV算法</title>
    <link href="http://chenyin.top/bioinfo/20190322-afa7.html"/>
    <id>http://chenyin.top/bioinfo/20190322-afa7.html</id>
    <published>2019-03-22T01:37:34.000Z</published>
    <updated>2019-04-23T07:22:45.327Z</updated>
    
    <content type="html"><![CDATA[<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p> Gagnon-Bartsch et al.提出了RUV-2用来标准化<strong>连续的</strong>微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化<strong>离散的</strong>RNA测序数据。</p><p> 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建<strong>泛化线性模型</strong> (Generalized Linear Model, GLM):<br>$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$<br><a id="more"></a><br>参数意义如下：</p><ul><li>$Y$ 是 $n{\times}J$ 的表达矩阵；</li><li>$W$ 是 $n{\times}k$ 与<strong>不需要的变异</strong>相关的<strong>多余变异相关矩阵</strong>（k是不需要的变异相关的变量的个数），$\alpha$ 是 $k{\times}J$ 的多余变异相关矩阵的系数（参数）；</li><li>$X$ 是 $n{\times}p$ 与<strong>感兴趣的变异</strong>相关的<strong>期望变异相关矩阵</strong>（p是感兴趣的变异相关的变量的个数），$\beta$ 是 $p{\times}J$ 的期望变异相关矩阵的系数（参数）；</li><li>$O$ 是一个 $n{\times}J$ 的矩阵，它可以置零，也可以包含其它的标准化过程（如<a href="https://en.wikipedia.org/wiki/Quantile_normalization" target="_blank" rel="noopener">UQ标准化</a>）。</li><li>矩阵 $X$ 是一个随机变量，是我们实验的测量值，是已知的（先验）。</li><li>矩阵 $W$ 是未观测的随机变量；$\alpha$、$\beta$、$k$ 都是未知参数。</li></ul><p>不同于先前的标准化方法，RUV可以使用GLM标准化技术同时标准化reads计数（$W\alpha$）和推断差异表达（$X\beta$）。标准化的计数也可以通过由原始计数对不需要的因子进行回归分析后求残差得到，但是直接从原始计数中移除不需要的因子（$W\alpha$）可能会损失掉 $X$ 的一部分。[<a href="">reference</a>]</p><p>同时估计 $W$, $\alpha$, $\beta$ 和 $k$ 是很难的。对于一个给定的 $k$ 值，我们尝试着用下面三种方法对$W$ 进行估计：</p><h2 id="1-基于阴性对照基因的RUVg"><a href="#1-基于阴性对照基因的RUVg" class="headerlink" title="1. 基于阴性对照基因的RUVg"></a>1. 基于阴性对照基因的RUVg</h2><ol><li>假设我们鉴定出了一个阴性对照基因 (negative control genes) 的集合（大小为 $J_c$），例如不差异表达的基因，对这个基因集合来说 $\beta_c=0$ 即 $\log{E[Y_c|W,X,O]}=W\alpha_c+O_c$，公式中的下标c将矩阵限制在了大小为 $J_c$ 的基因集合里。</li><li>定义 $Z=\log{Y}-O$，$Z^\ast$ 是 $Z$ 列向量中心化（$Z$ 的各个列向量均值都为0）的结果。</li><li>对 $Z_c^\ast$ 进行奇异值分解 (singular value decomposition, SVD) 即 $Z_c^\ast=U{\Lambda}V^T$。矩阵 $U$ 是 $n{\times}n$ 列正交矩阵，它的列向量是 $Z^\ast$ 的左奇异向量集；矩阵 $V$ 是 $J_c{\times}J_c$ 的列正交矩阵，它的列向量是 $Z^\ast$ 的右奇异向量集；$\Lambda$ 矩阵是由 $Z^\ast$ 的奇异值组成的非方形对角矩阵，大小为 $n{\times}J_c$。$Z^\ast$ 最少有 $\min{(n,J_c)}$ 个奇异值。对于一个给定的 $k$，通过 $\widehat{W\alpha_c}=U\Lambda_kV^T$ 估计 $W\alpha_c$，通过 $\hat{W}=U\Lambda_k$ 估计 $W$。$|lambda_k$ 是由 $\Lambda$ 导出的大小为 $n{\times}J_c$ 的非方形对角矩阵，保留 $\Lambda$ 中最大的 $k$ 个奇异值，将其它的奇异值置为0。</li><li>将 $\hat{W}$ 带入上面基于 $J$ 个基因构建的公式中，通过GLM回归估计 $\alpha$ 和 $\beta$。</li><li>（可选）将标准化的读段计数定义为 $Z$ 对 $\hat{W}$ 的普通最小二乘回归 (ordinary least squares, OLS) 的残差。 </li></ol><p>这是最基础的RUV-2的离散版本。其中的关键假设是我们能够找到这个阴性对照基因集合。然而，RUV-2已被证实对对照基因的选择十分敏感。我们因此考虑下面的两种方法：RUVr不需要阴性对照基因，RUVs对阴性对照基因选择的鲁棒性更强。</p><h2 id="2-基于残差的RUVr"><a href="#2-基于残差的RUVr" class="headerlink" title="2. 基于残差的RUVr"></a>2. 基于残差的RUVr</h2><ol><li>计算残差矩阵 $E(n{\times}J)$: 计数矩阵 $Y(n{\times}J)$ 关于感兴趣的协变量矩阵 $X(n{\times}J)$ 的初步GLM回归，例如异常值残差。这里用于回归计算的计数矩阵可以是未标准化的原始数据，也可以是经过其它标准化工具（例如UQ）处理过的数据。</li><li>对残差进行奇异值分解，即 $E=U{\Lambda}V^T$，通过 $\hat{W}=U\Lambda_k$ 估计 $w$。接下来的步骤与 <code>RUVg</code> 的第4、5步相同。</li></ol><h2 id="3-基于重复-阴性对照样本的RUVs"><a href="#3-基于重复-阴性对照样本的RUVs" class="headerlink" title="3. 基于重复/阴性对照样本的RUVs"></a>3. 基于重复/阴性对照样本的RUVs</h2><ol><li>假设在多个复制样本中具有生物学特征的（我们感兴趣的）某些协变量的表达量可看作恒定的，它们的计数差异与<code>RUVg</code>中的阴性对照基因一样，对我们后续的研究没有影响。现在假设有 $R$ 个复制组，$r(i){\in}{1,…,R}$ 表示样本 $i$ 所属的复制组；如果样本 $i$ 不属于任何一个复制组，则 $r(i)=0$。例如，对于SEQC数据集，样本A和样本B各自的64个<strong>复制本</strong>（$=4[\text{libraries}]{\times}2[\text{flow-cell}]{\times}8[\text{lanes}]$）分别组成了一个<strong>复制组</strong>。</li><li>对每一个复制本对应的计数矩阵进行<strong>列中心化</strong>处理，即矩阵各个列向量的均值都为0。去掉不属于预期复制组的样本，即筛选出 $n_d=\sum_i{I(r(i)\ne0)}$ 个样本对应的列中心化后的计数子矩阵 $Y_d(n_d{\times}J)$。 此时 $\log{E[Y_d|W,X,O]}=W_d\alpha+O_d$，对应的矩阵大小是 $(n_d{\times}J){\leftarrow}({n_d\times}k)({k\times}J)+(n_d{\times}J)$。</li><li>定义 $Z_d=\log{Y_d}-O_d$，$Z_d^\ast$ 是 $Z_d$ 列中心化的结果，$Z_d^\ast=U{\Lambda}V^T$。通过 $\hat{\alpha}=\Lambda_kV^T$（保留最大的 $k$ 个奇异值，$k{\le}\min{(n_d,J)}$）来估计 $\alpha$。</li><li>在所有 $n$ 个原始数据和 $J_c$ 个阴性对照基因上对 $Z_c$ 进行最小二乘回归（OLS）。估计讨厌因子 $W$：$\hat{W}=Z_c\hat\alpha_c^T(\hat\alpha_c\hat\alpha_c^T)^{-1}$。接下来的步骤与 <code>RUVg</code>的第4、5步相同。</li></ol><hr><h1 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h1><h2 id="两个数据集"><a href="#两个数据集" class="headerlink" title="两个数据集"></a>两个数据集</h2><ol><li><strong>SEQC data set</strong>: The third phase of the MicroArray Quality Control (MAQC) project, also known as the Sequencing Quality Control17 (SEQC) project, aims to assess the technical performance of high-throughput sequencing platforms by generating benchmarking data sets.</li><li><strong>Zebrafish (斑马鱼) data set</strong>: All procedures were conducted in compliance with US federal guidelines in an AAALAC-accredited facility and were approved by the UC Berkeley Office of Animal Care and Use. </li></ol><h2 id="两种讨厌因子"><a href="#两种讨厌因子" class="headerlink" title="两种讨厌因子"></a>两种讨厌因子</h2><p>本文分析了两种讨厌因子：<strong>library preparation</strong> &amp; <strong>flow-cell effects</strong>。</p><p><strong>flowcell</strong>：流动室，别称鞘流池、流动池，是流式细胞技术的基础关键部件。大概长这个样子：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/09.jpg" alt=""></p><p>作者用<strong>正交的主成分图</strong>展示了这两种讨厌因子：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/10.jpg" alt=""><br><span style="font-size:16px;color:gray">Scatterplot matrix of first three principal components (PC) for unnormalized counts (log scale, centered). The principal components are orthogonal linear combinations of the original 21,559-dimensional gene expression profiles, with successively maximal variance across the 128 samples, that is, the first principal component is the weighted average of the 21,559 gene expression measures that provides the most separation between the 128 samples. Each point corresponds to one of the 128 samples. The four sample A and the four sample B libraries are represented by different shades of blue and red, respectively (16 replicates per library). Circles and triangles represent samples sequenced in the first and second flow-cells, respectively. As expected for the SEQC data set, the first principal component is driven by the extreme biological difference between sample A and sample B. The second and third principal components clearly show library preparation effects (the samples cluster by shade) and, to a lesser extent, flow-cell effects reflecting differences in sequencing depths (within each shade, the samples cluster by shape).</span></p><h2 id="算法横向对比"><a href="#算法横向对比" class="headerlink" title="算法横向对比"></a>算法横向对比</h2><p><strong>上分位数标准化</strong> (Upper-quartile normalization, UQ)，UQ只能消除流细胞效应而对文库效应束手无策，RUV算法解决的就是如何消除不同文库的影响。</p><p><strong>局部加权回归散点平滑法</strong> (Locally Weighted Scatterplot Smoothing, LOWESS/LOESS)不能消除文库效应。</p><hr><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="ERCC-spike-in-controls"><a href="#ERCC-spike-in-controls" class="headerlink" title="ERCC spike-in controls"></a>ERCC spike-in controls</h2><p>ERCC 即 External RNA Controls Consortium，是斯坦福大学为了定制一套spike-in RNA而成立的专门性组织，主要的工作是设计了好用的spike-in RNA，方便microarray以及RNA-Seq进行内参定量。[<a href="https://jimb.stanford.edu/ercc/" target="_blank" rel="noopener">官方首页</a>]</p><p>RNA spike-in是一种数量和序列都已知的RNA转录本，用于校准RNA杂交实验（例如DNA微阵列实验、RT-qPCR、RNA测序等）的测量值。RNA spike-in作为对照组（控制组）探针，被设计成能与具有相应匹配序列的DNA分子结合，这个特异性结合的过程我们称之为<strong>杂交</strong>。在制备的过程中，已知数量的spike-in将与实验样本进行混合。spike-ins的杂交程度可以用来标准化样本RNA的测量值。[<a href="https://en.wikipedia.org/wiki/RNA_spike-in" target="_blank" rel="noopener">wiki</a>] [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1797020" target="_blank" rel="noopener">reference</a>]</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h1&gt;&lt;p&gt; Gagnon-Bartsch et al.提出了RUV-2用来标准化&lt;strong&gt;连续的&lt;/strong&gt;微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化&lt;strong&gt;离散的&lt;/strong&gt;RNA测序数据。&lt;/p&gt;
&lt;p&gt; 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建&lt;strong&gt;泛化线性模型&lt;/strong&gt; (Generalized Linear Model, GLM):&lt;br&gt;$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$&lt;br&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>批次效应（batch effect）</title>
    <link href="http://chenyin.top/bioinfo/20190319-cca5.html"/>
    <id>http://chenyin.top/bioinfo/20190319-cca5.html</id>
    <published>2019-03-19T06:38:32.000Z</published>
    <updated>2019-04-20T09:04:21.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a><span id="def">一、定义</span></h1><p>下面是大佬给出来的关于<strong>批次效应</strong>(batch effect)的定义：</p><blockquote><p>Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. <a href="https://www.nature.com/articles/nrg2825" target="_blank" rel="noopener">Leek et. al (2010)</a></p></blockquote><a id="more"></a><p>批次效应是测量结果中的一部分，它们因为实验条件的不同而具有不同的表现形式，并且与我们研究的变量没有半毛钱关系。一般批次效应可能在下述情形中出现：</p><ul><li>一个实验的不同部分在<strong>不同时间</strong>完成；</li><li>一个实验的不同部分由<strong>不同的人</strong>完成；</li><li>试剂用量不同、芯片不同、实验仪器不同；</li><li>将自己测的数据与从网上下载的数据混合使用；</li><li>……</li></ul><hr><h1 id="二、检测"><a href="#二、检测" class="headerlink" title="二、检测"></a>二、检测</h1><p>批次效应相关协变量已知时，直接聚类观察结果是否和相应协变量相关。<br>混合数据因为实验条件迥异，一般批次效应都很大。</p><p>以R为例，通过聚类检验是否存在批次效应。请先查看下面的<a href="#dataset">示例数据集</a>。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t() 转置函数</span></span><br><span class="line"><span class="comment"># dist() 距离函数：按照指定规则求行向量间的距离，因此要转置</span></span><br><span class="line">&gt; dist_mat &lt;- dist(t(edata))</span><br><span class="line">&gt; clustering &lt;- hclust(dist_mat) <span class="comment"># hclust 的输入结构与 dist 相同！</span></span><br><span class="line"><span class="comment"># 按照批次信息聚类</span></span><br><span class="line">&gt; plot(clustering, labels = pheno$batch)</span><br><span class="line"><span class="comment"># 按照是否是正常细胞聚类</span></span><br><span class="line">&gt; plot(clustering, labels = pheno$cancer)</span><br></pre></td></tr></table></figure></p><p>聚类结果如下：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/15.png" width="50%"><br>左边的红色框框是正常细胞中混入的癌细胞，右边蓝色框框中是癌细胞中混入的正常细胞。</p><p>还有许多检验批次效应的的方法，这篇<a href="https://www.itl.nist.gov/div898/handbook/eda/section4/eda42a3.htm" target="_blank" rel="noopener">文章</a>给出了多种检验方式：</p><ul><li>图分析：双柱状图、QQ图、箱线图、块图、…</li><li>定量分析：F检验、双样本t检验、…</li></ul><hr><h1 id="三、处理"><a href="#三、处理" class="headerlink" title="三、处理"></a>三、处理</h1><p>实验条件允许的条件下，应该优化实验设计，将引起批次效应的协变量采样<strong>分散</strong>开来。例如，对于时间批次效应，实验的不同部分应该在各个时间内均匀采样。这叫“治病就治本”。</p><p>但是大多数情况下实验条件不允许，如果够幸运的话批次效应相关的协变量已经被记录下来了，此时对批次效应进行验证，然后使用统计模型过滤；如果十分不幸，批次效应相关的协变量没有被记录或者不明显，我们就需要借助相关工具猜一下哪个变量可能造成了批次效应，然后使用统计模型过滤。前者叫<strong>参数化方法</strong>，后者叫<strong>非参数化方法</strong>。</p><h2 id="1-导入示例数据集"><a href="#1-导入示例数据集" class="headerlink" title="1.导入示例数据集"></a><span id="dataset">1.导入示例数据集</span></h2><h3 id="bladderbatch包"><a href="#bladderbatch包" class="headerlink" title="bladderbatch包"></a>bladderbatch包</h3><p><code>bladderbatch</code>包包含了一项<a href="https://www.ncbi.nlm.nih.gov/pubmed/15173019" target="_blank" rel="noopener">膀胱癌研究</a>中相关的57个样本的基因表达数据，这些数据已经使用RMA标准化，并且已经按照<a href="https://www.ncbi.nlm.nih.gov/pubmed/20838408" target="_blank" rel="noopener">相关协议</a>进行了预处理。</p><p>另外阅读R文档我们发现：</p><ul><li><code>eSet</code>是一个包含高通量实验元数据的一个类，它不能直接被实例化。</li><li><code>pData</code>方法在类<code>eSet</code>中被定义，它的作用是访问数据的元数据（注释信息）。</li><li><code>ExpressionSet</code>继承自<code>eSet</code>，同样是一个高通量测序数据的容器，由&gt; * <code>biobase</code>包引入，封装了<strong>表达矩阵</strong>和<strong>样本分组信息</strong>。表达矩阵存储在<code>exprs</code>中。</li></ul><p><code>bladderbatch</code>数据集是（类似）<code>ExpressionSet</code>类型，我们可以使用<code>pData()</code>加载元数据，使用<code>exprs()</code>加载表达谱数据。<br><code>bladderbatch</code>数据集用来演示如何校正批次效应。</p><h3 id="下载和加载数据集"><a href="#下载和加载数据集" class="headerlink" title="下载和加载数据集"></a>下载和加载数据集</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 1.安装并加载数据集</span></span><br><span class="line">&gt; BiocInstaller::biocLite(<span class="string">"bladderbatch"</span>)</span><br><span class="line">&gt; <span class="keyword">library</span>(bladderbatch) <span class="comment"># 或者 library("bladderbatch", character.only=TRUE)</span></span><br><span class="line"><span class="comment">## 2.查看当前可用数据集</span></span><br><span class="line">&gt; data()</span><br><span class="line"><span class="comment">## 3.检查是否有如下信息</span></span><br><span class="line">Data sets <span class="keyword">in</span> package ‘bladderbatch’:</span><br><span class="line">bladderEset (bladderdata)           Bladder Gene Expression Data Illustrating Batch Effects</span><br><span class="line"><span class="comment">## 加载数据集</span></span><br><span class="line">&gt; data(bladderdata) <span class="comment"># 实际加载进来的数据集名字叫做 bladderEset !</span></span><br><span class="line">&gt; pheno &lt;- pData(bladderEset) <span class="comment"># 使用 pData 加载元数据/注释信息</span></span><br><span class="line">&gt; edata &lt;- exprs(bladderEset) <span class="comment"># 使用 exprs 加载数据</span></span><br></pre></td></tr></table></figure><p><code>pheno</code>如下所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/16.png" width="20%"><br>样本的批次信息存储作为元数据存储在<code>pheno$batch</code>中（R中使用<code>$</code>访问对象的属性）。</p><p><code>edata</code>如下所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/17.png" width="50%"><br>一列表示一个样本（细胞），后面求距离需要转置。</p><hr><h2 id="2-R中的sva包"><a href="#2-R中的sva包" class="headerlink" title="2.R中的sva包"></a>2.R中的sva包</h2><p><code>sva</code>用于移除高通量测序数据中的<a href="#def"><strong>批次效应</strong></a>以及其它无关变量的影响。</p><p><code>sva</code>包含用于标识和构建高维数据集（例如基因表达、RNA测序/甲基化/脑成像数据等可以直接进行后续分析的数据）<strong>代理变量</strong>的函数。代理变量是直接从高维数据构建的协变量，可以在后续分析中用于调整未知的、未建模的或潜在的噪音源。</p><blockquote><p><strong>代理变量（surrogate/proxy variable）</strong>: A variable that can be measured (or is easy to measure) that is used in place of one that cannot be measured (or is difficult to measure). For example, whereas it may be difficult to assess the wealth of a household, it is relatively easy to assess the value of a house. See also proxy variable. (from <a href="http://www.oxfordreference.com/view/10.1093/oi/authority.20110803100544210" target="_blank" rel="noopener">Oxford Reference</a>)<br><strong>代理变量分析（Surrogate Variable Analysis）</strong>：<a href="https://digital.lib.washington.edu/researchworks/handle/1773/9586" target="_blank" rel="noopener">Click here</a></p></blockquote><p><code>sva</code>从三个方面消除人为设计造成的影响：</p><ol><li>为未知变异源构造代理变量；(Leek and Storey <a href="https://www.ncbi.nlm.nih.gov/pubmed/17907809" target="_blank" rel="noopener">2007 PLoS Genetics</a>, <a href="https://www.ncbi.nlm.nih.gov/pubmed/20941797" target="_blank" rel="noopener">2011 Pharm Stat.</a>)</li><li>使用ComBat直接移除已知的批次效应；<a href="https://academic.oup.com/biostatistics/article/8/1/118/252073" target="_blank" rel="noopener">(Johnson et al. 2007 Biostatistics)</a></li><li>使用已知的控制探针(known control probes)移除批次效应；<a href="https://www.biorxiv.org/content/10.1101/006585v2" target="_blank" rel="noopener">(Leek 2014 biorXiv)</a><br>移除批次效应和使用代理变量可以减少依赖性，稳定错误率估计值，提高重现性。</li></ol><p>查看<code>sva</code><a href="http://127.0.0.1:28090/library/sva/doc/sva.pdf" target="_blank" rel="noopener">在线文档</a>。</p><h3 id="gt-已记录批次信息"><a href="#gt-已记录批次信息" class="headerlink" title="&gt; 已记录批次信息"></a>&gt; 已记录批次信息</h3><p>当<strong>批次协变量</strong>已知时（即每个样本分属于哪一个批次记录在数据集的元数据中），可以使用<code>sva</code>的<code>ComBat</code>校正<strong>批次效应</strong>。<br><code>ComBat</code>使用参数（parametric）或者非参数（non-parametric）的<strong>经验贝叶斯框架</strong>（Empirical Bayes Frameworks）进行批次效应的校正。</p><p>先看<code>ComBat</code>的用法：摘自<a href="https://www.rdocumentation.org/packages/sva/versions/3.20.0/topics/ComBat" target="_blank" rel="noopener">官方文档</a><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; ComBat(dat, batch, mod=<span class="literal">NULL</span>, par.prior = <span class="literal">TRUE</span>, prior.plots = <span class="literal">FALSE</span>)</span><br><span class="line"><span class="comment"># dat: 基因组测量矩阵（探针维度 X 样本数），探针维度例如marker数、基因数.....，例如表达谱矩阵</span></span><br><span class="line"><span class="comment"># batch: 批次协变量，只能传入一个批次协变量！</span></span><br><span class="line"><span class="comment"># mod: 这是一个模式矩阵，里面包含了我们感兴趣的变量！</span></span><br><span class="line"><span class="comment"># par.prior: 基于参数/非参数，默认为基于参数</span></span><br></pre></td></tr></table></figure></p><p>有了背景知识我们就可以进行膀胱癌数据的批次校正：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; pheno$hasCancer &lt;- pheno$cancer == <span class="string">"Cancer"</span></span><br><span class="line"><span class="comment"># 或者 &gt; pheno$hasCancer &lt;- as.numeric(pheno$cancer == "Cancer")</span></span><br><span class="line">&gt; model &lt;- model.matrix(~hasCancer, data=pheno)</span><br><span class="line">&gt; combat_edata &lt;- ComBat(dat = edata, batch = pheno$batch, mod = model)</span><br><span class="line"><span class="comment"># 这里的 mod 参数就比较有意思了，它记录的是我们感兴趣的变量。因为初次接触R只能肤浅理解一下。</span></span><br><span class="line"><span class="comment"># 它应该是一个我们期望样本能被正确聚类所依据的协变量，它总是数值型变量</span></span><br></pre></td></tr></table></figure></p><p><code>model.matrix(...)</code>的详细解释见<a href="/R/20190319-1548.html">这里</a>。</p><p>画图：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; dist_mat_combat &lt;- dist(t(combat_edata))</span><br><span class="line">&gt; clustering_combat &lt;- hclust(dist_mat_combat, method = <span class="string">"complete"</span>)</span><br><span class="line">&gt; plot(clustering, labels = pheno$batch)</span><br><span class="line">&gt; plot(clustering, labels = pheno$cancer))</span><br></pre></td></tr></table></figure></p><p>我们发现批次效应被移除了：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/18.png" width="50%"></p><h3 id="gt-没有记录批次信息"><a href="#gt-没有记录批次信息" class="headerlink" title="&gt; 没有记录批次信息"></a>&gt; 没有记录批次信息</h3><p><a href="http://master.bioconductor.org/packages/release/workflows/vignettes/rnaseqGene/inst/doc/rnaseqGene.html#removing-hidden-batch-effects" target="_blank" rel="noopener">看这里</a></p><hr><h2 id="3-R中的ber包"><a href="#3-R中的ber包" class="headerlink" title="3.R中的ber包"></a>3.R中的ber包</h2><p>ber的全称就是batch effects removal，使用<code>&gt; install.packages(&quot;ber&quot;)</code>安装ber包，查看<a href="https://cran.r-project.org/web/packages/ber/ber.pdf" target="_blank" rel="noopener">用户手册</a>。</p><p>这个包里有6个函数，它们的作用就是校正<strong>微阵列标准数据</strong>中的批次效应。标准数据指的是：输入矩阵每一行代表独立的样本，每一列代表基因；批次信息作为已知的<strong>分类变量</strong>；期望变量可以大大提高批次效应校正的效率。</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>berr(Y, b, covariates = NULL)</code></td><td>using a two-stage regression approach <a href="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.pdf" target="_blank" rel="noopener">(M. Giordan. February 2013)</a></td></tr><tr><td><code>ber_bg(Y, b, covariates = NULL,partial=TRUE,nSim=150)</code></td><td>using a two-stage regression approach and bagging <a href="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.pdf" target="_blank" rel="noopener">(M. Giordan. February 2013)</a></td></tr><tr><td><code>combat_p(Y, b, covariates = NULL, prior.plots=T)</code></td><td>using a parametric empirical Bayes approach <a href="https://www.ncbi.nlm.nih.gov/pubmed/16632515" target="_blank" rel="noopener">(n Johnson et al. 2007)</a></td></tr><tr><td><code>combat_np(Y, b, covariates = NULL)</code></td><td>using a non-parametric empirical Bayes approach <a href="https://www.ncbi.nlm.nih.gov/pubmed/16632515" target="_blank" rel="noopener">(n Johnson et al. 2007)</a></td></tr><tr><td><code>mean_centering(Y, b)</code></td><td>using the means of the batches</td></tr><tr><td><code>standardization(Y, b)</code></td><td>using the means and the standard deviations of the batches</td></tr></tbody></table><p>上表中的：</p><ul><li><code>Y</code>是输入矩阵（样本数 $n{\times}g$ 探针数）</li><li><code>b</code>是 $n$ 维<strong>分类1向量</strong>，每个分量对应着每个样本的批次信息</li><li><code>covariates</code>是一个 $n$ 行的<code>data.frame</code>实例</li></ul><p>上面的6个函数都需要指定<code>b</code>，所以它们都是用来处理<strong>批次信息被记录</strong>的情形的，对于启发性的校正貌似没提出解决方案。</p><hr><h2 id="4-R中的RUVSeq包"><a href="#4-R中的RUVSeq包" class="headerlink" title="4.R中的RUVSeq包"></a>4.R中的RUVSeq包</h2><p>RUVSeq means <em>Remove Unwanted Variation from RNA-Seq Data</em>, which shows us how to conduct a differential expression (DE) analysis that controls for “unwanted variation”, e.g., batch, library preparation, and other nuisance effects, using the between-sample normalization methods proposed in <a href="https://www.nature.com/articles/nbt.2931" target="_blank" rel="noopener">Risso et al. (2014)</a>.</p><p>RUV算法基本原理参考<a href="/Bioinformatics/20190322-afa7.html">这里</a>，原文在<a href="https://www.nature.com/articles/nbt.2931" target="_blank" rel="noopener">这里</a>。</p><hr><h2 id="5-R中的BatchQC包"><a href="#5-R中的BatchQC包" class="headerlink" title="5. R中的BatchQC包"></a>5. R中的BatchQC包</h2><p><a href="https://bioconductor.org/packages/release/bioc/html/BatchQC.html" target="_blank" rel="noopener">BatchQC工具</a></p><hr><h1 id="四、FAQ"><a href="#四、FAQ" class="headerlink" title="四、FAQ"></a>四、FAQ</h1><ol><li><strong>标准化（normalization）可以消除批次效应吗？</strong> 只能缓解，不能消除。</li></ol><hr><h1 id="五、其它资料"><a href="#五、其它资料" class="headerlink" title="五、其它资料"></a>五、其它资料</h1><p>Stanford大学MOOC公开课讲义：<a href="http://genomicsclass.github.io/book/" target="_blank" rel="noopener">PH525x series - Biomedical Data Science</a></p><p><a href="https://bioinformatics.mdanderson.org/public-software/tcga-batch-effects/" target="_blank" rel="noopener">TCGA Batch Effects Viewer</a></p><p>From BioMedSearch: <a href="http://www.biomedsearch.com/nih/Removing-batch-effects-in-analysis/21386892.html" target="_blank" rel="noopener">Removing batch effects in analysis of expression microarray data: an evaluation of six batch adjustment methods.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、定义&quot;&gt;&lt;a href=&quot;#一、定义&quot; class=&quot;headerlink&quot; title=&quot;一、定义&quot;&gt;&lt;/a&gt;&lt;span id=&quot;def&quot;&gt;一、定义&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;下面是大佬给出来的关于&lt;strong&gt;批次效应&lt;/strong&gt;(batch effect)的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. &lt;a href=&quot;https://www.nature.com/articles/nrg2825&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Leek et. al (2010)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
  </entry>
  
</feed>
