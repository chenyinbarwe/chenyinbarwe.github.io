<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>阔落煮酒</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://chenyin.top/"/>
  <updated>2019-09-11T03:38:34.610Z</updated>
  <id>http://chenyin.top/</id>
  
  <author>
    <name>Barwe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>screen-SSH终端离线持久化工具</title>
    <link href="http://chenyin.top/os/20190911-89a2.html"/>
    <id>http://chenyin.top/os/20190911-89a2.html</id>
    <published>2019-09-11T03:03:26.000Z</published>
    <updated>2019-09-11T03:38:34.610Z</updated>
    
    <content type="html"><![CDATA[<p>有时候我们使用自己的电脑远程连接服务器（例如SSH）进行工作，某些任务我们希望放在<strong>前台</strong>运行但是其运行时间可能很长，如果程序运行期间我们需要断开连接，一般情况下这个前台任务也会随之中断。例如R交互式环境中的程序。screen工具就是为了解决这样一个问题。</p><a id="more"></a><p>screen工具不仅可以保证在断开远程连接的情况下继续运行当前任务，还可以实现单个<strong>实际窗口</strong>中操纵多个<strong>工作窗口</strong>。简单来说，新建一个<strong>screen会话</strong>会创建一个主进程，这个主进程对应着一个会话窗口。这个主进程是存储在服务器上的，它不受我们连接服务器的SSH进程的影响，因此当我们断开SSH连接时这个进程依旧存在，在下次重新连接服务器时依然可以恢复。而我们连接服务器时的工作环境中的任务实际上受SSH连接进程的影响，当我们断开连接时相关联的任务自然而然也就停止了。（PS: 下面提到的<strong>会话</strong>就是指一个虚拟窗口）</p><p>screen的用法如下：</p><ul><li><strong>怎么查询当前服务器中建立了哪些会话？</strong> <code>screen -ls</code> 即可</li><li><strong>怎么建立一个会话？</strong> <code>screen -S &lt;SOCKNAME&gt;</code> 可以建立一个虚拟会话，查看会话信息如下：<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/09/01.png" alt="screen_info_1"><br>由上可知，一个虚拟窗口的ID标识由<strong>进程号PID</strong>和<strong>会话名（SOCKNAME）</strong>组成，我们可以通过这两个信息恢复会话。除了 <code>screen -s ...</code> 之外，<code>screen -R ...</code> 也能建立一个新的虚拟窗口，与<code>-S</code>不同的是，<code>-R</code>是去尝试着恢复一个已有的会话，如果在已有会话中没有找到，他就会建立一个新的会话，跟“若目录不存在则创建目录”是一个意思。</li><li><strong>怎么恢复一个会话？</strong> <code>screen -r ...</code>可以恢复一个会话，当会话不存在时会报错；<code>screen -R ...</code>也能恢复一个会话，但是当会话不存在时会创建一个新的会话。注意：同一时间一个会话只能在一个实际窗口中打开（例如你可能会在不同的电脑上连接服务器或者在一台电脑上打开多个SSH会话）。当会话被挂起时（意味着此时没有窗口打开这个会话），会话信息中每个ID后面会标识出<code>(Detached)</code>，此时意味着你可以在当前窗口中打开这个会话继续工作；如果标识的是<code>(Attached)</code>，那么标识这个虚拟窗口已经在其它的地方被打开了，你将不能打开这个会话。如果你确定这属于异常情况，你可以使用<code>screen -d ...</code>强制挂起一个会话，此时状态会变成<code>(Detached)</code>，这表示你单方面终止了在某个未知地方打开的虚拟窗口。<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/09/02.png" alt="screen_info_2"><br>screen对你命令传入的会话名与会话ID进行匹配，例如你可以：<ul><li>只指定进程号PID，例如<code>screen -r 23288</code></li><li>在不引起歧义的情况下只指定会话名的首字母或前几个字母，例如<code>screen -r n</code>将恢复<code>23288.net</code></li><li>当然你也可以传入完整的<code>&lt;SOCKNAME&gt;</code>或者<code>&lt;PID&gt;.&lt;SOCKNAME&gt;</code></li></ul></li><li><strong>怎么退出并挂起当前会话？</strong> 依次摁下<code>Ctrl A D</code>三个键即可退出并挂起当前会话</li><li><strong>怎么挂起当前窗口中创建的指定会话？</strong>上面已经提到了，<code>screen -d ...</code>可强制挂起指定会话</li><li><strong>怎么删除一个会话？</strong>请确认你的虚拟窗口完成使命后再删除它，不然追悔莫及。<ul><li>直接<code>kill &lt;PID&gt;</code>可删除相应的会话</li><li>摁下<code>Ctrl A K D</code>可删除当前会话中的所有任务并退出当前会话</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候我们使用自己的电脑远程连接服务器（例如SSH）进行工作，某些任务我们希望放在&lt;strong&gt;前台&lt;/strong&gt;运行但是其运行时间可能很长，如果程序运行期间我们需要断开连接，一般情况下这个前台任务也会随之中断。例如R交互式环境中的程序。screen工具就是为了解决这样一个问题。&lt;/p&gt;
    
    </summary>
    
      <category term="os" scheme="http://chenyin.top/categories/os/"/>
    
    
  </entry>
  
  <entry>
    <title>综合转录&amp;表观数据的细胞分型</title>
    <link href="http://chenyin.top/bioinfo/20190531-c0e0.html"/>
    <id>http://chenyin.top/bioinfo/20190531-c0e0.html</id>
    <published>2019-05-31T03:49:11.000Z</published>
    <updated>2019-05-31T04:45:09.381Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaTitle.jpg" alt="">原文【<a href="https://www.nature.com/articles/nbt.4038" target="_blank" rel="noopener">Nature Biotechnology</a>】【<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5951394/" target="_blank" rel="noopener">PubMed</a>】</p><a id="more"></a><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>对人脑细胞进行<strong>更加细致</strong>的分类需要<strong>设计大量的实验从细胞的各个角度研究细胞</strong>或者<strong>设计数据处理方法整合现有数据对细胞进行注释</strong>。本文即从后者出发思考怎么综合多组学数据。这里提出了两个改良的高通量测序方法：</p><ul><li><strong>snDrop-seq</strong>: single-nucleus droplet-based sequencing </li><li><strong>scTHS-seq</strong>: single-cell transposome hypersensitive site sequencing</li></ul><p>基于两种方法获得了&gt;60000个单细胞核（成人视觉皮质、额皮质、小脑）的<strong>转录</strong>数据和<strong>开放</strong>数据。</p><p>这两组数据内蕴与细胞类型差异相关的调控元件和转录因子信息，可用于</p><ul><li>研究大脑复杂机制，本文研究了髓鞘再生（remyelination）</li><li>将疾病相关的风险变异关联到特定的细胞群，帮助认识大脑的病理</li></ul><p>多组学方法<strong>综合</strong>分析能够对<strong>复杂</strong>组织器官的的单细胞进行更加细致的研究（~interrogation）。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="转录组分析的不足"><a href="#转录组分析的不足" class="headerlink" title="转录组分析的不足"></a>转录组分析的不足</h2><p><strong>@ 人脑是个极其复杂的结构</strong></p><p>人脑是一个庞大、复杂的网络，它由大约一万亿个神经元组成，这些神经元在空间上有序组织，在功能上相互联系，它们镶嵌在更加庞大的神经胶质（glia）和非神经元细胞中。</p><p><strong>@ 分析细胞核比分析完整细胞更好</strong></p><p>建立完整的人脑细胞图谱，需要良好扩展性和无偏性的单细胞分析方法，这些方法不仅限于分析活体完整细胞，也能分析提取出来的细胞核信息。</p><p>如果能分析细胞核数据，将会好处多多$^{[1-4]}$：</p><ol><li>细胞可以是活体采集的，也可以是保存性的，例如尸体样本、冷冻样本</li><li>可以获得大量的RNA数据，能得到更加准确的分析结果</li><li>减小<strong>组织解离</strong>的误差$^{[5]}$</li></ol><p><strong>@ snRNA测序技术及后续分析方法</strong></p><p>单核转录组测序（SNS）能够以较大的测序深度（每个细胞大约八百万reads）分析多个人类大脑皮层区域上丰富的神经元亚型$^{[4]}$，但是它也有许多限制，例如</p><ul><li>低通量：每张微流芯片理论上只能对64个细胞测序</li><li>高成本</li><li>样本误差：微流芯片对十分少量的非神经细胞的细胞核的捕获能力很差</li></ul><p>本文提出了一个能够处理<strong>保存性样本</strong>、分析高通量<strong>snRNA-seq</strong>数据的方法。</p><h2 id="表观组锦上添花"><a href="#表观组锦上添花" class="headerlink" title="表观组锦上添花"></a>表观组锦上添花</h2><p>转录图谱能够鉴定复杂组织器官中<strong>功能差异显著</strong>的细胞类型，额外的<strong>表观信息</strong>能够使我们对表达图谱的受调节方式有更加全面的认识 —— 如虎添翼。</p><p>全基因组水平的研究已经建立了<strong>调控位点</strong>到了染色质开放性区域（同下文的调节子，如启动子内、增强子内）的映射，因此顺式调控位点能够区分细胞类型和谱系$^{[6,7]}$。这种细胞类型特异性的<strong>调节子</strong>的鉴定有利于强化我们对基因程序（如细胞分化、细胞定型、功能化）的理解。</p><p>进一步地，与多样性和疾病相关的基因变异大多数都位于内含子内或者基因间区域$^{[8]}$。与<strong>组织特异性</strong>调控位点$^{[6,7]}$的富集分析结合绘制的<strong>细胞类型特异性</strong>调控子图谱能够提供额外的信息帮助我们了解发病机理。</p><p>与转录组实验相比，表观组实验的一个障碍就是难以获得大量的单细胞样本。</p><p>最近的一些工作提高了实验灵敏度，将细胞需求减少到了几百个$^{[9]}$甚至是一个细胞$^{[10-13]}$；<br>然而，这类单细胞精度的分析方法是否适用于异质性的、保存性的人类组织（例如大脑）仍需验证。</p><h2 id="解决了什么问题"><a href="#解决了什么问题" class="headerlink" title="解决了什么问题"></a>解决了什么问题</h2><p>构建&lt;包括表观遗传信息在内的&gt;人脑细胞类型的综合图谱，需要&lt;用于分析<u>核转录数据</u>和<u>保存性样本数据</u>的&gt;更加高效的方法。</p><p>首先单细胞基因组研究中<strong>核分离技术</strong>是可信的（~amenable）$^{[14,15]}$。因此本文提出了两个优化的测序方法，在单细胞水平上帮助量化<u>核转录本</u>和<u>DNA的开放性</u>。因此我们可以在同一个<strong>保存性样本</strong>中同时对基因的<strong>表达</strong>和<strong>调节</strong>进行综合性分析。</p><p>本文前面部分解决了下面三个问题：</p><ul><li>描述人类<u>大脑皮质</u>和<u>小脑脑区</u>的细胞多样性</li><li>鉴定具有<strong>脑区特异性</strong>的<u>神经元/非神经细胞</u>类型</li><li>大规模地鉴定<u>转录因子的活性</u>和<u>目标基因的表达谱</u></li></ul><p>最后，还将<strong>疾病风险变异</strong>（risk variants）映射到了<u>细胞类型特异性的</u>调控区域，从概念上验证（proof-of-concept）出了一些可能与大脑疾病相关的病变细胞类型。</p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="人类大脑皮质和小脑脑区的单细胞分析"><a href="#人类大脑皮质和小脑脑区的单细胞分析" class="headerlink" title="人类大脑皮质和小脑脑区的单细胞分析"></a>人类大脑皮质和小脑脑区的单细胞分析</h2><h3 id="介绍了两种改进的测序方法"><a href="#介绍了两种改进的测序方法" class="headerlink" title="介绍了两种改进的测序方法"></a>介绍了两种改进的测序方法</h3><h4 id="snDrop-seq"><a href="#snDrop-seq" class="headerlink" title="snDrop-seq"></a>snDrop-seq</h4><p><strong>single-nucleus Droplet-bases sequencing</strong> <img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa01.jpg" alt="snDrop-seq实验步骤"></p><p>基于<strong>微滴</strong>实验技术$^{[16-18]}$的<strong>snDrop-seq</strong>，实现了高通量的单细胞转录组并行测序。<br>将此技术应用于六个成年人类遗体的大脑分析（包括视觉皮质、额皮质、侧小脑半球）。<br>=&gt; 这是一种<strong>转录组测序</strong>方法！！！</p><h4 id="scTHS-seq"><a href="#scTHS-seq" class="headerlink" title="scTHS-seq"></a>scTHS-seq</h4><p><strong>single-cell Transposome Hypersensitive Site sequence</strong> <img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa02.jpg" alt="scTHS-seq overview"></p><p>为了同时研究<strong>表观遗传</strong>，本文设计了一种定量<strong>单细胞DNA开放性</strong>的实验方法。<br>=&gt; 表观遗传 表型遗传 DNA/染色质开放性 ATAC</p><p>scTHS-seq利用了<strong>体外转录组线性扩增</strong>和<strong>人为设计Tn5转座酶突变</strong>$^{[15]}$的优势，实现比ATAC-seq$^{[20]}$灵敏度更高的测序方式，能较高覆盖远端具有细胞类型特异性的增强子$^{[21]}$。</p><p>联合这两个测序方法可以从同一个脑区样本中同时得到<strong>表达</strong>和<strong>调节</strong>特征，从而对<strong>细胞多样性</strong>进行独立无偏的分析。</p><h3 id="基因表达分析"><a href="#基因表达分析" class="headerlink" title="基因表达分析"></a>基因表达分析</h3><p>对snDrop-seq的结果进行基因的<strong>表达</strong>分析，质控后一共<strong>36,166</strong>个核，其中<strong>35,289</strong>个核（=视觉皮质<strong>19,368</strong>+额皮质<strong>10,319</strong>+小脑半球<strong>5,602</strong>）被成功分型。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa03.jpg" alt="snDrop-seq data sets"></p><p><strong>跨物种混合分析</strong>的证明了很小比例（2~11%）的<strong>双峰</strong>（doublets），这与全细胞测量结果相似。 （<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa04.jpg" target="_blank" rel="noopener">图</a>）</p><p>测序结果平均到每个细胞核有<strong>6,200</strong>个reads，这些reads大部分都落入了<strong>内含子区域</strong>，并且主要位于转录本的3’端，这是因为转录本和前体RNA都是依靠poly(A)捕获的。（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa05.jpg" target="_blank" rel="noopener">图</a>）</p><blockquote><p>与<strong>全细胞转录组</strong>方法相比较，<u>仅含细胞核</u>的Drop-seq和<u>完整细胞</u>的Drop-seq的覆盖度更低，但是却能产生相似的结果（例如检测到的基因比例）。尽管细胞核和完整细胞结果具有一致性，核测序结果还是存在一定的系统误差，特别是对于较长的基因，这可能是可变剪切导致的。</p></blockquote><p>总的来说，每核测到了<strong>928</strong>个不重复的转录本和<strong>719</strong>个基因（中值）。随着样本数量的增加，这一深度可以有效解决<u>细胞类型多样性</u>和<u>基因表达动态变化</u>的问题。</p><p>实际分型结果为<strong>35</strong>个明显的聚类簇，包括</p><ul><li>神经细胞<ul><li>皮质兴奋性神经元（Ex）</li><li>皮质抑制性神经元（In）</li><li>小脑颗粒细胞（Gran）</li><li>浦肯野神经元（Purk）</li></ul></li><li>非神经细胞<ul><li>内皮细胞（End）</li><li>平滑肌细胞或者周皮细胞</li><li>星形胶质细胞（Ast）</li><li>少突胶质细胞（Oli）以及他们的前体细胞（OPCs）</li><li>小星形胶质细胞（Mic）</li></ul></li></ul><p>还研究了这些簇在脑区分布上的异质性，例如</p><ul><li>小脑特异性的Ast（Ast_Cer）、OPCs</li><li>在视觉皮质和额皮质检测到了不同的兴奋性神经元（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa06.jpg" target="_blank" rel="noopener">图</a>）</li></ul><h3 id="基因调控分析"><a href="#基因调控分析" class="headerlink" title="基因调控分析"></a>基因调控分析</h3><p>为了<u>识别相应的调控信号</u>，基于<strong>scTHS-seq</strong>进行了基因的<strong>调控</strong>分析。</p><p>一共测定了<strong>32,869</strong>个细胞核。对每个脑区单独聚类，其中<strong>27,906</strong>个核（=视觉皮质<strong>13,232</strong>+额皮质<strong>4,753</strong>+小脑<strong>9,921</strong>）能成功分型。（这里没有给出聚类图，只给出结果表）</p><p>27906个核中，<strong>15,786</strong>个核映射到了snDrop-seq的聚类结果上。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa07.jpg" alt="F1d"></p><p>鉴定出了覆盖了大约144百万碱基对共<strong>287,381</strong>个与染色质开放性相关的peak。</p><p>每个细胞覆盖染色质开放性区域的reads个数（median）为<strong>10,168</strong>。</p><p>人鼠数据混合分析的<strong>低比例双峰</strong>特性说明可以使用<strong>combinatorial indexing protocols</strong>。</p><p>为了<u>鉴定出scTHS-seq数据中表观遗传差异显著的亚型</u>，本文采用了一种<strong>无偏聚类策略</strong>：将每个细胞每个基因组位点上观察到read的概率建模为<strong>censored poisson process</strong>。<br>这个方法解释了：即使是开放性最强位点的信号也会在匹配到少量reads时饱和。【Q】</p><blockquote><p>The scTHS-seq signal from even the most accessible site will saturate after only a few reads.</p></blockquote><h3 id="总结：解决了什么问题"><a href="#总结：解决了什么问题" class="headerlink" title="总结：解决了什么问题"></a>总结：解决了什么问题</h3><p>从表观组对细胞分型比从转录组对细胞分型更加困难，因为许多调控元件的功能仍然是未知的。<br>但是借助于邻近开放性位点的基因的信息，就能够区分皮质和小脑脑区的许多细胞类型。.<br>具体来说，调控分析主要解决了下面两个问题</p><ul><li>与额皮质、视觉皮质之间主要细胞类型相关的表观特征</li><li>尚未深入研究的小脑Gran神经细胞的神经元特征</li></ul><h2 id="转录组-gt-细胞类型-amp-脑区异质性"><a href="#转录组-gt-细胞类型-amp-脑区异质性" class="headerlink" title="转录组 =&gt; 细胞类型 &amp; 脑区异质性"></a>转录组 =&gt; 细胞类型 &amp; 脑区异质性</h2><p>转录组数据来源于snDrop-seq</p><h3 id="marker表达一致性"><a href="#marker表达一致性" class="headerlink" title="marker表达一致性"></a>marker表达一致性</h3><p><strong>1. 基于snDrop-seq的marker表达情况</strong></p><p>不同的脑区都使用<strong>转录本数据</strong>对细胞的归属和身份进行鉴定，检测到了<strong>细胞类型/细胞亚型特异性</strong>的marker基因的<strong>预期表达</strong>，表达谱与从人、鼠脑区的混池数据推出的结论高度一致。<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa08.jpg" alt="F2AB"><br>$$\text{Figure 2 - A \&amp; B}$$<br>A图是<strong>marker基因-细胞类型</strong>基因表达的小提琴图，从图中明显可以看到每个marker都在某一类细胞中大量表达。A图右侧的三列数据分别表示：细胞核数目、转录本数目、不同脑区采样的相对比例。B图是<strong>marker基因-细胞亚型</strong>的表达图谱。</p><p><strong>2. snDrop-seq的一致性验证</strong></p><p>比较小鼠视觉皮层和人颞叶数据，基于<strong>核</strong>的数据与基于<strong>全细胞</strong>的数据具有一致性。<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa09.jpg" alt="09"><br>$$\text{Supplementary Figure 7 - A \&amp; B}$$</p><p>神经元的过表达是以牺牲非神经细胞例如Ast和End为代价的。【Q】</p><blockquote><p>However, we observed over-representation of neurons at the expense of non-neuronal types such as Ast and End cells.</p></blockquote><p>从snDrop-seq数据计算细胞组成时仍然存在技术上的误差，可能是因为细胞转录本数量太少<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa10.jpg" alt="F2A1"><br>$$\text{Figure 2 - A}$$</p><p>Ex和In按照（先前对6个皮质区域进行单细胞核测序鉴定出的）细胞亚型的相关性进行注释<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa11.jpg" alt="Ex和In的细胞亚型的相关性热图"><br>$$\text{Supplementary Figure 7 - C}$$<br>从上图还可以看出，除了<strong>高度的一致性</strong>，snDrop-seq还能对<strong>细胞亚型</strong>的分型进行优化。例如，视觉皮质层Ex的细胞亚型Ex3细化为了Exa-d四种亚型。相比于作者之前的一项研究$^{[4]}$，snDrop-seq通过复杂组织的少量采样解决<strong>神经细胞亚型多样性</strong>问题灵敏度更高。（神奇般的引用了自己）</p><h3 id="细胞亚型的空间特异性（皮质层分布）"><a href="#细胞亚型的空间特异性（皮质层分布）" class="headerlink" title="细胞亚型的空间特异性（皮质层分布）"></a>细胞亚型的空间特异性（皮质层分布）</h3><h4 id="Ex和Gran"><a href="#Ex和Gran" class="headerlink" title="Ex和Gran"></a>Ex和Gran</h4><p>将Ex和Gran（通过<strong>SLC17A7</strong>和<strong>GRM4</strong>筛选）进一步分解为<strong>14种</strong>细胞亚型（13Ex+1Gran），如图Figure2A。</p><p>【好长的一句话，看图比较容易】除了对<strong>皮质层</strong>细胞进行更加细致的<strong>亚型分型</strong>（此处的皮质层包括第5层中不同的亚型$\text{HS3ST5}^-\text{PCP4}^+\text{/ Ex5a}$、$\text{HS3ST5}^+\text{PCP4}^-\text{/ Ex5b}$ 和 $\text{HTR2C}^+\text{PCP4}^+\text{TLE4}^+\text{/ Ex6a}$，Ex6a与第6层中的$\text{HTR2C}^-\text{TLE4}^+\text{/ Ex6b}$相邻），随着<u>可视的<strong>皮质层特异性</strong>的细胞亚型</u>的数量的增加，我们还可以解决巨大的<strong>脑区异质性</strong>问题，包括亚型$\text{RORB}^+\text{PCP4}^+\text{/ Ex3b}$、$\text{RORB}^+\text{NEFM}^{hi}\text{/ Ex3c}$、$\text{RORB}^+\text{PHACTR2}^+\text{EYA4}^+\text{/ Ex3d}$。如图Figure2C<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa12.jpg" alt="Ex的marker在不同皮质层的分布"><br>$$\text{Figure 2 - C}$$</p><p>从上图，Ex3d神经元的marker是<strong>EYA4</strong>，它在第4皮质层的视觉皮质中特异性表达，在额皮质中没有表达。</p><h4 id="In和Purk"><a href="#In和Purk" class="headerlink" title="In和Purk"></a>In和Purk</h4><p>我们将In和Purk（利用<strong>GAD1</strong>筛选，如图Figure2A）按照权威的中间神经元marker（例如<strong>VIP, RELN, PVALB, SST</strong>）和在单一亚型中表达的基因（例如<strong>THSD7B, CA8, GLCE</strong>）分解为<strong>13种</strong>细胞亚型（11In+2Purk），如下图。<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa13.jpg" alt="In和Purk神经元的亚型"><br>$$\text{Figure 2 - B - Bottom}$$</p><p>进一步研究了In神经元亚型的空间分布差异，如下图<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa14.jpg" alt="In神经元亚型的空间分布差异"><br>$$\text{Supplementary Figure 8 - E}$$</p><h3 id="小脑细胞类型分析"><a href="#小脑细胞类型分析" class="headerlink" title="小脑细胞类型分析"></a>小脑细胞类型分析</h3><p>前面做的都是大脑皮质，现在开始研究小脑</p><h4 id="小脑结构"><a href="#小脑结构" class="headerlink" title="小脑结构"></a>小脑结构</h4><p>相比于大脑皮质，小脑具有更加明显的细胞结构<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa15.jpg" alt="小脑的细胞类型"><br>$$\text{Figure 2 - E}$$</p><h4 id="marker的表达及空间特异性"><a href="#marker的表达及空间特异性" class="headerlink" title="marker的表达及空间特异性"></a>marker的表达及空间特异性</h4><p>本文解决了多个主要的细胞群，包括Gran和Purk神经元以及它们的支持类型<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa16.jpg" alt="小脑中各marker在各细胞类型中的表达情况"><br>$$\text{Figure 2 - F}$$<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa17.jpg" alt="小脑中各marker的蛋白质表达情况"><br>$$\text{Figure 2 - G}$$</p><h4 id="Purk神经元亚型"><a href="#Purk神经元亚型" class="headerlink" title="Purk神经元亚型"></a>Purk神经元亚型</h4><p>发现了两个明显的Purk神经元亚型Purk1和Purk2：它们都表达了抑制性的marker基因GAD1/GAD2（如图Figure 2F），但是可以通过marker基因SORCS3进行区分（SORCS3在Purk2中不表达）。</p><h4 id="Ast神经元亚型"><a href="#Ast神经元亚型" class="headerlink" title="Ast神经元亚型"></a>Ast神经元亚型</h4><p>鉴定出了已被证明存在的Ast的亚群（如图Figure 2F）：</p><ul><li><strong>Velate Astrocytes, VAs</strong>：表现出与皮质星形胶质细胞相似的转录特征，对Gran神经元起支持作用</li><li><strong>Bergmann Glia, Ast_Cer</strong>：一种特殊的星形胶质细胞，在小脑层状结构的发育过程中起着重要作用，支持和调节Purk神经元的突出活动。通过marker基因ALDH1A1筛选出来的Ast_Cer细胞丰富表达了AMPA受体编码基因GRIA1和SLC1A3（或者叫GLAST）。</li></ul><h4 id="OPCs神经元亚型"><a href="#OPCs神经元亚型" class="headerlink" title="OPCs神经元亚型"></a>OPCs神经元亚型</h4><p>进一步分解出了两个明显的OPCs细胞亚群：</p><ul><li>$\text{LUZP2}^+\text{CASK}^+$：表现出与皮质OPC相似的转录特点</li><li>$\text{ORAOV1}^+\text{LRP6}^+\text{RCN2}^+$：只在小脑中发现，未介绍功能</li></ul><h4 id="Oli神经元"><a href="#Oli神经元" class="headerlink" title="Oli神经元"></a>Oli神经元</h4><p>大部分小脑Oli起源于小脑外部，少数起源于local祖细胞。【很莫名的一句话】</p><blockquote><p> This is consistent with the majority of the cerebellar Oli originating from outside the cerebellum, and only a minority being derived from local progenitors$^{[30]}$.</p></blockquote><h4 id="形态学区分法的限制"><a href="#形态学区分法的限制" class="headerlink" title="形态学区分法的限制"></a>形态学区分法的限制</h4><p>也有人提出了基于形态学区分小脑半球的不同类型，但是其局限性很大：</p><ol><li>这些易于区分的细胞数量有限</li><li>小脑组织中Gran神经元极其丰富，形态学并不能帮助我我们完美地区分亚型。</li></ol><h2 id="转录表观联合模型"><a href="#转录表观联合模型" class="headerlink" title="转录表观联合模型"></a>转录表观联合模型</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>为了建立不同亚型<strong>转录状态</strong>和<strong>表观状态</strong>之间更加准确的<strong>对应关系</strong>，进行如下操作：</p><ul><li>从<strong>ATAC数据</strong>中寻找（与通过<strong>转录组数据</strong>求出的细胞亚群相对应的）细胞</li><li>从<strong>转录组数据</strong>中寻找（与通过<strong>ATAC数据</strong>求出的细胞亚群相对应的）细胞</li></ul><p>为了实现上述操作，进行了下面两个工作【Q】：</p><ul><li>以<strong>差异表达模式</strong>为基础，训练一个GBM预测<strong>差异开放</strong>的基因组位点</li><li>以<strong>差异开放性</strong>为基础，训练另外一个GBM预测<strong>差异表达</strong></li></ul><p>量化依据：</p><ul><li>一个<strong>位点</strong>到一个<strong>基因</strong>的<strong>距离</strong></li><li>某个位点或基因<strong>差异表达</strong>或者<strong>差异开放</strong>的程度</li></ul><p>模型示意图如下<br><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isa18.jpg" alt="表达and开放综合模型示意图"><br>$$\text{Figure 3 - a}$$</p><h3 id="可行性"><a href="#可行性" class="headerlink" title="可行性"></a>可行性</h3><p>尽管预测单个基因或者位点的差异表达或者差异开放的程度有点困难，联合考虑多个基因或者位点来对细胞分类的可信度还是挺高的。<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaSF10.jpg" target="_blank" rel="noopener">Supplementary Figure 10</a></p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>考虑到转录组数据的高精度特性，我们试图通过识别<u>观察到的转录组亚群</u>的染色质开放性特征，将该模型应用于进一步地划分染色质开放性聚类簇【Q】</p><ol><li>通过<strong>积累表达信号</strong>对已识别的细胞类型进行<strong>层次聚类</strong>，建立一个细胞类型关系的系统树图</li><li>在系统树图上进行<strong>迭代二分</strong>，识别出两个分支上<strong>差异表达</strong>的基因</li><li>使用GBM分类器预测<strong>差异开放</strong>的基因组位点</li><li>考虑所有预测的差异开放的位点将scTHS-seq的细胞分类到其中一个分支，其染色质开放模式与该分支的染色质开放模式一致</li><li>从起始的分类结果我们可以定义一个refined差异开放信号，综合二者判断最终所属分支，同时通过交叉验证评估分支注释的稳定性【Q】</li></ol><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>通过这种方法，我们从<strong>转录数据</strong>中识别出了（新的？）非神经元细胞和神经元细胞类型之间的差异表达基因。</p><p>通过预测的<strong>差异开放性位点</strong>，细化（染色质开放性数据中的）非神经元和神经元细胞类型置信度更高。</p><p>解决了神经元细胞类型的问题，我们可以重复这个过程来区分兴奋性神经元和抑制性神经元。<br>然后再次重复这个过程来区分抑制神经元中不同的细胞亚型（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3b.jpg" target="_blank" rel="noopener">Figure 3b</a> &amp; <a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3c.jpg" target="_blank" rel="noopener">Figure 3c</a>）。<br>依此类推。</p><p>通过这种方法，我们能够识别与抑制性神经元亚型（InA, InB）相关的表观遗传（开放性）差异</p><blockquote><p>InA和InB的差异：发育来源不同（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3b.jpg" target="_blank" rel="noopener">Figure 3b</a>）</p><ul><li>InA起源于内侧神经节的皮质下区域</li><li>InB起源于外侧/尾侧神经节的皮质下区域</li></ul></blockquote><p>然而，因为差异开放位点的数量不够，尝试从InA和InB中分解出更多的细胞亚型时结果<strong>不稳定</strong>。（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3b.jpg" target="_blank" rel="noopener">Figure 3b</a> &amp; <a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3c.jpg" target="_blank" rel="noopener">Figure 3c</a>）</p><p>仅仅通过差异<strong>开放</strong>信息区分ExL4和ExL5&amp;ExL6的差异是很难的，此时结合来自更高精度的转录本数据的差异<strong>表达</strong>信息能够帮助我们识别（更多的？）相关<strong>差异开放性</strong>位点，从而将<u>基于染色质开放性的</u>聚类簇分解成更加细致的亚型。</p><p>鉴定出主要细胞类型的置信度较高，如</p><ul><li>Oli, OPC, Ast, End, In, and Ex cells as expected from the visual cortex（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3c.jpg" target="_blank" rel="noopener">Figure 3c</a>）</li><li>Ast, Oli, In, and Ex cells in the frontal cortex and Gran, Oli, and End cells in the cerebellum（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaSF6.jpg" target="_blank" rel="noopener">Supplementary Figure 6</a>）</li></ul><p>我们进一步证实了，分解出来的细胞类型/亚型在marker的<strong>启动子</strong>位置出现了开放性富集，这表明了：表达和调控的一致性。（<a href="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaF3g.jpg" target="_blank" rel="noopener">Figure 3g</a>）</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>相比于转录组数据，表观组数据的细胞分型精度较低。但是整合两类数据能够重构出较为详细的大脑细胞类型开放性图谱，简单研究每个细胞类型调控过程的活性。</p><h2 id="髓鞘再生"><a href="#髓鞘再生" class="headerlink" title="髓鞘再生"></a>髓鞘再生</h2><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/MyelinSheath.jpg" alt=""></p><p>中枢神经系统的髓鞘是由少突胶质细胞缠绕神经轴突形成</p><h3 id="找TFs"><a href="#找TFs" class="headerlink" title="找TFs"></a>找TFs</h3><p>前面建立了可以根据表观组数据区分细胞类型/亚型的模型，现在寻找与每一种细胞类型/亚型特异性相关的TFs。</p><p>我们要找的是这样一些TFs：它们的预测结合位点在<u>具有细胞类型差异性的</u>开放性区域内<strong>过度表达</strong>。（Figure 4a）</p><p>扫描379个TFs（来自于JSPAR数据库，带有PWM），然后找到了<u>与至少一种细胞类型统计显著相关</u>的TFs。（Figure 4b）</p><p>深入研究了<u>不同空间位置的Ex和In特异性的</u>TF的活性。</p><p>使用snDrop-seq数据进行交叉验证，确保细胞类型特异性的TF也会在相应的细胞类型中高表达。</p><h3 id="成年人脑中OPCs向Oli的转变过程"><a href="#成年人脑中OPCs向Oli的转变过程" class="headerlink" title="成年人脑中OPCs向Oli的转变过程"></a>成年人脑中OPCs向Oli的转变过程</h3><p><strong>一个转录因子引发的惨案：新的中间细胞、人鼠转变差异</strong></p><p>【机翻】<u>为了证明前面提到的综合性分析方法具有一定的生物学实用价值，深入研究了成年人脑中OPCs向Oli的转变过程。</u>髓鞘再生是通过<strong>OPCs</strong>的神经元<strong>活化</strong>和<strong>分化</strong>成<strong>有髓少突胶质细胞</strong>而发生的，这些细胞重新包裹神经元轴突，以恢复跳跃传导和正常功能。这一过程的失调可导致严重的神经系统疾病，例如多发性硬化症（MS）。值得注意的是，我们发现了区分<strong>OPC</strong>和<strong>Oli</strong>细胞群的特异转录因子（<strong>F4b</strong>）。为了确定这些【Q】是否能揭示<strong>成人髓鞘再形成</strong>的关键调控过程，我们评估了视觉皮质中这些谱系的分化状态和相关基因表达特征。利用Destiny的扩散图谱，我们可以将<strong>OPC</strong>和<strong>Oli</strong>细胞沿着发育轨迹<strong>定位</strong>（F4C、SF12），并评估细胞在开始、中间和结束时的差异表达。在此过程中，我们发现了独立于实验批次的中间细胞（未成熟的<strong>Oli</strong>，称之为<strong>iOli</strong>），它们具有独特的表达特征（图4C-D，图S12，表S6），可以深入了解人类成年<strong>Oli</strong>成熟的早期机制。与小鼠实验结果一致，我们的人类<strong>OPC</strong>细胞群表达了与小鼠<strong>OPCs</strong>相关的marker（<strong>PDGFRA</strong>, <strong>CSPG4</strong>, <strong>SOX6</strong>, <strong>VCAN</strong>），但他们也表达了更忠诚的小鼠祖细胞的marker(<strong>ITPR2</strong>, <strong>NEU4</strong>)，表明我们的人类数据无法区分这些微妙的状态（图4D，表S6）。此外，成熟的<strong>Oli</strong>（<strong>mOli</strong>）细胞群表达了与髓鞘形成相关的marker（<strong>PLP1</strong>, <strong>MBP</strong>, <strong>MOG</strong>）（图4D，表S6），并没有像小鼠那样分解成进一步的亚型，这可能是成年人脑中缺乏幼年状态的原因。</p><h3 id="渐进表达特征-gt-OPC谷氨酸激活反应"><a href="#渐进表达特征-gt-OPC谷氨酸激活反应" class="headerlink" title="渐进表达特征 =&gt; OPC谷氨酸激活反应"></a>渐进表达特征 =&gt; OPC谷氨酸激活反应</h3><p>结合其他研究</p><p><u>然而，在OPCs、iOli和mOli中发现的渐进表达特征（表达量？），是跨脑区域保守且独立于排序方法的，可以进一步细化为OPC谷氨酸激活反应的各个阶段（图S13）</u>。最近的研究表明，<strong>AMPA</strong>和<strong>红藻氨酸</strong>受体介导了一种对谷氨酸的初始<strong>轴突-OPC突触反应</strong>，该反应将<strong>OPCs</strong>导向暴露的轴突位置，<strong>NMDA</strong>受体的激活将在轴突位置上引导髓鞘再生。与这一发现一致，我们的数据显示<strong>AMPA</strong>和红藻氨酸受体编码基因（<strong>GRIN/GRIK</strong>）在<strong>OPCs</strong>和<strong>iOli</strong>中富集，而<strong>NMDA受体</strong>编码基因（<strong>GRIN2A, GRIN2B</strong>）仅在<strong>iOli</strong>亚群中富集（图4D，表S6）。<strong>OPC</strong>成熟过程中所鉴定的基因集的功能个体发育经历了六个阶段，这些结果为神经细胞在髓鞘再生中的活动机制提供了独立的支持：(图S13)</p><ol><li>神经发生（祖细胞marker表达）</li><li>谷氨酸受体活性</li><li>突触传递</li><li>离子通道活性</li><li>膜组装</li><li>轴突鞘形成。</li></ol><h3 id="OPC和Oli亚群的差异上调基因的开放性"><a href="#OPC和Oli亚群的差异上调基因的开放性" class="headerlink" title="OPC和Oli亚群的差异上调基因的开放性"></a>OPC和Oli亚群的差异上调基因的开放性</h3><p>差异开放、开放互斥、保守调控</p><p><strong>为了了解这些基因表达动态的调控机制，我们基于scTHS-seq数据联合评估了视觉皮质中OPC和Oli亚群的差异上调基因的开放性。</strong> 与我们的表达数据一致，<strong>OPC</strong>、<strong>iOli</strong>和<strong>mOli</strong>基因的调控位点显示出差异开放性(图4E)。此外，<strong>OPC</strong>和<strong>iOli</strong>基因的开放性显示出<strong>几乎完全的互斥性</strong>，表明可能存在维持这两种状态的活跃调控机制。<strong>OPC</strong>开放性区域内最显著的转录因子活动与<strong>SOX9</strong>有关(图4F，表S7)（已知<strong>SOX9</strong>与小鼠<strong>OPC</strong>的特化、生存和迁移有关）。此外，我们发现<strong>iOli</strong>特异的开放性位点表现出了重要的<strong>TCF4</strong>转录因子的结合富集（图4 g、表S7）（<strong>TCF4</strong>在调节Wnt/β-catenin以促进小鼠髓鞘再生中扮演着很重要的角色）。因此，我们的转录因子分析揭示了<u>维持成年少突胶质细胞祖细胞并协调其成熟以进行再髓鞘化的</u>保守调控机制。</p><h2 id="将病变风险映射到大脑特定的细胞类型"><a href="#将病变风险映射到大脑特定的细胞类型" class="headerlink" title="将病变风险映射到大脑特定的细胞类型"></a>将病变风险映射到大脑特定的细胞类型</h2><p>细胞类型特异性表观基因组信息对于许多常见遗传疾病的致病细胞类型和特异性调控机制的识别具有重要价值，但对脑疾病的认识仍不充分。</p><p>为了解决这一问题，我们通过<strong>美国国立卫生研究院</strong>（NIH）的<a href="https://grasp.nhlbi.nih.gov/Search.aspx" target="_blank" rel="noopener"><strong>GRASP搜索工具</strong></a>（Genome-Wide Repository of Associations Between SNPs and Phenotypes）获取了$p&lt;10^{-6}$的SNPs，这些SNPs与10个大脑疾病相关，同时还选取了7个非大脑疾病相关SNPs作为对照。</p><p>由于致病突变常位于与GWAS SNPs连锁不平衡的不同位置，我们以某一特定疾病的所有GWAS SNPs为中心，在100 kb窗口中寻找富集的dna开放性区域，并通过<strong>随机排列</strong>评估其意义。</p><p>该分析证明了（在多种细胞类型和亚型中）很强的疾病特异性富集，与另一种可能（即一致性）形成对比。</p><p>值得注意的是，我们发现阿尔茨海默病(AD)风险变异在Mic中高度富集，这与在晚发性AD皮质和AD风险中被发现激活的显著Mic信号一致。</p><p>与ATAC-seq data批量数据的比较表明，我们的单细胞数据对于预测Mic调控位点及其相关的疾病特异性风险变异富集具有敏感性。</p><p>我们没有发现任何与大脑无关的疾病变异的显著富集神经元，这进一步支持了我们对疾病致病性的预测细胞类型。</p><p>事实上，大多数与大脑无关的富集都是与这些疾病密切相关的细胞类型，如自身免疫性疾病中的Mic和End细胞。</p><p>因此，我们的单细胞调控图与大量研究高度一致，可能与细胞类型特异性疾病风险相关。</p><p>尽管进一步地验证需要更多的样本和其它疾病的数据，发病机理尚未研究，我们的染色质图谱提供了一个细胞类型或细胞亚型特异性数据集，通过这些数据集可以从新的角度了解大脑疾病。</p><p><strong>总结：</strong></p><p>在线搜索疾病相关的SNPs，观察它们是否有细胞类型特异性：验证出了AD在Mic中富集，与大脑无关的疾病不会富集。</p><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>细胞类型组成的重构有利于加深我们对<u>人脑正常功能</u>和<u>功能失调以及发病机理</u>的理解。</p><p>本文使用了两个扩展性较强的方法（snDrop-seq和scTHS-seq）在单细胞水平上综合分析了成年遗体大脑的转录组和表观组数据。</p><p>使用核分离技术克服活体采样或者保存性样本预处理的阻碍，我们鉴定出了大脑皮质和小脑脑区神经细胞/非神经细胞一共35个细胞亚群。</p><p>我们的结果强调了<u>在复杂组织中进行单细胞稀疏采样的</u>价值：只要单细胞数据内含用于聚类和排序的所有信息，它们就能被整合起来，起到与混池测序相同的效果。</p><p>这一结论也能应用于染色质开放性数据，用来解释基于视觉皮质数据分解出了大量的细胞亚型。</p><p>然而，尽管可以增加scTHS的测序深度，染色质开放性数据本身用于比较精细的细胞类型分解比较困难，其灵敏度需要提高。</p><p>尽管snDrop-seq能够用于更加广泛的组织建谱，但是我们在小鼠的皮质Ast和Oli的细胞亚型的寻找过程中没有成功！！！然后分析了可能的原因。</p><p>还是解决了一些问题：细胞分型、髓鞘再生</p><p><strong>para2</strong></p><p>提出了一种整合转录组和表观组数据的策略，进行更加精细的细胞分型。</p><h1 id="Glossary"><a href="#Glossary" class="headerlink" title="Glossary"></a>Glossary</h1><p>提供……的视角 provide insights into …<br>综合分析 integrative analysis<br>最终 ultimately<br>使……成为必需 necessitate [vt]<br>总的来说 overall<br>与……相关 underlying<br>解释，诠释，解调 interrogation<br>被进一步的分解为 be further resolved into<br>起源于 stem from / derive from<br>uneven/even 不均匀的，奇数/偶数<br>notably 特别地，尤其地，引人注目地<br>be in line with 与……一致 corresponding with</p><p>基因内区域 intronic region<br>基因间区域 intergenic region<br>表观遗传构型 epigenetic configuration<br>髓鞘再生 remyelination</p><p><strong>cerebellum</strong> 英 [,serɪ’beləm]  美 [‘sɛrə’bɛləm] n. [解剖] <strong>小脑</strong> [ 复数 cerebellus或cerebella ]<br><strong>temporal lobe</strong> [解剖] <strong>颞叶</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/isaTitle.jpg&quot; alt=&quot;&quot;&gt;原文【&lt;a href=&quot;https://www.nature.com/articles/nbt.4038&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Nature Biotechnology&lt;/a&gt;】【&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5951394/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PubMed&lt;/a&gt;】&lt;/p&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="single cell" scheme="http://chenyin.top/tags/single-cell/"/>
    
  </entry>
  
  <entry>
    <title>markdown奇淫技巧</title>
    <link href="http://chenyin.top/cargo/20190514-71e.html"/>
    <id>http://chenyin.top/cargo/20190514-71e.html</id>
    <published>2019-05-14T03:13:16.000Z</published>
    <updated>2019-05-14T03:19:02.166Z</updated>
    
    <content type="html"><![CDATA[<h1 id="原生表格中插入包含-的代码"><a href="#原生表格中插入包含-的代码" class="headerlink" title="原生表格中插入包含 | 的代码"></a>原生表格中插入包含 <code>|</code> 的代码</h1><p>由于markdown原生表格使用”|”分隔列，在表格中直接输入”|”会使表格结构解析错误。</p><p>下面这种写法可以使表格正常解析：</p><table><thead><tr><th style="text-align:center">样式</th><th style="text-align:center">代码</th></tr></thead><tbody><tr><td style="text-align:center"><code>A &#124; B</code></td><td style="text-align:center"><code>&lt;code&gt;A &amp;#124; B&lt;/code&gt;</code></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;原生表格中插入包含-的代码&quot;&gt;&lt;a href=&quot;#原生表格中插入包含-的代码&quot; class=&quot;headerlink&quot; title=&quot;原生表格中插入包含 | 的代码&quot;&gt;&lt;/a&gt;原生表格中插入包含 &lt;code&gt;|&lt;/code&gt; 的代码&lt;/h1&gt;&lt;p&gt;由于markdow
      
    
    </summary>
    
      <category term="cargo" scheme="http://chenyin.top/categories/cargo/"/>
    
    
  </entry>
  
  <entry>
    <title>R模型公式</title>
    <link href="http://chenyin.top/R/20190514-95ad.html"/>
    <id>http://chenyin.top/R/20190514-95ad.html</id>
    <published>2019-05-14T03:00:25.000Z</published>
    <updated>2019-05-14T03:10:26.650Z</updated>
    
    <content type="html"><![CDATA[<p>在进行 <strong>方差分析</strong>（ANOVA）或者 <strong>回归分析</strong> 时我们常常会遇到 <strong>~</strong> 操作符，这对R新手来说实在是难以理解，遂查查<a href="http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html" target="_blank" rel="noopener">文档</a>整理整理。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/tildeBig.jpg" alt=""></p><a id="more"></a><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aov(formula, data = <span class="literal">NULL</span>, projections = <span class="literal">FALSE</span>, qr = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, <span class="keyword">...</span>)</span><br><span class="line"></span><br><span class="line">lm(formula, data, subset, weights, na.action, method = <span class="string">"qr"</span>, model = <span class="literal">TRUE</span>, x = <span class="literal">FALSE</span>, y = <span class="literal">FALSE</span>, qr = <span class="literal">TRUE</span>, singular.ok = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, offset, <span class="keyword">...</span>)</span><br><span class="line"></span><br><span class="line">glm(formula, family = gaussian, data, weights, subset, na.action, start = <span class="literal">NULL</span>, etastart, mustart, offset, control = list(<span class="keyword">...</span>), model = <span class="literal">TRUE</span>, method = <span class="string">"glm.fit"</span>, x = <span class="literal">FALSE</span>, y = <span class="literal">TRUE</span>, singular.ok = <span class="literal">TRUE</span>, contrasts = <span class="literal">NULL</span>, <span class="keyword">...</span>)</span><br></pre></td></tr></table></figure><p>R函数例如 <code>aov()</code>, <code>lm()</code>, <code>glm()</code> 都提供了 <code>formula</code> 参数供用户定义将要进行的分析中涉及到的变量（反应变量、解释变量）。这个 <code>formula</code> 参数直接决定了R构建和测试的模型结构。<code>formula</code> 参数的基本格式为：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response_variable[反应变量] ~ explanatory_variables[解释变量]</span><br></pre></td></tr></table></figure><p>上式中的波浪号（tilde）可以理解为“通过……建模”或者“是……的函数”。</p><p>上式的技巧多在于，如何书写解释变量。</p><p>最基础的回归分析的 <code>formula</code> 参数格式如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y ~ x</span><br></pre></td></tr></table></figure><p>上式中的”x” 称之为 <strong>解释变量</strong>（Explanatory Variable）或者 <strong>自变量</strong>（Independent Variable, IV）,”y”称之为 <strong>反应变量</strong>（Response Variable）或者 <strong>因变量</strong>（Dependent Variable, DV）。</p><p>如果还有其它的解释变量添加到表达式后面即可。下式表示构建两变量的多回归模型：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y ~ x + z</span><br></pre></td></tr></table></figure><p>类似于上面的式子我们称之为 <strong>模型公式</strong>（Model Formula），它将传递给 <code>formula</code> 参数。</p><p>如何书写正确优雅的模型公式是一件很有意思的事情……</p><p>尤其注意，我们在其它地方使用的数学运算符（例如四则运算符）在模型公式里都有新的含义，也就是说我们不能像读一般数学公式那样去阅读模型公式。</p><p>下面这张表列出了模型公式中常用符号的意义，我们可以直观地感受到它们与一般意义地显著差别：</p><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:center">例子</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center"><code>+</code></td><td style="text-align:center"><code>+ x</code></td><td style="text-align:center">包含了解释变量x</td></tr><tr><td style="text-align:center"><code>-</code></td><td style="text-align:center"><code>- x</code></td><td style="text-align:center">删除解释变量x（没太理解）</td></tr><tr><td style="text-align:center"><code>:</code></td><td style="text-align:center"><code>x : z</code></td><td style="text-align:center">包含了解释变量x和z间的互作</td></tr><tr><td style="text-align:center"><code>*</code></td><td style="text-align:center"><code>x * z</code></td><td style="text-align:center">包含了解释变量x和z，以及它们之间的互作</td></tr><tr><td style="text-align:center"><code>/</code></td><td style="text-align:center"><code>x / z</code></td><td style="text-align:center">嵌套：包含了嵌套在x中的z</td></tr><tr><td style="text-align:center"><code>&#124;</code></td><td style="text-align:center"><code>x &#124; z</code></td><td style="text-align:center">条件：包含了给定z时的x</td></tr><tr><td style="text-align:center"><code>^</code></td><td style="text-align:center"><code>(u + v + w + z) ^ 3</code></td><td style="text-align:center">包含了四个变量，以及它们之间最多三个变量间的互作</td></tr><tr><td style="text-align:center"><code>poly</code></td><td style="text-align:center"><code>poly(x, 3)</code></td><td style="text-align:center">多项式回归：正交多项式</td></tr><tr><td style="text-align:center"><code>Error</code></td><td style="text-align:center"><code>Error(a/b)</code></td><td style="text-align:center">指定误差项</td></tr><tr><td style="text-align:center"><code>I</code></td><td style="text-align:center"><code>I(x*z)</code></td><td style="text-align:center"><code>I()</code> 中的表达式保留一般的数学意义，表示包含了x乘以z这个新变量</td></tr><tr><td style="text-align:center"><code>1</code> (数字)</td><td style="text-align:center"><code>- 1</code></td><td style="text-align:center">截距：删除截距，即通过原点回归</td></tr></tbody></table><p>同一个模型可以通过不同的公式表达：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model 1</span></span><br><span class="line">y ~ u + v + w + u:v + u:w + v:w + u:v:w</span><br><span class="line">y ~ u * v * w</span><br><span class="line">y ~ (u + v + w)^<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model 2</span></span><br><span class="line">y ~ u + v + w + u:v + v:w + v:w</span><br><span class="line">y ~ u * v * w - u:v:w</span><br><span class="line">y ~ (u + v + w)^<span class="number">2</span></span><br></pre></td></tr></table></figure><p>解释变量的属性（例如二值变量、分类变量、数值变量……）决定了模型的特性，例如对公式 <code>y ~ x + z</code>：</p><ul><li>如果x和y是两个分类变量，该公式表示方差分析[?]</li><li>如果x和y是两个数值变量，该公式表示多回归[?]</li><li>如果一个是数值变量一个是分类变量，则表示相关性分析[?]</li></ul><p><strong>关于 <code>Error()</code> 的一点见解</strong></p><p>略</p><!-- url -->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在进行 &lt;strong&gt;方差分析&lt;/strong&gt;（ANOVA）或者 &lt;strong&gt;回归分析&lt;/strong&gt; 时我们常常会遇到 &lt;strong&gt;~&lt;/strong&gt; 操作符，这对R新手来说实在是难以理解，遂查查&lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文档&lt;/a&gt;整理整理。&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/05/tildeBig.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="R" scheme="http://chenyin.top/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>Basset：CNN学习新的染色体开放位点</title>
    <link href="http://chenyin.top/bioinfo/20190423-1702.html"/>
    <id>http://chenyin.top/bioinfo/20190423-1702.html</id>
    <published>2019-04-23T07:07:47.000Z</published>
    <updated>2019-04-23T07:37:11.922Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/25.jpg" alt=""></p><p>尝试着将神经网络的元件与生物学意义联系起来。大胆假设，小心求证！</p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568/" target="_blank" rel="noopener">PMC</a>   |  <a href="https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract" target="_blank" rel="noopener">Genome Res.</a>  |  <a href="https://github.com/davek44/Basset" target="_blank" rel="noopener">GitHub</a></p><a id="more"></a><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从<a href="https://www.encodeproject.org/" target="_blank" rel="noopener">ENCODE Project Consortium</a>下载125种细胞类型的数据。<br>从<a href="http://www.roadmapepigenomics.org/" target="_blank" rel="noopener">Roadmap Epigenomics Consortium</a>下载39种细胞类型的数据。<br>数据形式为DNase-seq的peak信息，保存在<a href="https://vip.biotrainee.com/d/167-bed" target="_blank" rel="noopener">BED格式</a>的文件中。<br>使用未去重叠（overlap）的peak数据。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><ol><li>以1%的FDR使用模拟方法修改原始数据集——robustness</li><li>归并重叠的peaks共 $2,071,886$ 个峰，比对到hg19参考基因组<ul><li><strong>标准输入</strong>为每个位点600bp的DNA序列长度</li><li><strong>标准标签</strong>为一个164维的二值向量，该向量表示这个峰（位点）在164种细胞类型中的开放情况（1为开放，0为不开放）</li></ul></li><li>将数据集切分为训练集、测试集和验证集<ul><li>training data：训练模型参数</li><li>testing data：计算序列特异性参数</li><li>validation data：用于early stopping</li></ul></li><li>应模型后续分析需要，使用GENOME v18 reference catalog将位点分为promoter（转录起始位点周围2kb区域）、intragenic（与基因区域发生重叠）、intergenic（位于基因间区域内）三类</li></ol><hr><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>训练模型和测试模型的用途有细微差别。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练模型是一个3CNN+2FC的简单神经网络，此项目的亮点不在神经网络的设计，而在于将神经网络的元件与生物学意义结合起来，这也是以往项目中我所想不透的地方。训练网络如下：<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/22.jpg" alt=""></p><p>模型的输入是一个one-hot编码的 $4 \times 600$ 的序列（与处理中已经将所有位点的序列长度截取到了600bp）。</p><p>the first CNN是本模型的重点，它包括了一个卷积层、一个激活层和一个池化层。</p><p>卷积层使用的是300个 $4 \times M$ 的一维卷积核（filter，滤波器），其中 $M$ 长度跟motif长度相当（这里取19b）。作者在这里给每个filter赋予了生命力，认为它们不仅是一种网络元件。因为<u>每个卷积核是基于所有序列优化得来的</u>，所以我们认为卷积核代表了所有序列共有的一种信息，即模式。这是符合思考逻辑的。</p><p>简而言之，每个卷积核可能代表了一种motif，这个motif具体的生物学意义未知。此项目试图从蛋白质结合motif出发去验证卷积核中是否存在相应的结构与之对应。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/23.jpg" alt=""></p><p>这是一种不错的假设-验证思想。</p><p>模型的最后一层也就是第二个全连接层的输出是164个节点，分别代表了该600bp的序列在164种类型细胞种的开放情况。参考标准标签计算binary loss进行参数优化。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试阶段作者做了很有意思的事情。</p><p>为了计算卷积核（motif）对（预测的）开放性打分结果的影响程度，将第一CNN的输出结果（每个卷积核会将 $4 \times 600$ 的序列转化为（600-19+1）维的向量）全部用其平均值替换，这将消除卷积核的特异性影响。用替换结果进行后续计算，将计算结果与未经替换的预测结果进行比较。计算差异的平方和，这个值作为该卷积核的influence值。</p><p>IC（Information Content）值得原文计算方法如下：<br>$$<br>\mathrm{IC} = \sum_{i,j}{m_{ij}\log_2 m_{ij}} - \sum_{i,j}{ b_j \log_{2}b_{j}}<br>$$<br>写成这样似乎更清楚点：<br>$$<br>\mathrm{IC} = \sum_{i=1}^{19} \sum_{j=1}^4 m_i^{(j)} \log_2 m_i^{(j)} - 19\sum_{j=1}^4 b^{(j)} \log_2 b^{(j)}<br>$$</p><p>其中向量 $b$ 是四个碱基分布的背景值，向量 $m\;(4 \times 19)$ 是motif各个位置的碱基概率分布，等效于一个卷积核。</p><p>有趣的是，除了对卷积核进行上述操作计算IC值influence值，作者还直接从数据库中（CIS-BP数据库）下载已知的蛋白质motif代替卷积核进行计算，最后一同进行比较分析。<img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/24.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/25.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;尝试着将神经网络的元件与生物学意义联系起来。大胆假设，小心求证！&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PMC&lt;/a&gt;   |  &lt;a href=&quot;https://genome.cshlp.org/content/early/2016/05/03/gr.200535.115.abstract&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Genome Res.&lt;/a&gt;  |  &lt;a href=&quot;https://github.com/davek44/Basset&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
      <category term="深度学习" scheme="http://chenyin.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://chenyin.top/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="基因组开放位点" scheme="http://chenyin.top/tags/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%BC%80%E6%94%BE%E4%BD%8D%E7%82%B9/"/>
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>采样5 - M-H采样解决接受率α过小的问题</title>
    <link href="http://chenyin.top/stat/20190422-551c.html"/>
    <id>http://chenyin.top/stat/20190422-551c.html</id>
    <published>2019-04-22T02:06:51.000Z</published>
    <updated>2019-04-22T03:13:44.323Z</updated>
    
    <content type="html"><![CDATA[<p><strong>M-H采样</strong> 是 <strong>Metropolis-Hastings采样</strong> 的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。</p><p>M-H采样解决了<a href="/stat/20190417-ea8c.html">原始MCMC采样</a>中接受率过小的问题。</p><a id="more"></a><p>对于细致平稳条件<br>$$<br>\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji}<br>$$<br>既然 $\alpha_{ij}​$ 太小，我们就把它扩大到1，多么简单粗暴：<br>$$<br>\alpha(i, j)=\min \left \lbrace \frac{\pi(j) Q(j, i)}{\pi(i) Q(i, j)},\; 1 \right \rbrace<br>$$<br>M-H采样过程如下：</p><ol><li>已知平稳分布 $\pi(x)​$，预设状态转移次数阈值 $n​$ 和采样样本数 $m​$，选定任意的某个马氏链对应的状态转移矩阵 $Q(i,\;j)​$</li><li>从任意简单概率采样得 $x_0$</li><li>从 $t=1$ 到 $t=n+m-1$ 循环采样：<ol><li>从 $q(x|x_{t-1})$ 中采样得到状态 $\ast$ 的采样值 $x^\ast$</li><li>从均匀分布中采样得到 $u \sim uniform(0,\;1)$</li><li>如果 $u \lt \alpha(t-1,\;\ast)=\min \left \lbrace \frac{\pi(\ast) Q(\ast, \;t-1)}{\pi(t-1) Q(t-1,\;\ast)},\; 1 \right \rbrace$ ，则接受采样值 $x^\ast$，这意味着我们接受了状态 $t-1$ 到状态 $\ast$ 的转移，即 $x_t=x^\ast$；否则拒绝转移，即 $x_t=x_{t-1}$</li></ol></li><li>$\left \lbrace x_n,\; x_{n+1},\; \cdots,\; x_{n+m-1} \right \rbrace$ 即为平稳分布对应的采样集</li></ol><p>一般强化先验知识有助于简化计算。如果我们预设的 $Q​$ 是对称矩阵，即 $Q(i,\;j)=Q(j,\;i)​$，那么此时 $\alpha​$ 可以简化为<br>$$<br>\alpha(i,\;j) = \min \left \lbrace \frac{\pi_j}{\pi_i},\; 1 \right \rbrace<br>$$<br>尽管M-H采样解决了[原始MCMMC采样][]过小的问题，它还是有很多缺陷，特别是在特征维度很大时：</p><ol><li>接受概率 $\alpha$ 难以计算，或进行了许多的无用计算（拒绝采样）；</li><li>联合分布难以计算，相比之下条件概率分布计算更容易</li></ol><p><a href="/stat/20190422-dcdd.html">Gibbs采样</a>解决了上面的问题。</p><!-- URL -->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;M-H采样&lt;/strong&gt; 是 &lt;strong&gt;Metropolis-Hastings采样&lt;/strong&gt; 的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。&lt;/p&gt;
&lt;p&gt;M-H采样解决了&lt;a href=&quot;/stat/20190417-ea8c.html&quot;&gt;原始MCMC采样&lt;/a&gt;中接受率过小的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
  </entry>
  
  <entry>
    <title>采样4 - 细致平稳条件和MCMC采样</title>
    <link href="http://chenyin.top/stat/20190417-ea8c.html"/>
    <id>http://chenyin.top/stat/20190417-ea8c.html</id>
    <published>2019-04-17T03:07:54.000Z</published>
    <updated>2019-07-09T01:07:01.007Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/07/01.jpg" alt=""><br><a id="more"></a></p><h1 id="细致平稳条件"><a href="#细致平稳条件" class="headerlink" title="细致平稳条件"></a>细致平稳条件</h1><p><strong>&gt; 细致平稳条件（Detailed Banlance Condition）</strong></p><p>当一个系统微观上达到平衡时满足 <strong>细致平稳条件</strong>：$\pi_i​$ 表示系统处于状态 $i​$ 的概率，$P_{i{\to}j}​$ 表示系统从状态 $i​$ 转移到状态 $j​$ 的概率，一个反应（转移）应该与它的逆反应（逆转移）达到平衡，即<br>$$<br>\pi_i P_{i \to j} = \pi_j P_{j \to i} \tag{1}<br>$$<br><a href="https://en.wikipedia.org/wiki/Detailed_balance" target="_blank" rel="noopener">细致平稳条件</a> 是 Ludwig Boltzmann 于1872年提出来的。</p><p><strong>&gt; 马尔可夫链具有平稳分布的充分条件</strong></p><p>假设马尔可夫链的状态分布为 $\pi$，状态转移矩阵为 $P$，如果存在某一个时间节点使得对于任意的状态 $i$ 和 $j$ 有<br>$$<br>\pi_i P_{ij} = \pi_j P_{ji} \tag{2}<br>$$<br>上述细致平稳条件成立，则该马尔可夫链具有<strong>平稳分布</strong> $\pi$（Stationary Distribution）。</p><p>细致平稳条件是马氏链收敛的充分条件，而不是必要条件。</p><blockquote><p>充分性证明：</p><p>因为 $\sum_i \pi_i P_{ij} = \sum_i \pi_j P_{ji} = \pi_j \sum_i P_{ji} = \pi_j$</p><p>所以 $ \sum_i \pi_i P_{ij} = \pi P = \pi $</p></blockquote><p>仅在二状态系统下，细致平稳条件也是马氏链收敛的必要条件，即充要条件。</p><p><strong>&gt; 从目标平稳分布 $\pi(x)$ 出发寻找转移矩阵 $P$</strong></p><p>假设目标平稳分布 $\pi(x)$ 已知，根据细致平稳条件，我们只需要确定一个矩阵 $P$ 作为相应的状态转移概率矩阵，就能确定这样一个马尔可夫链：转移矩阵为 $P$，平稳分布为 $\pi(x)$。</p><p>但是，矩阵 $P$ 的确定并不是一件很容易的事情，MCMC采样算法很巧妙的解决了这个问题。</p><h1 id="MCMC采样"><a href="#MCMC采样" class="headerlink" title="MCMC采样"></a>MCMC采样</h1><p><strong>&gt; 马尔可夫蒙特卡洛采样（Markov Chain Monte Carlo，MCMC）的基本思想</strong></p><p>我们随机确定一个马尔可夫链的状态转移矩阵 $Q$，它并不满足细致平衡条件，即<br>$$<br>\pi_i Q_{ij} \ne \pi_j Q_{ji} \tag{3}<br>$$<br>为了使等式成立，引入非零 $\alpha_{ij}$ 项，即<br>$$<br>\pi_i Q_{ij} \alpha_{ij} = \pi_j Q_{ji} \alpha_{ji} \tag{4}<br>$$<br>取<br>$$<br>\begin{cases}<br>\alpha_{ij} &amp;= \pi_j Q_{ji} \<br>\alpha_{ji} &amp;= \pi_i Q_{ij}<br>\end{cases} \tag{5}<br>$$<br>此时目标平稳分布 $\pi(x)$ 对应的马氏链的状态转移概率矩阵 $P$ 可表示为<br>$$<br>P_{ij} = Q_{ij} \alpha_{ij} \tag{6}<br>$$<br>由(5)式可知，$0 \le \alpha_{ij}^{(k)} \le 1$，对于特定的 $Q_{ij}$，$\alpha_{ij}$ 越小，则相应的 $P_{ij}$ 越小：这可以解释为 $\alpha_{ij}$ 是对 $Q_{ij}$ 的接受率，目标状态转移矩阵 $P$ 可以通过任意一个马尔可夫链的状态转移矩阵 $Q$ 以一定概率 $\alpha$ 接受获得。</p><p><strong>&gt; MCMC采样算法过程</strong></p><blockquote><ol><li>初始化状态分布 $\pi_0$ 并采样 $x_0$</li><li>循环采样直到获得 $m$ 个样本：<ol><li>$t$ 时刻马氏链的状态分布为 $\pi_t$（未知，我们也不关系纯向量的值），从 $Q(x|x_t)$ 采样得样本 $x^*$</li><li>从均匀分布中采样 $u \sim uniform(0,\;1)​$</li><li>如果 $u &lt; \alpha_{x_t \to x^<em>} = \pi_(x^</em>)Q(x^<em>,\;x_t)$ 则接受转移 $x_t \to x^</em>$，即 $x_{t+1}=x^*$；否则不接受转移，即 $x_{t+1}=x_t$</li><li>舍弃第 $0$ 到第 $n-1$ 共 $n$ 个样本，保留第 $n$ 到第 $n+m-1$ 共 $m$ 个样本作为采样样本 </li></ol></li></ol></blockquote><p>上面的 $\alpha_{ij}$ 实际上很小，这导致大部分采样都会被拒绝，这意味着我们的 $n$ 要预设的非常大。</p><p>基于这个困境，<strong>M-H采样</strong> 对 <strong>MCMC采样</strong> 进行了优化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/07/01.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样3 - 离散马尔可夫链采样</title>
    <link href="http://chenyin.top/stat/20190416-1f84.html"/>
    <id>http://chenyin.top/stat/20190416-1f84.html</id>
    <published>2019-04-16T08:03:14.000Z</published>
    <updated>2019-04-17T05:45:25.680Z</updated>
    
    <content type="html"><![CDATA[<p>如果我们的目标分布是<strong>简单</strong>、<strong>离散</strong>的分布，这个目标分布可以当作某个马尔可夫链的<strong>平稳分布</strong>，这个平稳分布对应着一个<strong>状态转移矩阵</strong>。如果这个状态转移矩阵已知，我们就可以十分容易的得到目标分布的采样集。</p><p>离散马尔可夫链采样只是理论基础，实用价值不大。</p><a id="more"></a><h1 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h1><p>下面直接以例子说明采样过程：</p><p>假设一个质量分布不均匀的六面骰子掷出六个点数的概率服从一个简单、离散的概率分布，记为 $\pi(x)$，$x=\lbrace 1\;2\;3\;4\;5\;6 \rbrace$。</p><p>$\pi(x)$ 可以通过大量独立重复实验用频率进行估计，如果知道了状态转移矩阵 $P$ 就可以很容易的进行采样。</p><p>先假设转移概率矩阵 $P$ 已知，这里模拟一个状态转移矩阵作为示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">36</span>).reshape([<span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">p = p/p.sum(axis=<span class="number">1</span>).reshape([<span class="number">-1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>$$<br>p = \left[\begin{matrix}<br>0.18206229 &amp; 0.1764297  &amp; 0.23015706 &amp; 0.13247823 &amp; 0.23143627 &amp;0.04743645 \\<br>0.03751516 &amp; 0.33564335 &amp; 0.39400589 &amp; 0.12847041 &amp; 0.07526389 &amp;0.0291013  \\<br>0.12921795 &amp; 0.25957537 &amp; 0.01825006 &amp; 0.32088546 &amp; 0.26899888 &amp;0.00307228 \\<br>0.22883623 &amp; 0.07873173 &amp; 0.23422034 &amp; 0.23744235 &amp; 0.08045652 &amp;0.14031283 \\<br>0.14770889 &amp; 0.20742323 &amp; 0.19634175 &amp; 0.07048638 &amp; 0.16482596 &amp;0.21321379 \\<br>0.07080479 &amp; 0.24460837 &amp; 0.22450135 &amp; 0.21595409 &amp; 0.11542076 &amp;0.12871064<br>\end{matrix}\right]<br>$$<br><strong>&gt; 1. 指定初始分布</strong></p><p>因为马尔可夫链的平稳分布只与状态转移矩阵相关，而与初始状态分布无关。</p><p>所以我们可以从一个从任意初始化状态分布出发进行采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pi = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">6</span>)</span><br><span class="line">pi = pi / pi.sum()</span><br></pre></td></tr></table></figure><p>$$<br>\pi_0 = [0.22081522,0.11290264,0.23593329,0.0524546,0.1917839,0.18610888]<br>$$</p><p><strong>&gt; 2. 指定记录采样值时的状态转移次数 $n$ 和需要记录的采样数 $m$</strong></p><p>整个采样过程中需要进行的状态转移次数为<br>$$<br>N= n+m-1<br>$$<br>将要记录下的采样值集合为<br>$$<br>X = \lbrace \underbrace{x_n,\;x_{n+1},\;\cdots,\;x_{n+m-1}}_{\text{共 $$m 项}} \rbrace<br>$$<br>采样集合 $X$ 即为平稳分布 $\pi(x)$ 对应的样本集。</p><p>实际应用中，$n$ 凭经验确定。</p><p><strong>&gt; 3. 从初始分布采样</strong></p><p>从初始分布采样的意思是：从简单分布（例如均匀分布）采样，以初始分布的确定采样值（常见的方法是以初始分布的累积分布函数确定采样值）。</p><p>计算累积分布函数（python中调用 <code>np.cumsum(arr)</code>）：<br>$$<br>F_0 = [0.22081522,0.33371785,0.56965314,0.62210774,0.81389112,1]<br>$$<br>从[0,1]均匀分布采样一次（<code>np.random.uniform(0, 1)</code>）得 $p_0 = 0.27364499172190815​$</p><p>因为 $F_0^{(1)}&lt;p_0&lt;F_0^{(2)}​$，我们认为本次采样值 $x_1=2​$，即本次骰子之出来的点数是2。</p><p><strong>&gt; 4. 基于 $P(x|x_1)​$ 进行第二次采样</strong></p><p>$P(x|x_1=2)$ 即从状态转移矩阵 $P$ 的第二行采样<br>$$<br>P(x|x_1) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]<br>$$<br>从[0,1]均匀分布采样一次得 $p_1 = 0.23181961317728317$，$x_2=2$。</p><p><strong>&gt; 5. 基于 $P(x|x_2)​$ 进行第三次采样</strong></p><p>$P(x|x_2=2)$ 即从状态转移矩阵 $P$ 的第二行采样<br>$$<br>P(x|x_2) = [ 0.03751516\;0.33564335\;0.39400589\;0.12847041\;0.07526389\;0.0291013 ]<br>$$<br>从[0,1]均匀分布采样一次得 $p_2= 0.5880393198555685​$，$x_3=3​$。</p><p><strong>&gt; 基于 $P(x|x_{n-1})$ 进行第 $n$ 次采样</strong> </p><p>$P(x|x_{n-1})$ 即从状态转移矩阵 $P$ 的第 $x_{n-1}$ 行采样，得到第 $n$ 个采样值 $x_n$，这个采样值将作为我们第一个目标样本。也就是说前面的 $n-1$ 个采样会被舍弃掉。</p><p>继续采样，直到获取第 $n+m-1$ 个采样值。</p><h1 id="挠头"><a href="#挠头" class="headerlink" title="挠头"></a>挠头</h1><p><strong>Q1 由上知是从矩阵 $P$ 而不是每个状态分布 $\pi$ 中采样，因为不论状态转移多少次，矩阵 $P$ 总是不变的，为什么不直接从矩阵 $P$ 中采样，而是从初始分布采样舍弃 $n-1$ 个样本？</strong></p><p>这个问题看似复杂，其实很简单：因为总是选取矩阵 $P$ 的某一行采样，至于要选取哪一行取决于上一次的采样结果。那么，<strong>第一次使用矩阵 $P$ 进行采样</strong>（区别于<strong>第一次采样</strong>）应该选取哪一行呢？这是不确定的。因此第一次采样我们从指定的初始分布采样，第二次采样（即第一次使用矩阵 $P$ 进行采样）基于第一次采样结果进行。</p><p><strong>Q2 为什么要舍弃前面的 $n-1$ 个采样值？</strong></p><p>因为初始分布是我们自己定义的，而采样值服从的分布应该是平稳分布而不是初始分布或者达到平稳分布前的任何一个分布。</p><p>$n$ 一般是人为设定的阈值，代表第 $n$ 次转移时已经达到平稳分布，显然这不是一个十分精确的阈值，但是在实际应用过程中影响不大。</p><p>状态的分布伴随着采样过程而改变，这是一个隐藏的过程。当状态分布收敛时的采样值集为平稳分布的采样值。（这句话很玄学，暂时没太理解！）</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上面的马尔可夫链采样的基础是状态概率矩阵 $P$ 已知，但是实际上 $P$ 是很难求出来的，即我们知道的只有平稳分布 $\pi(x)$。这似乎是一个死循环~</p><p>MCMC采样巧妙地解决了这个问题。</p><p>M-H采样和Gibbs采样是MCMC采样地改进版。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果我们的目标分布是&lt;strong&gt;简单&lt;/strong&gt;、&lt;strong&gt;离散&lt;/strong&gt;的分布，这个目标分布可以当作某个马尔可夫链的&lt;strong&gt;平稳分布&lt;/strong&gt;，这个平稳分布对应着一个&lt;strong&gt;状态转移矩阵&lt;/strong&gt;。如果这个状态转移矩阵已知，我们就可以十分容易的得到目标分布的采样集。&lt;/p&gt;
&lt;p&gt;离散马尔可夫链采样只是理论基础，实用价值不大。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样2 - 离散马尔可夫链的几个性质</title>
    <link href="http://chenyin.top/stat/20190416-ea6c.html"/>
    <id>http://chenyin.top/stat/20190416-ea6c.html</id>
    <published>2019-04-16T03:05:25.000Z</published>
    <updated>2019-07-09T01:12:19.268Z</updated>
    
    <content type="html"><![CDATA[<p>互通、可约、周期、常返、遍历<br><a id="more"></a></p><h1 id="状态间的互通性"><a href="#状态间的互通性" class="headerlink" title="状态间的互通性"></a>状态间的互通性</h1><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/15.jpg" alt=""></p><p>互通性的三个性质：</p><ol><li>自返律（假设状态有<strong>自环</strong>）：$\mathbf{i} \leftrightarrow \mathbf{i}​$</li><li>对称律：$\mathbf{i} \leftrightarrow \mathbf{j}​$当且仅当$\mathbf{j} \leftrightarrow \mathbf{i}​$</li><li>传递律：如果$\mathbf{i} \leftrightarrow \mathbf{k}$且$\mathbf{k} \leftrightarrow \mathbf{j}$，那么$\mathbf{i} \leftrightarrow \mathbf{j}$ </li></ol><h1 id="链的可约性"><a href="#链的可约性" class="headerlink" title="链的可约性"></a>链的可约性</h1><p><strong>不可约性</strong>：如果一个马氏链的任意两个状态都互通，则这个马氏链不可约；否则可约。</p><p>不可约的马氏链：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/16.jpg" alt=""></p><p>可约的马氏链：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/17.jpg" alt=""></p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/18.jpg" alt=""></p><h1 id="状态的周期性"><a href="#状态的周期性" class="headerlink" title="状态的周期性"></a>状态的周期性</h1><p>状态的周期性有个挠头的定义（$from$ 张波《应用随机过程》）：</p><blockquote><p>记$d_i$为数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$的最大公约数，则称它为状态$i$的周期。</p><p>若对一切$n{\ge}1$有$p_{i i}^{(n)}=0$，则约定$d_{i}=\infty$。</p><p>当$d_i&gt;1​$时，称$i​$是有周期的状态；当$d_i=1​$时，称$i​$是非周期的状态。</p></blockquote><p>数集$\lbrace{n : n \geq 1, p_{i i}^{(n)}&gt;0}\rbrace$指的是从状态$i$出发再次回到状态$i​$的<strong>步数</strong>的集合。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/19.jpg" alt=""></p><p>上图状态1再次回到状态1的方式有：</p><ol><li>$1{\to}1$：步数为1</li><li>$1\to2\to1$：步数为2</li><li>重复方式1，或者重复方式2，或者方式1和方式2的组合，例如$1\to1\to1$，$1\to2\to1\to1$</li></ol><p>因此状态1回到状态1的步数集合是{1, 2, 3, 4, 5, …}，$d_i=1$，状态1是非周期的；</p><p>状态2回到状态2的基本方式是{$2\to1\to2$：2步，$2\to1\to1\to2$：3步}，步数集合是{2，3，4，5，…}，最大公约数$d_i=1$，因此状态2是非周期的。</p><p>再看下图：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/21.jpg" alt=""></p><p>状态1自返的基本方式是：</p><ul><li>$1\to2\to3\to4\to1$，步数为4</li><li>$1\to5\to6\to7\to8\to9\to1$，步数为6</li></ul><p>状态1自返的步数集合是{4，6，8，10，…}，最大公约数$d_i=2$，所以状态1是周期的。</p><p>状态2，3，4的步数集合是{4，8，10，12，14，…}，都是周期的。</p><p>状态5，6，7，8，9的步数集合是{6，10，12，18，…}，都是周期的。</p><p>如何简单判断一个状态是<strong>周期/非周期</strong>的？</p><ul><li>带有<strong>自环</strong>的状态一定是非周期的（因为$d_i=1$），但不是所有非周期的状态都有自环，如下条</li><li>与非周期状态互通的状态一定是非周期的</li></ul><h1 id="状态的常返性"><a href="#状态的常返性" class="headerlink" title="状态的常返性"></a>状态的常返性</h1><p><strong>常返性</strong>即：马氏链由一个状态出发之后能否再次回归到本状态的特性。</p><p>常返性分为三种：</p><ul><li><strong>正</strong>常返（必定会返回，平均返回时间为有限值）</li><li><strong>零</strong>常返（必定会返回，平均返回时间为 $\infty​$ ）</li><li><strong>非</strong>常返（可能不再返回）</li></ul><p>定义略</p><p><strong>不可约马氏链的状态一致性</strong>定理：不可约马氏链的状态集全为正常返，或者全为零常返，或者全为非常返，并且每个状态的周期相同。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/20.jpg" alt=""></p><p>上图不可约马氏链中：</p><ul><li>如果 $p&lt;q$，那么全为正常返；</li><li>如果 $p=q$，那么全为零常返；</li><li>如果 $p&gt;q$，那么全为非常返。</li></ul><h1 id="状态-链的遍历性"><a href="#状态-链的遍历性" class="headerlink" title="状态/链的遍历性"></a>状态/链的遍历性</h1><p>如果齐次马氏链中的某个状态是非周期、正常返状态，称这个状态是<strong>可遍历</strong>的。</p><p>如果马氏链所有状态全互通（不可约）、可遍历（非周期、正常返），称这个马氏链为<strong>遍历链</strong>。</p><p>遍历链存在一个<strong>平稳分布</strong>：</p><ul><li>平稳分布与初始状态无关</li><li>平稳分布是唯一的</li><li>平稳分布全部大于0</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;互通、可约、周期、常返、遍历&lt;br&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>采样1 - 逆变换采样和拒绝采样</title>
    <link href="http://chenyin.top/stat/20190415-a232.html"/>
    <id>http://chenyin.top/stat/20190415-a232.html</id>
    <published>2019-04-15T09:27:14.000Z</published>
    <updated>2019-04-22T01:40:45.196Z</updated>
    
    <content type="html"><![CDATA[<p><strong>蒙特卡洛方法（Monte Carlo Method）</strong>尝试利用计算机模拟随机数（伪随机数）解决一类问题，这类问题通常是：1. 所求解的问题本身具有内在随机性，例如中子与原子核的相互作用受量子力学规律的制约；2. 所求解问题可以转化为某种随机分布的特征数，例如通过撒豆子的方式计算不规则图形的面积。蒙特卡洛法是一种以概率统计理论为指导的数值计算方法。</p><p><strong>抽样</strong>（<strong>采样</strong>）指从总体中抽取一部分作为样本。计算机模拟中，抽样意味着从一个概率分布中生成一个观察值，这涉及到一个随机的过程。一般认为计算机只能进行均匀分布的采样，对于复杂的概率分布，需要进行采样方法设计。</p><a id="more"></a><h1 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h1><p>从图像上理解连续型随机变量的采样是个什么玩意儿十分形象：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/10.jpg" alt=""><br>左图是正态分布的<strong>概率密度函数（Probability Density Function，PDF）</strong>记为$f(x)​$，右图是正态分布的<strong>累积分布函数（Cumulative Distribute Function，CDF）</strong>记为$F(x)​$。随机变量的采样就是在$[0,1]​$均匀分布采样的基础上，选取尽可能分散的点，使这些点尽可能地拟合CDF曲线。</p><!-- 当$F(x)$的反函数$F^{-1}(u)$的定义域始终是$[0,1]$。如果我们在$F^{-1}(u)$的定义域上多次均匀采样，此时可以认为相邻采样点的横坐标之差相同，反函数曲线变化趋势只与相邻点的纵坐标相关。这样，我们基于$[0,1]$的均匀采样产生了一组数（或者说一组点），这组点很好的拟合了反函数$F^{-1}(u)$，这就意味着这组点也能很好拟合$F(x)$。相应的，这组点的纵坐标取值集合就是我们针对$F(x)$的一组采样。--><h1 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h1><p>逆变换采样虽然简单有效，但是其应用场景十分有限：当累积分布函数或者反函数难求时，而实际情况往往是这样。</p><p>下图中的$f(x)$是我们采样的目标PDF，当其CDF或者CDF的反函数不容易求的时就不能直接对$f(x)$进行采样。<strong>拒绝采样（Rejection Sampling）</strong>的基本思想是借助这样一个参考概率密度函数$f_r(x)$即下图中的$Mg(x)$：</p><ul><li>$f_r(x)$十分容易进行采样，例如取均匀分布意味着参考PDF可以直接进行逆变换采样</li><li>$f_r(x)$位于$f(x)$上方，即对任意$x$有$f_r(x){\ge}f(x)$</li><li>$Mg(x)$表示将均匀分布$g(x)$向上移动，此时以$f(x)$的极大值确定$M$的值效果比较好</li></ul><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/11.jpg" width="50%"></p><p>从图上来看，参考PDF“罩住”了目标PDF：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/12.jpg" width="50%"></p><p>拒绝采样的过程如下：</p><ol><li><p>从$f_r(x)$进行一次采样$x_i$</p></li><li><p>计算$x_i$的<strong>接受概率</strong>$\alpha$（Acceptance Probability）：<br>$$<br>\alpha=\frac{f\left(x_{i}\right)}{f_r\left(x_{i}\right)}<br>$$</p></li><li><p>从$(0,1)$均匀分布中进行一次采样$u$</p></li><li>如果$\alpha{\ge}u$，接受$x_i$作为一个来自$f(x)$的采样；否则，重复第1步</li></ol><p>显然对于特定的目标PDF，参考PDF不止一个，不同PDF的$\max(\alpha)$不同。以均匀分布采样为例，当参考PDF从上面越靠近目标PDF采样效率越高，相应的寻找这样的参考PDF的难度就越大。采样效率高意味着对于那些概率密度较小的区域有更大的几率能够采样到。</p><p>为了平衡采样效率和参考PDF的确定难度，提出了<strong>自适应拒绝采样</strong>。</p><h1 id="自适应拒绝采样"><a href="#自适应拒绝采样" class="headerlink" title="自适应拒绝采样"></a>自适应拒绝采样</h1><p>当参考PDF不能很好的“罩住”目标PDF时，那些未罩住区域内的采样点被拒绝的概率就会很大，采样效率低。所以如果能够找到一个跟目标PDF非常接近的参考PDF，即参考PDF计划能够完全从上面贴合目标PDF，此时能够达到较好的采样效率。</p><p>当目标PDF是<strong>log-concave函数</strong>时可以采用<strong>自适应拒绝采样（Adaptive Rejection Sampling，ARS）</strong>。</p><blockquote><p><strong>log-concave函数</strong>：当概率密度函数$f(x)$是凹函数（concave）且$\log{f(x)}$仍然是凹函数时，$f(x)$称之为log-concave函数：<br>$$<br>f(\theta x+(1-\theta) y) \geq \theta f(x)+(1-\theta) f(y) \<br>\log f(\theta x+(1-\theta) y) \geq \theta \log f(x)+(1-\theta) \log f(y)<br>$$</p></blockquote><p>在log-concave函数上随机选取一些点做切线：</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/13.jpg" width="50%"></p><p>将log-concave函数变换回原来的PDF，此时上图的切线将变成曲线（取指数），它！们！弯！了！</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/14.jpg" width="50%"></p><p>将这组弯了的“切线”组成成一个分段函数，这个分段函数将会很好的贴合目标PDF。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;蒙特卡洛方法（Monte Carlo Method）&lt;/strong&gt;尝试利用计算机模拟随机数（伪随机数）解决一类问题，这类问题通常是：1. 所求解的问题本身具有内在随机性，例如中子与原子核的相互作用受量子力学规律的制约；2. 所求解问题可以转化为某种随机分布的特征数，例如通过撒豆子的方式计算不规则图形的面积。蒙特卡洛法是一种以概率统计理论为指导的数值计算方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;抽样&lt;/strong&gt;（&lt;strong&gt;采样&lt;/strong&gt;）指从总体中抽取一部分作为样本。计算机模拟中，抽样意味着从一个概率分布中生成一个观察值，这涉及到一个随机的过程。一般认为计算机只能进行均匀分布的采样，对于复杂的概率分布，需要进行采样方法设计。&lt;/p&gt;
    
    </summary>
    
      <category term="stat" scheme="http://chenyin.top/categories/stat/"/>
    
    
      <category term="采样" scheme="http://chenyin.top/tags/%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>git疑难杂症</title>
    <link href="http://chenyin.top/bug/20190415-bb05.html"/>
    <id>http://chenyin.top/bug/20190415-bb05.html</id>
    <published>2019-04-15T08:14:20.000Z</published>
    <updated>2019-04-15T08:18:08.016Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/09.jpg" alt=""></p><p><strong>报错关键词</strong>：Updates were rejected because a pushed branch tip is behind its remote counterpart.</p><p><strong>原因分析</strong>：直接在远程master分支进行修改或者有其他人修改后已经提交到了远程master，而本地使用test（非本地master分支）分支进行再次修改后直接push到远程master分支，此时本地master分支的版本还是远程master分支修改之前的版本，即本地master的版本落后于远程master的版本，因此导致push失败。</p><p><strong>解决办法</strong>：先 <code>git checkout master</code> 到本地master分支，再 <code>git pull 远程仓库 master</code> 拉取最新版本，再 <code>git checkout test</code> 回到本地工作分支，再 <code>git push 远程仓库 master</code> 推送最新版本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/09.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;报错关键词&lt;/strong&gt;：Updates were rejected b
      
    
    </summary>
    
      <category term="bug" scheme="http://chenyin.top/categories/bug/"/>
    
    
  </entry>
  
  <entry>
    <title>python3中的字符串与编码问题</title>
    <link href="http://chenyin.top/python/20190411-d7e5.html"/>
    <id>http://chenyin.top/python/20190411-d7e5.html</id>
    <published>2019-04-11T06:33:52.000Z</published>
    <updated>2019-04-14T16:02:20.276Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/05.jpg" alt=""></p><p>python3的编码方式比python2已经简单很多了，不过还是让我等菜鸟头疼。廖老师的教程写的着实通俗易懂，恍然大悟。</p><a id="more"></a><h1 id="ASCII、Unicode和UTF-8"><a href="#ASCII、Unicode和UTF-8" class="headerlink" title="ASCII、Unicode和UTF-8"></a>ASCII、Unicode和UTF-8</h1><p>所有的字符串都会拆分成单个字符进行传输和存储。对字符串编码不仅要考虑字符集的完整性、不同语言字符集的兼容性，还要考虑传输存储空间大小。</p><p><strong>ASCII</strong></p><p>ASCII全称是American Standard Code for Information Interchange (美国信息交换标准代码)。</p><p>计算机是美国人发明的，因此早期的ASCII编码只考虑到26个英文字母和一些简单的字符。由于数量较少，我们只需要用<strong>一个字节</strong>就可以完成编码，所以ASCII编码对应的整数值从0到255。</p><p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/ascii.png" alt=""></p><p><strong>Unicode</strong></p><p>ASCII编码不能用来编码中文字符，当然也不能用来编码日文字符、韩文字符和阿拉伯文字符等等等等。如果每个国家都为了是计算机能识别自己特有的字符而使用自己的编码方式，当处理多国语言文本时就会出现许多问题。所以大家统一规定了Unicode编码。换句话说，Unicode编码兼容地球上所有国家的语言符号！真是一个令人振奋的消息~</p><p>一般情况下，Unicode使用<strong>两个字节</strong>对字符进行编码（对于某些十分偏僻的字符可能要用到更多的字节）。对于原来的ASCII编码的字符，在它们的ASCII编码（二进制编码）前加上一个字节的0（即8个0）即可完美转化为Unicode编码。</p><table><thead><tr><th style="text-align:center">字符</th><th style="text-align:center">ASCII编码</th><th style="text-align:center">Unicode编码</th><th style="text-align:center">UTF-8编码</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center"><code>01000001</code></td><td style="text-align:center"><code>00000000 01000001</code></td><td style="text-align:center"><code>01000001</code></td></tr><tr><td style="text-align:center">中</td><td style="text-align:center">不能编码</td><td style="text-align:center"><code>01001110 00101101</code></td><td style="text-align:center"><code>11100100 10111000 10101101</code></td></tr></tbody></table><p>好了，现在通过Unicode能够兼容所有语言的字符集了。但是又出现了新的问题：当我的文本（几乎）是全英文是，如果直接使用Unicode编码文件，其传输的字节数量和占用的磁盘空间都是ASCII编码的两倍，显然十分多余。为了解决这个问题，又出现了UTF-8编码。</p><p><strong>UTF-8</strong></p><p>UTF-8编码是不定长的编码方式：那些可以用ASCII编码的字符转化成UTF-8编码时仍然只有一个字节，UTF-8编码的汉字一般是三个字节，少数特殊字符所用的字节数更多。所以，UTF-8是完美兼容ASCII编码的，只支持ASCII编码的应用仍然可以在UTF-8编码上运行。</p><p><strong>总结</strong></p><p>总结以下：</p><ol><li>ASCII编码只适用于英文字符，固定单字节；</li><li>Unicode编码是内存中的编码方式，固定多字节；</li><li>UTF-8编码优化了Unicode编码，不定字节；</li></ol><h1 id="计算机编码工作方式"><a href="#计算机编码工作方式" class="headerlink" title="计算机编码工作方式"></a>计算机编码工作方式</h1><p>字符串所在的场景不外乎两种：</p><ol><li>当我们在python程序中处理字符串时，它们存储在内存中，此时使用Unicode编码字符串；</li><li>当我们存储字符串时往往使用UTF-8等编码方式（建议只适用UTF-8编码存储文件）。</li></ol><p><strong>场景1：记事本编辑文件</strong></p><p>文本文件通常以UTF-8编码存储在磁盘上，记事本应用读取文件到内存中，此时字符串改用Unicode编码，当我们编辑完成之后需要使用UTF-8编码文件保存到磁盘。</p><p><strong>场景2：网页浏览</strong></p><p>当我们浏览某些网页的时候，服务器会自动生成信息，这些字符串是Unicode编码；服务器使用UTF-8编码这些字符串传输至浏览器并显示。</p><h1 id="python3中的字符串"><a href="#python3中的字符串" class="headerlink" title="python3中的字符串"></a>python3中的字符串</h1><p>python中字符串类型是<code>str</code>，它们以Unicode方式编码。</p><p><strong>字符</strong>通过Unicode编码可以与数字（二进制、十进制、十六进制…）进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取字符的Unicode编码的十进制整数值</span></span><br><span class="line">ord(<span class="string">'A'</span>) <span class="comment"># 65</span></span><br><span class="line">ord(<span class="string">'中'</span>) <span class="comment"># 20013</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 将十进制整数通过Unicode编码方式转化为字符</span></span><br><span class="line">chr(<span class="number">65</span>) <span class="comment"># 'A'</span></span><br><span class="line">chr(<span class="number">20013</span>) <span class="comment"># '中'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 注意：</span></span><br><span class="line"><span class="comment"># ord()只能转换字符，传递字符串会报错</span></span><br><span class="line"><span class="comment"># 传入()接收的整数如果超过Unicode编码的范围也会报错</span></span><br></pre></td></tr></table></figure><p>我们在程序中处理的字符串都是Unicode编码的，一个字符占据了多个字节。</p><p>当进行网络传输或者本地存储时，<code>str</code>类型的字符串需要被转换为字节数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单单一个'中'是Unicode编码的，直观上就没有反映出字节信息</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad'</span></span><br><span class="line"><span class="comment"># 从encode结果我们可以看到'中'包含了三个字节</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="string">b'\xd6\xd0'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面两种方式军会报错LookupError</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'unicode'</span>) <span class="comment"># 因为'中'本来就是Unicode编码了</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中'</span>.encode(<span class="string">'ascii'</span>) <span class="comment"># ASCII不能编码汉字字符</span></span><br></pre></td></tr></table></figure><p>python中</p><ol><li><p><code>str.encode(CODING_TYPE)</code>方法可以将Unicode字符串转换成CODING_TYPE编码的字符串；</p></li><li><p><code>bytes.decode(ORIGIN_TYPE)</code>方法将ORIGIN_TYPE编码的字节串解码成Unicode字符串。</p></li><li><p>encode的参数是目标编码方式，decode的参数是源字节串的编码方式。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad'</span>.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'中'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xd6\xd0'</span>.decode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="string">'中'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## .decode(..., errors='ignore')可忽略掉部分不能正确解码的字节</span></span><br><span class="line"><span class="comment"># \xe4\xb8\xad 被忽略</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xd6\xd0'</span>.decode(<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># \xd6\xd0 被忽略</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xd6\xd0'</span>.decode(<span class="string">'gbk'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 下面两种是错误的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad'</span>.decode(<span class="string">'gbk'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xd6\xd0'</span>.decode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p><code>len(...)</code>函数接收<strong>字符串</strong>时计算的是字符串的<strong>字符数目</strong>，接收<strong>字节串</strong>时计算的是字节串的<strong>字节数</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">len(<span class="string">'中国'</span>) <span class="comment">#=&gt; 2</span></span><br><span class="line">len(<span class="string">'中国'</span>.encode(<span class="string">'utf-8'</span>)) <span class="comment">#=&gt; 6，等价于 len(b'\xe4\xb8\xad\xe5\x9b\xbd')</span></span><br><span class="line">len(<span class="string">'中国'</span>.encode(<span class="string">'gbk'</span>)) <span class="comment">#=&gt; 4，等价于 len(b'\xd6\xd0\xb9\xfa')</span></span><br></pre></td></tr></table></figure><p><strong>.py源文件</strong>也是文本，我们在保存是应该保存为UTF-8编码。为了使python解释器能正确读取源文件，我们需要指定解释器读取源文件的编码方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure><p>notepad++应该选择 <strong>Encoding in UTF-8 without BOM</strong> 编码文件！</p><h1 id="中文文件的读写"><a href="#中文文件的读写" class="headerlink" title="中文文件的读写"></a>中文文件的读写</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding参数指定原文件的编码格式，可设置errors='ignore'</span></span><br><span class="line"><span class="keyword">with</span> open(INPUT_FP, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    OUTPUT_STRING = ...</span><br><span class="line">    </span><br><span class="line"><span class="comment"># encoding指定新文件的编码格式</span></span><br><span class="line"><span class="keyword">with</span> open(OUPUT_FP, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    writer.write(OUTPUT_STRING)</span><br></pre></td></tr></table></figure><p>参考自<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431664106267f12e9bef7ee14cf6a8776a479bdec9b9000" target="_blank" rel="noopener">廖雪峰的python教程-python基础-字符串和编码</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/05.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;python3的编码方式比python2已经简单很多了，不过还是让我等菜鸟头疼。廖老师的教程写的着实通俗易懂，恍然大悟。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://chenyin.top/categories/python/"/>
    
    
  </entry>
  
  <entry>
    <title>灵感源自macaron让人十分舒适的十种颜色</title>
    <link href="http://chenyin.top/cargo/20190410-3bf3.html"/>
    <id>http://chenyin.top/cargo/20190410-3bf3.html</id>
    <published>2019-04-10T03:09:17.000Z</published>
    <updated>2019-04-14T16:02:20.846Z</updated>
    
    <content type="html"><![CDATA[<p>Macaron是一种用蛋白、杏仁粉、白砂糖和糖霜制作，并夹有水果酱或奶油的法式甜点。口感丰富，外脆内柔，外观五彩缤纷，精致小巧。Macaron 10 色因为它们的视觉舒适而广为流传。</p><p><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/01.jpg" alt=""></p><a id="more"></a><table><thead><tr><th style="text-align:center">英文名</th><th style="text-align:center">代码</th><th style="text-align:center">视觉改释</th></tr></thead><tbody><tr><td style="text-align:center">bewitched tree</td><td style="text-align:center"><code>#19CAAD</code> 或 <code>rgb(19,202,173)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#19CAAD"></div></td></tr><tr><td style="text-align:center">mystical green</td><td style="text-align:center"><code>#8CC7B5</code> 或 <code>rgb(140,199,181)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#8CC7B5"></div></td></tr><tr><td style="text-align:center">light heart blue</td><td style="text-align:center"><code>#A0EEE1</code> 或 <code>rgb(160,238,225)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#A0EEE1"></div></td></tr><tr><td style="text-align:center">glass gall</td><td style="text-align:center"><code>#BEE7E9</code> 或 <code>rgb(190,231,233)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#BEE7E9"></div></td></tr><tr><td style="text-align:center">silly fizz</td><td style="text-align:center"><code>#BEEDC7</code> 或 <code>rgb(190,237,199)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#BEEDC7"></div></td></tr><tr><td style="text-align:center">brain sand</td><td style="text-align:center"><code>#D6D5B7</code> 或 <code>rgb(214,213,183)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#D6D5B7"></div></td></tr><tr><td style="text-align:center">mustard addicted</td><td style="text-align:center"><code>#D1BA74</code> 或 <code>rgb(209,186,116)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#D1BA74"></div></td></tr><tr><td style="text-align:center">magic powder</td><td style="text-align:center"><code>#E6CEAC</code> 或 <code>rgb(230,206,172)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#E6CEAC"></div></td></tr><tr><td style="text-align:center">true blush</td><td style="text-align:center"><code>#ECAD9E</code> 或 <code>rgb(236,173,158)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#ECAD9E"></div></td></tr><tr><td style="text-align:center">merry cranesbill</td><td style="text-align:center"><code>#F4606C</code> 或 <code>rgb(244,96,108)</code></td><td style="text-align:center"><div style="display:inline-block;width:208px;height:30px;background:#F4606C"></div></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Macaron是一种用蛋白、杏仁粉、白砂糖和糖霜制作，并夹有水果酱或奶油的法式甜点。口感丰富，外脆内柔，外观五彩缤纷，精致小巧。Macaron 10 色因为它们的视觉舒适而广为流传。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/04/01.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="cargo" scheme="http://chenyin.top/categories/cargo/"/>
    
    
  </entry>
  
  <entry>
    <title>降维04 - TSNE引领时尚</title>
    <link href="http://chenyin.top/ml/20190328-acd8.html"/>
    <id>http://chenyin.top/ml/20190328-acd8.html</id>
    <published>2019-03-28T03:12:08.000Z</published>
    <updated>2019-04-23T07:23:06.304Z</updated>
    
    <content type="html"><![CDATA[<div style="display: inline-block; width: 670px; height: 456px; overflow: hidden; border: 1px solid #ddd"><br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.gif" style="margin-top: -114px !important; border: none"><br></div><p><strong>t-SNE</strong> (<strong>t-distributed Stochastic Neighbor Embedding</strong>) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。</p><a id="more"></a><p><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/02.png" alt=""></p><p>相比于SNE，t-SNE的主要优化有：联合概率替代条件概率、低维空间下使用t分布代替高斯分布。<a href="#ref1">[1]</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>早期的<strong>可视化 (Visualization)</strong> 工具<u>不负责解释数据</u>，这就限制了这些工具在真实世界数据上的应用，因为我们要想解释数据，我们还是只能靠人眼看。相比于能解释数据的监督学习而言，可视化只需要展示训练数据，而不需要训练模型使它能够拟合到测试数据集。可视化的任务简单许多。<a href="#ref2">[2]</a></p><h2 id="线性降维"><a href="#线性降维" class="headerlink" title="线性降维"></a>线性降维</h2><p> 将数据从高位空间映射到低维空间的过程我们称之为 <strong>map</strong>，相应的，低维空间中的映射点被称之为 <strong>map points</strong>。降维算法已经注意到，要将高维空间中的数据结构问题尽可能的保留到低维空间。</p><p> 但是传统的<strong>线性降维 (Linear dimentionality reduction)</strong>  算法，例如PCA、MDS，更加侧重<u>在低维空间中保持高维空间中的<strong>差异性</strong></u>，即尽可能地分开数据。同时它们更加关注数据地<strong>全局特征</strong>，这点与非线性降维算法显著不同。</p><h2 id="非线性降维"><a href="#非线性降维" class="headerlink" title="非线性降维"></a>非线性降维</h2><p>大部分<strong>非线性降维 (non-linear dimentionality reduction)</strong> 算法关注的是 <u>在低维空间中保持高维空间地<strong>局部特征</strong></u>。这就意味着，它们不能同时关注数据的全局特征和局部特征。全局特征就是基于所有数据进行的解释，比如聚类结果就是基于所有数据进行的，理想情况下每个数据点都能找到它自己所属的类；局部特征只是基于部分数据点进行的推导，比如在SNE算法中，总是计算离中心点欧式距离小的部分点进行下降，它关注的是以中心点为圆心，以有限长度为半径的（超）球体内的点。</p><p>下面7个常见的非线性降维算法，它们在局部特征提取上都是很优秀的：Sammon mapping <a href="#ref3">[3]</a>, CCA <a href="#ref4">[4]</a>, SNE <a href="#ref5">[5]</a>, Isomap <a href="#ref6">[6]</a>, MVU <a href="#ref7">[7]</a>, LLE <a href="#ref8">[8]</a>, Laplacian Eigenmaps <a href="#ref9">[9]</a>。</p><h2 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h2><p>t-SNE继承自SNE算法，同样是非线性降维，它的优势在于：能够保持大部分局部特征到低维空间，同时不丢失全局特征（例如聚类）。</p><p>与SNE一样，t-SNE的思想还是计算两个点间的<strong>相似度</strong> (similarity)。</p><h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1><p>SNE尽管能得到比较好的可视化结果，但是它的损失函数难以优化，并且还存在 <strong>crowding problem (拥挤问题)</strong> 。相比之下，t-SNE能缓和上面提到的所有问题（优化问题和拥挤问题），与SNE相比，t-SNE主要在两个方面进行改进：<br>1.使用<strong>对称</strong>的损失函数，新的损失函数求导会更加容易。<a href="#ref10">[10]</a><br>2.计算低维空间中两点的相似度使用<strong>t分布</strong>而不是高斯分布，t分布是一种<a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution" target="_blank" rel="noopener"><strong>重尾分布 (heavy-tailed distribution)</strong></a>，它能够有效缓解拥挤问题和优化问题，后面将会详细介绍。</p><h2 id="优化SNE成对称结构"><a href="#优化SNE成对称结构" class="headerlink" title="优化SNE成对称结构"></a>优化SNE成对称结构</h2><p><strong>联合概率替换条件概率</strong></p><p>在SNE中我们通过<strong>条件概率</strong>分别计算高维空间和低维空间中<strong>点对</strong>间的相似度：<br>$$\begin{cases}<br>&amp; p_{ij}=p(x_j|x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{k{\ne}i}{\exp(-||x_i-x_k||^2)}} \\<br>&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}}<br>\end{cases}$$</p><p>然后在t-SNE中我们将条件概率换成<strong>联合概率</strong>：<br>$$\begin{cases}<br>&amp; p_{ij}=p(x_j,x_i)=\frac{\exp(-||x_i-x_j||^2)}{\sum_{m{\ne}n}{\exp(-||x_m-x_n||^2)}} \\<br>&amp; q_{ij}=q(y_j,y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}<br>\end{cases}$$</p><p>注意上面两种表述方式的分母的差异：</p><ul><li>条件概率的分母是中心点 $x_i$ 与其它所有点的相似度之和;</li><li>联合概率的分母没有中心点一说，计算的是所有点对（n个数据点有 $C_n^2$ 个点对）的相似度之和。</li><li>条件概率中 $p_{ij}{\ne}p_{ji}$，而联合概率中 $p_{ij}=p_{ji}$（q同理），这正好也与分母的这种差异吻合。</li></ul><p><strong>注意到联合概率算法会产生一个条件概率算法不会遇到的问题：离群点。</strong></p><p>观察上面的联合概率公式，对于离群点 $x_i$，所有与它配对计算出来的 $p_{ij}$ 或者 $p_{ji}$ 的 $||x_i-x_j||^2$ 将会特别大，这导致 $p_{ij}$ 或者 $p_{ji}$ 总是特别的小，即与 $x_i$ 相关的 $p_{ij}$ 或者 $p_{ji}$ 在对损失函数的贡献总是特别小。这相当于自动减小了那些低密度区域的点在损失函数中的权重，使得通过相似性确定离群点在低维空间中的位置更加困难。<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/03.png" alt=""></p><p>所以呢，必须想办法消除这种效应，增大离群点在损失函数中的比重，文中用用条件概率公式代替上述 $p_{ij}$ 的定义，即：<br>$$\begin{cases}<br>&amp; p_{ij}=\frac{p(x_j|x_i)+p(x_i|x_j)}{2n}  \\<br>&amp; q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{m{\ne}n}{\exp{(-||y_m-y_n||^2)}}}<br>\end{cases}$$</p><p>这样每个点 $x_i$ 对损失函数的贡献度 $p(x_i)=\sum_jp_{ij}&gt;\frac1{2n}$，这就保证了离群点的贡献不会太少。</p><p><strong>KL散度作为损失函数</strong></p><p>t-SNE仍然使用KL散度作为损失函数，所不同的是，这里求的是两个联合概率分布之间的散度：<br>$$C=KL(P||Q)=\sum_i\sum_jp_{ij}\log{\frac{p_{ij}}{q_{ij}}}$$</p><p>此时KL损失函数求导的结果更加简洁：<br>$$\frac{\partial{C}}{\partial{y_i}}=4\sum_k(p_{ik}-q_{ik})(y_i-y_k)$$</p><p>SNE求导结果为：<br>$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$</p><h2 id="解决SNE的拥挤问题"><a href="#解决SNE的拥挤问题" class="headerlink" title="解决SNE的拥挤问题"></a>解决SNE的拥挤问题</h2><h3 id="什么是拥挤问题"><a href="#什么是拥挤问题" class="headerlink" title="什么是拥挤问题"></a>什么是拥挤问题</h3><h4 id="流形的直观理解"><a href="#流形的直观理解" class="headerlink" title="流形的直观理解"></a>流形的直观理解</h4><p>manifold的<a href="https://en.wikipedia.org/wiki/Manifold" target="_blank" rel="noopener">Wiki解释</a>：</p><blockquote><p>In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. </p></blockquote><p>而中文概念“流形”是由北大已故数学教授江泽涵先生提出来。江老的堂姐夫是胡适？… … 不过“流形：这个词真的很艺术，我初次见到时就感叹其形象而不能自已。流形的<a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E5%BD%A2" target="_blank" rel="noopener">Wiki中文解释</a>：</p><blockquote><p>是局部具有欧几里得空间性质的空间，是欧几里得空间中的曲线、曲面等概念的推广。欧几里得空间就是最简单的流形的实例。地球表面这样的球面则是一个稍微复杂的例子。一般的流形可以通过把许多平直的片折弯并粘连而成。</p></blockquote><p><strong>为什么说二维流形面上的点距容易建模 (model)？</strong></p><p>这个问题直观上理解最是简单。首先对于欧几里得空间，我们普通人类最多能直观感受到三维。换算成黎曼空间，就意味着我们只能在三维空间中直观感不超过二维流形曲面的存在，二维流形曲面上的距离就是曲面内连接它们的最短曲线长度。经典的二维流形曲面如下（Swiss Roll 流形, <a href="http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html%29" target="_blank" rel="noopener">Swiss Roll dataset</a>）：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/04.png" alt=""></p><h4 id="为什么存在拥挤"><a href="#为什么存在拥挤" class="headerlink" title="为什么存在拥挤"></a>为什么存在拥挤</h4><p>为了便于可视化，我们会将高维流形上的点映射到二维空间，同时最大程度的保留它们的相对位置（这种每个点相对于整体数据点的定位就是一种全局特征）。然而这种映射是很难完美实现的，举个例子，十维空间（欧几里得空间或者黎曼空间）中可以很容易找到11个相互等距的点（就好比二维空间中能轻易找到三个相互等距的点一样），然而映射到二维空间是不可能找到11个相互等距的点的，势必会有一些点会相互靠近挤在一起，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/05.png" alt=""><br>以二维相互等距的三个点映射到一维空间为例，无论怎么努力，三点都不可能再等距。归根揭底，不同维度空间内的距离分布是不同的，降维映射难免尽如人意。</p><p>再以球内区域为例解释crowding现象：以数据点 $x_i$ 为中心的球的体积与 $r^m$ 直接相关（ $r$ 是半径，$m$ 是球所在空间的维度）。如果在十维流形曲面上数据点均匀分布在这个球中，我们试图在二维流形曲面上以 $y_i$ 为中心对与 $x_i$ 相关的两两距离进行建模。此时我们就会遇到传说中的拥挤问题：与容纳中心点附近数据点的区域相比，容纳适中距离数据点的区域显得不够用。</p><p>在均匀分布的条件下，等距点的数量与半径相关，距离越大数量越多，这意味着映射到低维空间就会越“挤”。因此如果我们想要较为准确的在二维流形曲面中对以 $x_i$ 为中心的两两距离建模，我们就必须把距离 $x_i$ 适中位置的那些点往更远的地方推置（因为太挤了）。</p><!--在SNE中，数据点 $y_i$ 与其它点之间存在一个微弱的引力，所有的力相互作用最终使所有的点**分散在**了自己的收敛位置，这使得点的分布具有连续性质，在聚类时也不会出现断层的现象。--><p>不只是SNE，其它局部特征提取算法例如Sammon mapping等也都面临着拥挤难题。</p><h3 id="怎么解决拥挤问题"><a href="#怎么解决拥挤问题" class="headerlink" title="怎么解决拥挤问题"></a>怎么解决拥挤问题</h3><p>一种叫做<strong>UNI-SNE</strong>的改良算法<a href="#ref10">[10]</a>提出了一种解决办法：给每一个两两相似度添加一个背景值，背景值采样自均匀分布并以一定的比例 $\rho$ 进行混合。由于每个点对之间都引进了背景值，因此不管低维空间中两个映射点离的多么远，$q_{ij}$ 永远不会小于 $\frac{\rho}{n(n-1)/2}$（n个数据点可组合成 $C_n^2$ 个点对）。</p><p>引进背景值导致对于高维空间中相距很远的两个数据点总有$q_{ij}&gt;p_{ij}$（$p_{ij}{\rightarrow}0$ 时 $q_{ij}{\rightarrow}\frac{\rho}{n(n-1)/2}$），这表示低维空间中点对并没有完全拟合高维空间的点对相似度，映射后相似度变小。</p><p>尽管UNI-SNE的效果比SNE好，但是其损失函数却很难优化。目前较好的优化UNI-SNE的方法是：开始的时候将背景值混合比例设为0，这实际上等效于运行SNE；当SNE开始使用<strong>模拟退火</strong>策略时增大背景值的混合比例，促进自然分类间的gaps形成。</p><p>直接优化UNI-SNE并不可行，因为低维空间中两个相距很远的映射点的 $q_{ij}$ 几乎全部来自于背景值，即高维空间中相应两点间（即使他们的 $p_{ij}$ 很大）的距离对 $q_{ij}$ 的影响很小，这使得映射后的两点间的 $q_{ij}$ 没有什么实际意义。这表示，如果一个自然类的两部分在优化早期就分开了，就再也不会再聚合在一起了。</p><h2 id="低维空间采用柯西分布表达联合概率"><a href="#低维空间采用柯西分布表达联合概率" class="headerlink" title="低维空间采用柯西分布表达联合概率"></a>低维空间采用柯西分布表达联合概率</h2><p>UNI-SNE通过添加背景值使低维空间中相距甚远的 $q_{ij}$ 不至于趋近于0。</p><p>本文提出了一种新的解决办法，采用与高斯分布性质极其相似的重尾分布计算联合概率。右重尾分布使得当随机变量取值很大时其对应的概率值高斯分布要大，典型的重尾分布是t分布，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/06.png" alt=""><br>t分布实际上是不同方差的高斯分布的混合分布，它的性质与高斯分布十分接近，而且更加容易计算：因为高斯分布涉及到指数运算，而t分布只需要求倒数。</p><p>这里采用的是自由度 $\nu=1$ 的t、分布，又叫做<strong>柯西分布</strong>，其概率密度函数如下：<br>$$f(x;x_0,\gamma)=\frac1{\pi\gamma[1+(\frac{x-x_o}{\gamma})]^2}$$</p><p>取 $x_0=0, \gamma=1$ 得标准柯西分布：<br>$$f(x;0,1)=\frac1{\pi[1+(x-x_0)]^2}$$</p><p>用标准柯西分布表示联合概率：<br>$$q_{ij}=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum_{m{\ne}n}{(1+||y_m-y_n||^2)^{-1}}}=\frac{\sum_{m{\ne}n}(1+||y_m-y_n||^2)}{1+||y_i-y_j||^2}$$</p><p>求导结果如下：<br>$$\frac{\partial{C}}{\partial{y_i}}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}}$$</p><h2 id="Pseudo-code"><a href="#Pseudo-code" class="headerlink" title="Pseudo code"></a>Pseudo code</h2><p>下面是<strong>精简版t-SNE算法</strong>伪代码，非常简洁：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/07.png" alt=""></p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p><strong>添加微小的动量项可以减少到达最优解的迭代次数</strong></p><p><strong>精简版的t-SNE算法采用适应性学习率加速训练</strong>：在梯度较稳定的方向上增大学习率。<a href="#ref11">[11]</a></p><p>尽管精简版算法已经可以吊打其它非参数降维技术了，还是可以继续优化，文中提出了两个技巧：</p><p><strong>1. early compression</strong>：优化起始的时候将所有的映射点初始化在原点附近，有利于映射点移动、形成自然类。early compression通过给损失函数加上一个L2惩罚项实现。<br><strong>2. early exaggeration</strong>：在优化的初始阶段将所有的 $p_{ij}$ 扩大指定倍数加快收敛速度。</p><p>总结一下模型优化的参数配置：</p><ul><li>起始的50次迭代中将所有的 $p_{ij}$ 乘以4（这个步骤在精简版算法的伪代码中没有写出来）；</li><li>梯度下降的迭代轮数T设为1000；</li><li>动量项 $\alpha^{(t)}$ 当 $t&lt;250$ 时设为0.5，当 $t{\ge}250$ 时设为0.8；</li><li>学习率初始值设为100，每次迭代都将进行适应性更新 <a href="#ref11">[11]</a>。</li></ul><p><a href="http://ticc. uvt.nl/˜lvdrmaaten/tsne" target="_blank" rel="noopener">算法的Matlab实现</a></p><h1 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h1><p>作者分析了三个不足之处。</p><p><strong>1. 不能用于低维空间超过三维的情况</strong></p><p>因为t-SNE算法在映射空间利用了<strong>柯西分布</strong>的重尾特性解决拥挤问题，柯西分布是自由度为1的t分布，这种特性在二维空间中表现十分优异。但如果需要降到三维以上的映射空间，1自由度的t分布不能很好的保留局部特征，我们可能需要使用更多自由度的t分布。</p><p><strong>2. 本征维度诅咒</strong></p><p>t-SNE虽然能够保留全局特征，但是总体上还是基于局部特征进行的降维，这表示t-SNE对原始数据的 <strong>本征维度 (intrinsic dimentionality)</strong> 十分敏感，因为本征维度过高，我们就不能再把流形曲面的局部区域当欧几里得空间处理了，数据点间的局部特征更加复杂 <a href="#ref12">[12]</a>。不仅t-SNE，其它主流的基于局部特征提取的降维算法（如Isomap，LLe）都面临着这个诅咒。</p><p>作者提出了一种可行的办法：先用 <strong>自编码器 (autoencoder)</strong><a href="#ref13">[13]</a> 对数据进行压缩，这类模型可以大大降低原始数据的维度，同时最大保留高维数据的特征。经过编码的数据再进行t-SNE降维。</p><p><strong>3. 损失函数不凸~</strong></p><p>不幸的是当前主流降维算法使用的损失函数都是凸函数，而t-SNE优化的超参更多，这使得其损失函数是非凸的。这意味着，不同的超参取值、不同的初始化都可能收敛到不同的（局部最优）解。但是作者表示，如果固定这些超参，t-SNE就可以应用于不同的可视化任务，优化结果不会随着不同批次而发生变化。</p><p>t-SNE降维结果中点间的距离是没有实际意义的。原始的t-SNE训练很慢，后面有许多改进，比如 </p><ul><li>multiple maps of t-SNE</li><li>parametric t-SNE</li><li>… …</li></ul><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><span id="ref1">[1]</span> <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="noopener">Maaten, L. V. D., &amp; Hinton, G. (2008). <strong>Visualizing data using t-SNE</strong>. Journal of machine learning research, 9(Nov), 2579-2605.</a><br><span id="ref2">[2]</span> M.C. Ferreira de Oliveira and H. Levkowitz. <strong>From visual data exploration to visual data mining: A survey</strong>. IEEE Transactions on Visualization and Computer Graphics, 9(3):378–394, 2003.<br><span id="ref3">[3]</span> J.W. Sammon. <strong>A nonlinear mapping for data structure analysis</strong>. IEEE Transactions on Computers, 18(5):401–409, 1969.<br><span id="ref4">[4]</span> P. Demartines and J. Herault. <strong>´ Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets</strong>. IEEE Transactions on Neural Networks, 8(1):148–154, 1997<br><span id="ref5">[5]</span> G.E. Hinton and S.T. Roweis. <strong>Stochastic Neighbor Embedding</strong>. In Advances in Neural Information Processing Systems, volume 15, pages 833–840, Cambridge, MA, USA, 2002. The MIT Press.<br><span id="ref6">[6]</span> J.B. Tenenbaum, V. de Silva, and J.C. Langford. <strong>A global geometric framework for nonlinear dimensionality reduction</strong>. Science, 290(5500):2319–2323, 2000.<br><span id="ref7">[7]</span> K.Q. Weinberger, F. Sha, and L.K. Saul. <strong>Learning a kernel matrix for nonlinear dimensionality reduction</strong>. In Proceedings of the 21st International Confernence on Machine Learning, 2004.<br><span id="ref8">[8]</span> S.T. Roweis and L.K. Saul. <strong>Nonlinear dimensionality reduction by Locally Linear Embedding</strong>. Science, 290(5500):2323–2326, 2000.<br><span id="ref9">[9]</span> M. Belkin and P. Niyogi. <strong>Laplacian Eigenmaps and spectral techniques for embedding and clustering</strong>. In Advances in Neural Information Processing Systems, volume 14, pages 585–591, Cambridge, MA, USA, 2002. The MIT Press.<br><span id="ref10">[10]</span> J.A. Cook, I. Sutskever, A. Mnih, and G.E. Hinton. <strong>Visualizing similarity data with a mixture of maps</strong>. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, volume 2, pages 67–74, 2007.<br><span id="ref11">[11]</span> R.A. Jacobs. <strong>Increased rates of convergence through learning rate adaptation</strong>. Neural Networks, 1: 295–307, 1988.<br><span id="ref12">[12]</span> Y. Bengio. <strong>Learning deep architectures for AI</strong>. Technical Report 1312, Universite´ de Montreal, ´ 2007.<br><span id="ref13">[13]</span> G.E. Hinton and R.R. Salakhutdinov. <strong>Reducing the dimensionality of data with neural networks</strong>. Science, 313(5786):504–507, 2006.</p>]]></content>
    
    <summary type="html">
    
      &lt;div style=&quot;display: inline-block; width: 670px; height: 456px; overflow: hidden; border: 1px solid #ddd&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.gif&quot; style=&quot;margin-top: -114px !important; border: none&quot;&gt;&lt;br&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;t-SNE&lt;/strong&gt; (&lt;strong&gt;t-distributed Stochastic Neighbor Embedding&lt;/strong&gt;) 是目前来说效果较好的数据降维与可视化方法，但是大量占用内存、计算时间长的缺点也很突出。&lt;/p&gt;
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="t-SNE" scheme="http://chenyin.top/tags/t-SNE/"/>
    
  </entry>
  
  <entry>
    <title>降维03 - SNE原理</title>
    <link href="http://chenyin.top/ml/20190325-80ae.html"/>
    <id>http://chenyin.top/ml/20190325-80ae.html</id>
    <published>2019-03-25T12:44:14.000Z</published>
    <updated>2019-04-14T16:02:20.120Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.png" width="100%"></p><blockquote><p><a href="http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf" target="_blank" rel="noopener">Hinton, G. E., &amp; Roweis, S. T. (2003). <strong>Stochastic neighbor embedding</strong>. In Advances in neural information processing systems (pp. 857-864).</a></p></blockquote><p><strong>随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE)</strong> 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 </p><a id="more"></a><h1 id="定义条件概率"><a href="#定义条件概率" class="headerlink" title="定义条件概率"></a>定义条件概率</h1><p>部分情况下，高维空间中两个点间的相似性可以用基于欧式距离的<strong>不相似度</strong> $d_{ij}$ 来衡量：<br>$$d_{ij}^2=\frac{||x_i-x_j||^2}{2\sigma_i^2}$$</p><p>SNE用<strong>条件概率</strong>来代替欧式距离度量两个高维数据点间的相似性。即：以点 $x_i$ 为中心，点 $x_i$ 选择 $x_j$ 作为自己邻居的概率记为 $p(x_j|x_i)$，定义<br>$$p_{ij}=p(x_j|x_i)=\frac{\exp(-d_{ij}^2)}{\sum_{k{\ne}i}{\exp(-d_{ik}^2)}} $$<br>注意这里对于 $p(x_i|x_i)$ 的理解有点怪异：它表示点 $x_i$ 选择自己作为邻居的概率，显然自己永远不可能是自己的邻居，所以 $p(x_i|x_i)=0$，而不是1。</p><h1 id="确定方差的确定"><a href="#确定方差的确定" class="headerlink" title="确定方差的确定"></a>确定方差的确定</h1><p>上式中的 $\sigma_i^2$ 是以 $x_i$ 为中心的高斯分布的<strong>方差</strong>：不同点周围的点的密度是不一样的，所以每个点的高斯分布对应的方差 $\sigma_i^2$ 也不相同，周围点密度大的中心点对应的方差应该较小。作者定义了<strong>困惑度 k (perplexity)</strong> ：手动指定的超参，代表某个点的有效邻居数，这个值对所有点都是常数。$\sigma_i^2$ 的取值将使得以点 $x_i$ 为中心选择其它所有点作为邻居的分布对应的<strong>熵</strong>等于 $\log{k}$，即<br>$$H(P_i)=-\sum_{j{\ne}i}{p(x_j|x_i)\log_2{p(x_j|x_i)}}=\log_2k$$<br>理论上可以通过上面的式子可以针对每个点 $x_i$ 解出对应的 $\sigma_i^2$。</p><p><strong>熵 $H(P_i)$ 的理解</strong></p><ul><li><strong>不确定性</strong>：熵本身就意味着不确定性，当区域点密集时，中心点位置的不确定性就小；</li><li><strong>能量</strong>：不确定性大意味着能量大，拉不住中心点，它要到处跑；</li><li><strong>有效邻居数</strong>：点密集时中心点的有效邻居就多。</li></ul><h1 id="映射到低维空间"><a href="#映射到低维空间" class="headerlink" title="映射到低维空间"></a>映射到低维空间</h1><p>在低维空间（二维或者三维）确定一点 $y_i$，它与高维空间的点 $x_i$ 对应，我们手动设置点 $y_i$ 的条件概率分布，即固定以 $y_i$ 为中心点的高斯分布对应的方差为 $\frac12$，当 $j{\ne}i$ 时：<br>$$ q_{ij}=q(y_j|y_i) = \frac {\exp{(-||y_i-y_j||^2)}} {\sum_{k{\ne}i}{\exp{(-||y_i-y_k||^2)}}} $$<br>当j=i时 $q(y_j|y_i)=0$ 。</p><p>此时，如果低维点 $y_i$ 能够正确表示高维点 $x_i$，意味着 $q(y_j|y_i)=p(x_j|x_i)$。为了使两个概率（近似）相等，我们可以最小化<strong>KL散度</strong>。损失函数如下：<br>$$C=\sum_iKL(P_i|Q_i)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$</p><ul><li>$P_i$ 表示：给定点 $x_i$，其它所有点的条件概率分布；</li><li>$Q_i$ 表示：给定低维空间映射点 $y_i$，其它所有低维映射点的条件概率分布。</li></ul><p>KL散度又叫<strong>相对熵</strong>，用来度量两个分布间的距离。假设P是真实分布，Q是模型分布，$KL(P|Q)$ 表示用Q表示P分布的数据所需的额外信息。   </p><p><strong>KL散度是不对称的</strong> </p><p>KL散度中包含 $\log\frac{p_{ij}}{q_{ij}}$ 意味着这个映射不是对称的，即：</p><ul><li>使用距离较小的低维点表示距离较大的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于小于0，则损失C较小；</li><li>使用距离较大的低维点表示距离较小的高维点时，$\log\frac{p_{ij}}{q_{ij}}$ 倾向于大于0，则损失C较大。</li></ul><p>这里就存在一个问题：当两个高维点距离很远，而我构造两个距离很近的低维点能够使损失函数更小，却与实际的目的不相符！所以，SNE算法使得高维空间中距离近的点在低维空间中距离仍然很近，但是远的点就嘿嘿嘿了。</p><h1 id="最小化损失函数"><a href="#最小化损失函数" class="headerlink" title="最小化损失函数"></a>最小化损失函数</h1><p>从 $q_{ij}$ 的定义式的分母部分可知，低维空间中点 $y_i$ 选择点 $y_j$ 的概率 $q_{ij}$ 与低维空间中的每一个映射点都有关系（分母起到了normalization的作用），但是求导结果却十分简洁：<br>$$\frac{\partial{C}}{\partial{y_i}}=2\sum_k{(y_i-y_k)[(p_{ik}-q_{ik})+(p_{ki}-q_{ki})]}$$</p><p>想沿着所有点以最陡梯度下降是不现实的，不仅低效，还可能陷入糟糕费解的局部最优。上面的梯度公式右边理论上是针对所有低维映射点进行迭代，但是实际上，相距较远的一堆点间的影响十分小（抽象），在计算时往往可以忽略不计，也就是说：仅仅计算与中心点相距较近的一部分点，即<strong>邻居</strong> ，这也是为什么算法中含有单词neighbour的原因了吧。</p><p>选择中心点的部分较近的邻居参与计算表示，我们只保留了中心点附近区域的特性，而忽略了整体局势，所以说SNE是保留局部特征而非全局特征的算法。这个局部特性主要反应为 $\sigma_i^2$：局部点密集方差小，局部点稀疏方法大。方差确定的方法前面已经陈述了~。</p><h2 id="带动量项的梯度更新"><a href="#带动量项的梯度更新" class="headerlink" title="带动量项的梯度更新"></a>带动量项的梯度更新</h2><p>为了加速优化过程、避免很一般的局部解，可以在每次下降时添加动量项。<strong>动量 (momentum)</strong> 的作用就是在下降到局部最优时，小球仍然具有沿切线方向的分量，这使得小球将继续朝前运动，这会有两种结果：</p><ul><li>小球越过障碍，继续前行；</li><li>小球回退，返回局部最优解；<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/08.png" alt=""><br>具体的，更新公式如下：<br>$$\gamma^{(t)}=\gamma^{(t-1)}-{\eta}\frac{\partial{C}}{\partial{\gamma}}+\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$$</li></ul><p>式中 $\alpha(t)$ 即动量，动量项 ($\alpha(t)(\gamma^{(t-1)}-\gamma^{(t-2)})$) 还与上一次运动幅度有关，直观的看，上一次运动越剧烈，下一次就越刹不住车。</p><h2 id="随机抖动"><a href="#随机抖动" class="headerlink" title="随机抖动"></a>随机抖动</h2><p><strong>随机抖动</strong> (random jitter) 是一种初始化技巧，即将低维空间中的所有数据点初始化在离坐标原点极近的地方。在迭代的过程中，它们将抖抖抖抖抖动直至收敛。尽管还是很慢，不过在节约时间和选择更优局部解时还是有明显提升的。</p><h2 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h2><p>在优化早期给每一步迭代加上高斯噪音，这可以帮助避免不好的局部最优解。随着迭代次数变多，噪音方差将逐渐减小。当方差变化非常慢时，这表明开始形成全局结构。（玄学）</p><p>然而，高斯噪音的数量和衰减速率是十分难以确定的，它们不仅与动量相关，也受学习率的影响。怎么办？多算几次！666</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.png&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hinton, G. E., &amp;amp; Roweis, S. T. (2003). &lt;strong&gt;Stochastic neighbor embedding&lt;/strong&gt;. In Advances in neural information processing systems (pp. 857-864).&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;随机近邻嵌入算法 (Stochastic Neighbor Embedding, SNE)&lt;/strong&gt; 由Hinton在2003年提出来的基于条件概率、只保留局部特征的非线性降维方法。 &lt;/p&gt;
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="非线性降维" scheme="http://chenyin.top/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4/"/>
    
      <category term="SNE" scheme="http://chenyin.top/tags/SNE/"/>
    
      <category term="t-SNE" scheme="http://chenyin.top/tags/t-SNE/"/>
    
  </entry>
  
  <entry>
    <title>降维02 - 主成分分析 (PCA)</title>
    <link href="http://chenyin.top/ml/20190325-c024.html"/>
    <id>http://chenyin.top/ml/20190325-c024.html</id>
    <published>2019-03-25T07:13:19.000Z</published>
    <updated>2019-04-14T16:02:20.109Z</updated>
    
    <content type="html"><![CDATA[<p>PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了<strong>线性代数</strong>之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。<a id="more"></a></p><p>PCA是最易于理解的<strong>特征提取</strong>过程：通过对原始特征的<strong>线性组合</strong>构造新的“特征”，这些特征不同于原始特征，但是又能与原始特征一样表达原始数据的信息。</p><p><strong>PCA (Primary Component Analysis, 主成分分析)</strong> 为什么叫做主成分分析呢？因为PCA构造出的新特征地位并不是等同的，即这些新特征的重要程度存在差异：</p><ul><li><strong>第一主成分 (the first component)</strong> 是新特征中最重要的特征，它在所有新特征中方差最大，这意味着它对数据变异的贡献是最大的；</li><li><strong>第二主成分 (the second component)</strong> 在保证不影响第一主成分的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异）；</li><li><strong>第三主成分 (the third component)</strong> 在不保证第一和第二主成分呢的基础上试图解释剩下的变异（即总变异 - 第一主成分引起的变异 - 第二主成分引起的变异）;</li><li>依次类推……</li></ul><h1 id="线代原理"><a href="#线代原理" class="headerlink" title="线代原理"></a>线代原理</h1><p>预备知识：线性代数（矩阵运算、特征值&amp;特征向量、特征值分解）</p><h2 id="可对角化"><a href="#可对角化" class="headerlink" title="可对角化"></a>可对角化</h2><p>如果一个n阶方阵A相似于对角矩阵，即存在可逆矩阵$P$使得$P^{-1}AP$是对角矩阵，则称方阵A是<strong>可对角化</strong>的。</p><p><strong>n阶方阵A可对角化的充要条件是A每个特征值的几何重数与代数重数相等</strong>：<strong>代数重数</strong>指<strong>特征多项式</strong>中该特征值的幂次，<strong>几何重数</strong>指特征值对应的线性无关的特征向量的个数。</p><p><strong>n阶方阵A可对角化的充要条件是A有n个线性无关的特征向量</strong>：几何重数与代数重数相等意味着n个线性无关的特征向量。</p><p>即使方阵A可逆也不能保证每个特征值的代数重数与几何重数相等，因此A可逆不是A可对角化的充要条件！</p><h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>如果矩阵A是一个<strong>可对角化</strong>的方阵，它就可以进行特征值分解，即A可表示为：<br>$$A=Q{\Lambda}Q^{-1}$$<br>其中</p><ul><li>$Q$ 是n阶方阵，它的n个列向量是方阵A的n个特征向量</li><li>$\Lambda$ 是对角方阵，对角线元素是方阵A的特征值，其位置与 $Q$ 中的特征向量位置相对应</li></ul><p>特征值分解的应用？求逆。<br>如果方阵A是<strong>非奇异矩阵</strong>（即可以进行特征值分解且特征值不含0），则 $A^{-1}=Q{\Lambda}^{-1}Q^{-1}$，其中 $[{\Lambda}^{-1}]_{ii}=\frac1{\lambda_i}$。</p><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>特征值分解对A的要求格外严格：可逆、特征值不含0、方阵……<br>放松特征值分解的限制，将A扩展到任意 $m{\times}n$ 的矩阵即得到 <strong>奇异值分解 (Singular Value Decomposition, SVD)</strong> 。</p><p>假设M是定义在<strong>实数域</strong>或者<strong>复数域</strong>上的 $m{\times}n$ 阶的矩阵：<br>$$M=U{\Sigma}V^\ast$$<br>其中</p><ul><li>U是 $m{\times}m$ 阶<strong>酉矩阵</strong>：U的m个列向量实际上是 $M^{\ast}M$ 的特征向量，称为M的<strong>左奇异向量</strong>。</li><li>$\Sigma$ 是 $m{\times}n$ 阶<strong>非负实数对角矩阵</strong>：对角线元素称为M的<strong>奇异值</strong>，一般情况下奇异值按<strong>从大到小</strong>的顺序排列！</li><li>$V^\ast$ 是 $V$ 的<strong>共轭转置</strong>，是 $n{\times}n$ 阶<strong>酉矩阵</strong>：V的n个列向量实际上是 $MM^\ast$ 的特征向量，称为M的<strong>右奇异向量</strong>。</li></ul><blockquote><p><strong>共轭转置</strong>：共轭转置与转置是两个概念，当矩阵定义在实数域上时二者结果相同，矩阵A的共轭转置记为 $A^\ast$，定义如下：<br>$$A^\ast=(\overline{A})^T=\overline{A^T}$$<br>其中，$\overline{A}$ 表示对A的元素<strong>复共轭</strong>，当A定义在实数域时 $\overline{A}=A$。</p></blockquote><p>当矩阵M定义在实数域时有：<br>$$M=U{\Sigma}V^T$$<br>我们在应用SVD时一般都是定义在实数域上的哟~</p><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>上面提到了对于任意 $m{\times}n$ 阶矩阵M的SVD分解：<br>$$M_{m{\times}n}=U_{m{\times}m}{\Sigma_{m{\times}n}}V_{n{\times}n}^T$$<br>直观图如下（这里假设样本数量m多于特征数量n，这意味着M有n个奇异值）：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/19.png" alt=""><br>其中 $\Sigma$ 矩阵很有意思，当m&gt;n时，矩阵 $\Sigma_{m{\times}n}$ 中只有子矩阵 $\Sigma_{n{\times}n}$ 的对角线上的值不为0，如下图所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/20.png" alt=""></p><p>以scRNA测序为例：假设在表达谱矩阵中，一行表示一个细胞中不同基因的表达量，一列表示一个基因在不同细胞中的表达量。这与我们的习惯（一列表示一个细胞，一行表示一个基因）有所不同！</p><p>对应到上述SVD分解式我们发现，n表示细胞数量，m表示基因数量。我们降维的结果肯定是要保证细胞总数m不变，而将基因数目从n减小到k。</p><p>具体的，取 $\Sigma$ 中最大的k个奇异值，即取 $\Sigma_{k{\times}k}$ 子矩阵，相应的取U的前k列和V的前k列（即$V^\ast$的前k行），即：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/21.png" alt=""><br>此时<br>$$M_{m{\times}n}=U_{m{\times}k}{\Sigma_{k{\times}k}}V_{k{\times}n}^T$$<br>上式中的 $U_{m{\times}k}$ 就是 $M_{m{\times}n}$ 从n维特征空间降到k维特征空间的结果。注意矩阵 $U_{m{\times}k}$ 的k个列向量并不在矩阵  $M_{m{\times}n}$ 中，而是M中的n个列向量线性组合的结果。</p><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCA的大名如雷贯耳，曾经的我也以为PCA是个什么很复杂的东西，但是学习了&lt;strong&gt;线性代数&lt;/strong&gt;之后才发现，PCA的原理简单而不失优雅，粗暴而不失趣味。
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="PCA" scheme="http://chenyin.top/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>降维01 - 特征选择和特征提取</title>
    <link href="http://chenyin.top/ml/20190325-d2ce.html"/>
    <id>http://chenyin.top/ml/20190325-d2ce.html</id>
    <published>2019-03-25T07:13:10.000Z</published>
    <updated>2019-04-14T16:02:20.106Z</updated>
    
    <content type="html"><![CDATA[<p>大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的<strong>降维</strong>浓缩技术应运而生。<a id="more"></a></p><p>降维的好处有哪些？</p><ol><li>减少数据维度，存储数据需要的空间也会减少（盘霸可忽略~）；</li><li>低维数据可以减少计算量，缩短模型训练时间；</li><li>很多算法在高维数据上的表现远远没有在低维数据上好；</li><li>去掉冗余特征（强相关特征），提高数据的质量；</li><li>有助于可视化，我们只能形象观察三维及以下的数据！</li></ol><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>降维总是围绕着减少特征数进行的，根据对特征的操作可分为：</p><ul><li><strong>特征选择</strong>：保留原始特征集的子集，即选取部分原始特征；</li><li><strong>特征提取</strong>：构造不同于原始特征的新特征，新特征往往是原始特征的组合，替代原始特征表达原始数据想表达的信息。<br>特征提取是降维算法研究的核心内容。</li></ul><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择只是对每个特征进行评估，去掉不重要的或者选出重要的：</p><ol><li>缺失值比率：按缺失值比率删除特征；</li><li>低方差滤波：删除方差小的特征；</li><li>高相关滤波：只保留高相关特征中的一个；</li><li>随机森林：计算每个特征的重要性；</li><li>前向特征选择：依次增加特征数检验模型性能；</li><li>反向特征消除：依次减少特征数检验模型性能。</li></ol><h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>特征提取才是降维思想的核心内容，降维算法家族枝繁叶茂，先做一个总体分类：<img src="" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大数据包含了丰富的先验知识，即几乎包含了一切我们感兴趣的信息。但是数据量过大也会使我们在分析时感到茫然无措。特征过多使得我们不可能对单个特征进行详细解析，大部分时候我们是将所有特征当成一个整体进行考虑，或者分析特征之间的关系。对高维数据数据进行预处理是一种不错的选择，此时各种各样的&lt;strong&gt;降维&lt;/strong&gt;浓缩技术应运而生。
    
    </summary>
    
      <category term="ml" scheme="http://chenyin.top/categories/ml/"/>
    
    
      <category term="降维" scheme="http://chenyin.top/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>批次效应校正与RUV算法</title>
    <link href="http://chenyin.top/bioinfo/20190322-afa7.html"/>
    <id>http://chenyin.top/bioinfo/20190322-afa7.html</id>
    <published>2019-03-22T01:37:34.000Z</published>
    <updated>2019-04-23T07:22:45.327Z</updated>
    
    <content type="html"><![CDATA[<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p> Gagnon-Bartsch et al.提出了RUV-2用来标准化<strong>连续的</strong>微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化<strong>离散的</strong>RNA测序数据。</p><p> 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建<strong>泛化线性模型</strong> (Generalized Linear Model, GLM):<br>$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$<br><a id="more"></a><br>参数意义如下：</p><ul><li>$Y$ 是 $n{\times}J$ 的表达矩阵；</li><li>$W$ 是 $n{\times}k$ 与<strong>不需要的变异</strong>相关的<strong>多余变异相关矩阵</strong>（k是不需要的变异相关的变量的个数），$\alpha$ 是 $k{\times}J$ 的多余变异相关矩阵的系数（参数）；</li><li>$X$ 是 $n{\times}p$ 与<strong>感兴趣的变异</strong>相关的<strong>期望变异相关矩阵</strong>（p是感兴趣的变异相关的变量的个数），$\beta$ 是 $p{\times}J$ 的期望变异相关矩阵的系数（参数）；</li><li>$O$ 是一个 $n{\times}J$ 的矩阵，它可以置零，也可以包含其它的标准化过程（如<a href="https://en.wikipedia.org/wiki/Quantile_normalization" target="_blank" rel="noopener">UQ标准化</a>）。</li><li>矩阵 $X$ 是一个随机变量，是我们实验的测量值，是已知的（先验）。</li><li>矩阵 $W$ 是未观测的随机变量；$\alpha$、$\beta$、$k$ 都是未知参数。</li></ul><p>不同于先前的标准化方法，RUV可以使用GLM标准化技术同时标准化reads计数（$W\alpha$）和推断差异表达（$X\beta$）。标准化的计数也可以通过由原始计数对不需要的因子进行回归分析后求残差得到，但是直接从原始计数中移除不需要的因子（$W\alpha$）可能会损失掉 $X$ 的一部分。[<a href="">reference</a>]</p><p>同时估计 $W$, $\alpha$, $\beta$ 和 $k$ 是很难的。对于一个给定的 $k$ 值，我们尝试着用下面三种方法对$W$ 进行估计：</p><h2 id="1-基于阴性对照基因的RUVg"><a href="#1-基于阴性对照基因的RUVg" class="headerlink" title="1. 基于阴性对照基因的RUVg"></a>1. 基于阴性对照基因的RUVg</h2><ol><li>假设我们鉴定出了一个阴性对照基因 (negative control genes) 的集合（大小为 $J_c$），例如不差异表达的基因，对这个基因集合来说 $\beta_c=0$ 即 $\log{E[Y_c|W,X,O]}=W\alpha_c+O_c$，公式中的下标c将矩阵限制在了大小为 $J_c$ 的基因集合里。</li><li>定义 $Z=\log{Y}-O$，$Z^\ast$ 是 $Z$ 列向量中心化（$Z$ 的各个列向量均值都为0）的结果。</li><li>对 $Z_c^\ast$ 进行奇异值分解 (singular value decomposition, SVD) 即 $Z_c^\ast=U{\Lambda}V^T$。矩阵 $U$ 是 $n{\times}n$ 列正交矩阵，它的列向量是 $Z^\ast$ 的左奇异向量集；矩阵 $V$ 是 $J_c{\times}J_c$ 的列正交矩阵，它的列向量是 $Z^\ast$ 的右奇异向量集；$\Lambda$ 矩阵是由 $Z^\ast$ 的奇异值组成的非方形对角矩阵，大小为 $n{\times}J_c$。$Z^\ast$ 最少有 $\min{(n,J_c)}$ 个奇异值。对于一个给定的 $k$，通过 $\widehat{W\alpha_c}=U\Lambda_kV^T$ 估计 $W\alpha_c$，通过 $\hat{W}=U\Lambda_k$ 估计 $W$。$|lambda_k$ 是由 $\Lambda$ 导出的大小为 $n{\times}J_c$ 的非方形对角矩阵，保留 $\Lambda$ 中最大的 $k$ 个奇异值，将其它的奇异值置为0。</li><li>将 $\hat{W}$ 带入上面基于 $J$ 个基因构建的公式中，通过GLM回归估计 $\alpha$ 和 $\beta$。</li><li>（可选）将标准化的读段计数定义为 $Z$ 对 $\hat{W}$ 的普通最小二乘回归 (ordinary least squares, OLS) 的残差。 </li></ol><p>这是最基础的RUV-2的离散版本。其中的关键假设是我们能够找到这个阴性对照基因集合。然而，RUV-2已被证实对对照基因的选择十分敏感。我们因此考虑下面的两种方法：RUVr不需要阴性对照基因，RUVs对阴性对照基因选择的鲁棒性更强。</p><h2 id="2-基于残差的RUVr"><a href="#2-基于残差的RUVr" class="headerlink" title="2. 基于残差的RUVr"></a>2. 基于残差的RUVr</h2><ol><li>计算残差矩阵 $E(n{\times}J)$: 计数矩阵 $Y(n{\times}J)$ 关于感兴趣的协变量矩阵 $X(n{\times}J)$ 的初步GLM回归，例如异常值残差。这里用于回归计算的计数矩阵可以是未标准化的原始数据，也可以是经过其它标准化工具（例如UQ）处理过的数据。</li><li>对残差进行奇异值分解，即 $E=U{\Lambda}V^T$，通过 $\hat{W}=U\Lambda_k$ 估计 $w$。接下来的步骤与 <code>RUVg</code> 的第4、5步相同。</li></ol><h2 id="3-基于重复-阴性对照样本的RUVs"><a href="#3-基于重复-阴性对照样本的RUVs" class="headerlink" title="3. 基于重复/阴性对照样本的RUVs"></a>3. 基于重复/阴性对照样本的RUVs</h2><ol><li>假设在多个复制样本中具有生物学特征的（我们感兴趣的）某些协变量的表达量可看作恒定的，它们的计数差异与<code>RUVg</code>中的阴性对照基因一样，对我们后续的研究没有影响。现在假设有 $R$ 个复制组，$r(i){\in}{1,…,R}$ 表示样本 $i$ 所属的复制组；如果样本 $i$ 不属于任何一个复制组，则 $r(i)=0$。例如，对于SEQC数据集，样本A和样本B各自的64个<strong>复制本</strong>（$=4[\text{libraries}]{\times}2[\text{flow-cell}]{\times}8[\text{lanes}]$）分别组成了一个<strong>复制组</strong>。</li><li>对每一个复制本对应的计数矩阵进行<strong>列中心化</strong>处理，即矩阵各个列向量的均值都为0。去掉不属于预期复制组的样本，即筛选出 $n_d=\sum_i{I(r(i)\ne0)}$ 个样本对应的列中心化后的计数子矩阵 $Y_d(n_d{\times}J)$。 此时 $\log{E[Y_d|W,X,O]}=W_d\alpha+O_d$，对应的矩阵大小是 $(n_d{\times}J){\leftarrow}({n_d\times}k)({k\times}J)+(n_d{\times}J)$。</li><li>定义 $Z_d=\log{Y_d}-O_d$，$Z_d^\ast$ 是 $Z_d$ 列中心化的结果，$Z_d^\ast=U{\Lambda}V^T$。通过 $\hat{\alpha}=\Lambda_kV^T$（保留最大的 $k$ 个奇异值，$k{\le}\min{(n_d,J)}$）来估计 $\alpha$。</li><li>在所有 $n$ 个原始数据和 $J_c$ 个阴性对照基因上对 $Z_c$ 进行最小二乘回归（OLS）。估计讨厌因子 $W$：$\hat{W}=Z_c\hat\alpha_c^T(\hat\alpha_c\hat\alpha_c^T)^{-1}$。接下来的步骤与 <code>RUVg</code>的第4、5步相同。</li></ol><hr><h1 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h1><h2 id="两个数据集"><a href="#两个数据集" class="headerlink" title="两个数据集"></a>两个数据集</h2><ol><li><strong>SEQC data set</strong>: The third phase of the MicroArray Quality Control (MAQC) project, also known as the Sequencing Quality Control17 (SEQC) project, aims to assess the technical performance of high-throughput sequencing platforms by generating benchmarking data sets.</li><li><strong>Zebrafish (斑马鱼) data set</strong>: All procedures were conducted in compliance with US federal guidelines in an AAALAC-accredited facility and were approved by the UC Berkeley Office of Animal Care and Use. </li></ol><h2 id="两种讨厌因子"><a href="#两种讨厌因子" class="headerlink" title="两种讨厌因子"></a>两种讨厌因子</h2><p>本文分析了两种讨厌因子：<strong>library preparation</strong> &amp; <strong>flow-cell effects</strong>。</p><p><strong>flowcell</strong>：流动室，别称鞘流池、流动池，是流式细胞技术的基础关键部件。大概长这个样子：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/09.jpg" alt=""></p><p>作者用<strong>正交的主成分图</strong>展示了这两种讨厌因子：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/10.jpg" alt=""><br><span style="font-size:16px;color:gray">Scatterplot matrix of first three principal components (PC) for unnormalized counts (log scale, centered). The principal components are orthogonal linear combinations of the original 21,559-dimensional gene expression profiles, with successively maximal variance across the 128 samples, that is, the first principal component is the weighted average of the 21,559 gene expression measures that provides the most separation between the 128 samples. Each point corresponds to one of the 128 samples. The four sample A and the four sample B libraries are represented by different shades of blue and red, respectively (16 replicates per library). Circles and triangles represent samples sequenced in the first and second flow-cells, respectively. As expected for the SEQC data set, the first principal component is driven by the extreme biological difference between sample A and sample B. The second and third principal components clearly show library preparation effects (the samples cluster by shade) and, to a lesser extent, flow-cell effects reflecting differences in sequencing depths (within each shade, the samples cluster by shape).</span></p><h2 id="算法横向对比"><a href="#算法横向对比" class="headerlink" title="算法横向对比"></a>算法横向对比</h2><p><strong>上分位数标准化</strong> (Upper-quartile normalization, UQ)，UQ只能消除流细胞效应而对文库效应束手无策，RUV算法解决的就是如何消除不同文库的影响。</p><p><strong>局部加权回归散点平滑法</strong> (Locally Weighted Scatterplot Smoothing, LOWESS/LOESS)不能消除文库效应。</p><hr><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="ERCC-spike-in-controls"><a href="#ERCC-spike-in-controls" class="headerlink" title="ERCC spike-in controls"></a>ERCC spike-in controls</h2><p>ERCC 即 External RNA Controls Consortium，是斯坦福大学为了定制一套spike-in RNA而成立的专门性组织，主要的工作是设计了好用的spike-in RNA，方便microarray以及RNA-Seq进行内参定量。[<a href="https://jimb.stanford.edu/ercc/" target="_blank" rel="noopener">官方首页</a>]</p><p>RNA spike-in是一种数量和序列都已知的RNA转录本，用于校准RNA杂交实验（例如DNA微阵列实验、RT-qPCR、RNA测序等）的测量值。RNA spike-in作为对照组（控制组）探针，被设计成能与具有相应匹配序列的DNA分子结合，这个特异性结合的过程我们称之为<strong>杂交</strong>。在制备的过程中，已知数量的spike-in将与实验样本进行混合。spike-ins的杂交程度可以用来标准化样本RNA的测量值。[<a href="https://en.wikipedia.org/wiki/RNA_spike-in" target="_blank" rel="noopener">wiki</a>] [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1797020" target="_blank" rel="noopener">reference</a>]</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h1&gt;&lt;p&gt; Gagnon-Bartsch et al.提出了RUV-2用来标准化&lt;strong&gt;连续的&lt;/strong&gt;微阵列数据，移除不需要的变异。这里基于前面的方法进行扩展，用以标准化&lt;strong&gt;离散的&lt;/strong&gt;RNA测序数据。&lt;/p&gt;
&lt;p&gt; 对于表达矩阵（样本数 $n{\times}J$ 基因数）,构建&lt;strong&gt;泛化线性模型&lt;/strong&gt; (Generalized Linear Model, GLM):&lt;br&gt;$$ \log{E[Y|W,X,O]}=W\alpha+X\beta+O $$&lt;br&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
      <category term="paper" scheme="http://chenyin.top/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>批次效应（batch effect）</title>
    <link href="http://chenyin.top/bioinfo/20190319-cca5.html"/>
    <id>http://chenyin.top/bioinfo/20190319-cca5.html</id>
    <published>2019-03-19T06:38:32.000Z</published>
    <updated>2019-04-20T09:04:21.146Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a><span id="def">一、定义</span></h1><p>下面是大佬给出来的关于<strong>批次效应</strong>(batch effect)的定义：</p><blockquote><p>Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. <a href="https://www.nature.com/articles/nrg2825" target="_blank" rel="noopener">Leek et. al (2010)</a></p></blockquote><a id="more"></a><p>批次效应是测量结果中的一部分，它们因为实验条件的不同而具有不同的表现形式，并且与我们研究的变量没有半毛钱关系。一般批次效应可能在下述情形中出现：</p><ul><li>一个实验的不同部分在<strong>不同时间</strong>完成；</li><li>一个实验的不同部分由<strong>不同的人</strong>完成；</li><li>试剂用量不同、芯片不同、实验仪器不同；</li><li>将自己测的数据与从网上下载的数据混合使用；</li><li>……</li></ul><hr><h1 id="二、检测"><a href="#二、检测" class="headerlink" title="二、检测"></a>二、检测</h1><p>批次效应相关协变量已知时，直接聚类观察结果是否和相应协变量相关。<br>混合数据因为实验条件迥异，一般批次效应都很大。</p><p>以R为例，通过聚类检验是否存在批次效应。请先查看下面的<a href="#dataset">示例数据集</a>。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t() 转置函数</span></span><br><span class="line"><span class="comment"># dist() 距离函数：按照指定规则求行向量间的距离，因此要转置</span></span><br><span class="line">&gt; dist_mat &lt;- dist(t(edata))</span><br><span class="line">&gt; clustering &lt;- hclust(dist_mat) <span class="comment"># hclust 的输入结构与 dist 相同！</span></span><br><span class="line"><span class="comment"># 按照批次信息聚类</span></span><br><span class="line">&gt; plot(clustering, labels = pheno$batch)</span><br><span class="line"><span class="comment"># 按照是否是正常细胞聚类</span></span><br><span class="line">&gt; plot(clustering, labels = pheno$cancer)</span><br></pre></td></tr></table></figure></p><p>聚类结果如下：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/15.png" width="50%"><br>左边的红色框框是正常细胞中混入的癌细胞，右边蓝色框框中是癌细胞中混入的正常细胞。</p><p>还有许多检验批次效应的的方法，这篇<a href="https://www.itl.nist.gov/div898/handbook/eda/section4/eda42a3.htm" target="_blank" rel="noopener">文章</a>给出了多种检验方式：</p><ul><li>图分析：双柱状图、QQ图、箱线图、块图、…</li><li>定量分析：F检验、双样本t检验、…</li></ul><hr><h1 id="三、处理"><a href="#三、处理" class="headerlink" title="三、处理"></a>三、处理</h1><p>实验条件允许的条件下，应该优化实验设计，将引起批次效应的协变量采样<strong>分散</strong>开来。例如，对于时间批次效应，实验的不同部分应该在各个时间内均匀采样。这叫“治病就治本”。</p><p>但是大多数情况下实验条件不允许，如果够幸运的话批次效应相关的协变量已经被记录下来了，此时对批次效应进行验证，然后使用统计模型过滤；如果十分不幸，批次效应相关的协变量没有被记录或者不明显，我们就需要借助相关工具猜一下哪个变量可能造成了批次效应，然后使用统计模型过滤。前者叫<strong>参数化方法</strong>，后者叫<strong>非参数化方法</strong>。</p><h2 id="1-导入示例数据集"><a href="#1-导入示例数据集" class="headerlink" title="1.导入示例数据集"></a><span id="dataset">1.导入示例数据集</span></h2><h3 id="bladderbatch包"><a href="#bladderbatch包" class="headerlink" title="bladderbatch包"></a>bladderbatch包</h3><p><code>bladderbatch</code>包包含了一项<a href="https://www.ncbi.nlm.nih.gov/pubmed/15173019" target="_blank" rel="noopener">膀胱癌研究</a>中相关的57个样本的基因表达数据，这些数据已经使用RMA标准化，并且已经按照<a href="https://www.ncbi.nlm.nih.gov/pubmed/20838408" target="_blank" rel="noopener">相关协议</a>进行了预处理。</p><p>另外阅读R文档我们发现：</p><ul><li><code>eSet</code>是一个包含高通量实验元数据的一个类，它不能直接被实例化。</li><li><code>pData</code>方法在类<code>eSet</code>中被定义，它的作用是访问数据的元数据（注释信息）。</li><li><code>ExpressionSet</code>继承自<code>eSet</code>，同样是一个高通量测序数据的容器，由&gt; * <code>biobase</code>包引入，封装了<strong>表达矩阵</strong>和<strong>样本分组信息</strong>。表达矩阵存储在<code>exprs</code>中。</li></ul><p><code>bladderbatch</code>数据集是（类似）<code>ExpressionSet</code>类型，我们可以使用<code>pData()</code>加载元数据，使用<code>exprs()</code>加载表达谱数据。<br><code>bladderbatch</code>数据集用来演示如何校正批次效应。</p><h3 id="下载和加载数据集"><a href="#下载和加载数据集" class="headerlink" title="下载和加载数据集"></a>下载和加载数据集</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 1.安装并加载数据集</span></span><br><span class="line">&gt; BiocInstaller::biocLite(<span class="string">"bladderbatch"</span>)</span><br><span class="line">&gt; <span class="keyword">library</span>(bladderbatch) <span class="comment"># 或者 library("bladderbatch", character.only=TRUE)</span></span><br><span class="line"><span class="comment">## 2.查看当前可用数据集</span></span><br><span class="line">&gt; data()</span><br><span class="line"><span class="comment">## 3.检查是否有如下信息</span></span><br><span class="line">Data sets <span class="keyword">in</span> package ‘bladderbatch’:</span><br><span class="line">bladderEset (bladderdata)           Bladder Gene Expression Data Illustrating Batch Effects</span><br><span class="line"><span class="comment">## 加载数据集</span></span><br><span class="line">&gt; data(bladderdata) <span class="comment"># 实际加载进来的数据集名字叫做 bladderEset !</span></span><br><span class="line">&gt; pheno &lt;- pData(bladderEset) <span class="comment"># 使用 pData 加载元数据/注释信息</span></span><br><span class="line">&gt; edata &lt;- exprs(bladderEset) <span class="comment"># 使用 exprs 加载数据</span></span><br></pre></td></tr></table></figure><p><code>pheno</code>如下所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/16.png" width="20%"><br>样本的批次信息存储作为元数据存储在<code>pheno$batch</code>中（R中使用<code>$</code>访问对象的属性）。</p><p><code>edata</code>如下所示：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/17.png" width="50%"><br>一列表示一个样本（细胞），后面求距离需要转置。</p><hr><h2 id="2-R中的sva包"><a href="#2-R中的sva包" class="headerlink" title="2.R中的sva包"></a>2.R中的sva包</h2><p><code>sva</code>用于移除高通量测序数据中的<a href="#def"><strong>批次效应</strong></a>以及其它无关变量的影响。</p><p><code>sva</code>包含用于标识和构建高维数据集（例如基因表达、RNA测序/甲基化/脑成像数据等可以直接进行后续分析的数据）<strong>代理变量</strong>的函数。代理变量是直接从高维数据构建的协变量，可以在后续分析中用于调整未知的、未建模的或潜在的噪音源。</p><blockquote><p><strong>代理变量（surrogate/proxy variable）</strong>: A variable that can be measured (or is easy to measure) that is used in place of one that cannot be measured (or is difficult to measure). For example, whereas it may be difficult to assess the wealth of a household, it is relatively easy to assess the value of a house. See also proxy variable. (from <a href="http://www.oxfordreference.com/view/10.1093/oi/authority.20110803100544210" target="_blank" rel="noopener">Oxford Reference</a>)<br><strong>代理变量分析（Surrogate Variable Analysis）</strong>：<a href="https://digital.lib.washington.edu/researchworks/handle/1773/9586" target="_blank" rel="noopener">Click here</a></p></blockquote><p><code>sva</code>从三个方面消除人为设计造成的影响：</p><ol><li>为未知变异源构造代理变量；(Leek and Storey <a href="https://www.ncbi.nlm.nih.gov/pubmed/17907809" target="_blank" rel="noopener">2007 PLoS Genetics</a>, <a href="https://www.ncbi.nlm.nih.gov/pubmed/20941797" target="_blank" rel="noopener">2011 Pharm Stat.</a>)</li><li>使用ComBat直接移除已知的批次效应；<a href="https://academic.oup.com/biostatistics/article/8/1/118/252073" target="_blank" rel="noopener">(Johnson et al. 2007 Biostatistics)</a></li><li>使用已知的控制探针(known control probes)移除批次效应；<a href="https://www.biorxiv.org/content/10.1101/006585v2" target="_blank" rel="noopener">(Leek 2014 biorXiv)</a><br>移除批次效应和使用代理变量可以减少依赖性，稳定错误率估计值，提高重现性。</li></ol><p>查看<code>sva</code><a href="http://127.0.0.1:28090/library/sva/doc/sva.pdf" target="_blank" rel="noopener">在线文档</a>。</p><h3 id="gt-已记录批次信息"><a href="#gt-已记录批次信息" class="headerlink" title="&gt; 已记录批次信息"></a>&gt; 已记录批次信息</h3><p>当<strong>批次协变量</strong>已知时（即每个样本分属于哪一个批次记录在数据集的元数据中），可以使用<code>sva</code>的<code>ComBat</code>校正<strong>批次效应</strong>。<br><code>ComBat</code>使用参数（parametric）或者非参数（non-parametric）的<strong>经验贝叶斯框架</strong>（Empirical Bayes Frameworks）进行批次效应的校正。</p><p>先看<code>ComBat</code>的用法：摘自<a href="https://www.rdocumentation.org/packages/sva/versions/3.20.0/topics/ComBat" target="_blank" rel="noopener">官方文档</a><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; ComBat(dat, batch, mod=<span class="literal">NULL</span>, par.prior = <span class="literal">TRUE</span>, prior.plots = <span class="literal">FALSE</span>)</span><br><span class="line"><span class="comment"># dat: 基因组测量矩阵（探针维度 X 样本数），探针维度例如marker数、基因数.....，例如表达谱矩阵</span></span><br><span class="line"><span class="comment"># batch: 批次协变量，只能传入一个批次协变量！</span></span><br><span class="line"><span class="comment"># mod: 这是一个模式矩阵，里面包含了我们感兴趣的变量！</span></span><br><span class="line"><span class="comment"># par.prior: 基于参数/非参数，默认为基于参数</span></span><br></pre></td></tr></table></figure></p><p>有了背景知识我们就可以进行膀胱癌数据的批次校正：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; pheno$hasCancer &lt;- pheno$cancer == <span class="string">"Cancer"</span></span><br><span class="line"><span class="comment"># 或者 &gt; pheno$hasCancer &lt;- as.numeric(pheno$cancer == "Cancer")</span></span><br><span class="line">&gt; model &lt;- model.matrix(~hasCancer, data=pheno)</span><br><span class="line">&gt; combat_edata &lt;- ComBat(dat = edata, batch = pheno$batch, mod = model)</span><br><span class="line"><span class="comment"># 这里的 mod 参数就比较有意思了，它记录的是我们感兴趣的变量。因为初次接触R只能肤浅理解一下。</span></span><br><span class="line"><span class="comment"># 它应该是一个我们期望样本能被正确聚类所依据的协变量，它总是数值型变量</span></span><br></pre></td></tr></table></figure></p><p><code>model.matrix(...)</code>的详细解释见<a href="/R/20190319-1548.html">这里</a>。</p><p>画图：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; dist_mat_combat &lt;- dist(t(combat_edata))</span><br><span class="line">&gt; clustering_combat &lt;- hclust(dist_mat_combat, method = <span class="string">"complete"</span>)</span><br><span class="line">&gt; plot(clustering, labels = pheno$batch)</span><br><span class="line">&gt; plot(clustering, labels = pheno$cancer))</span><br></pre></td></tr></table></figure></p><p>我们发现批次效应被移除了：<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/18.png" width="50%"></p><h3 id="gt-没有记录批次信息"><a href="#gt-没有记录批次信息" class="headerlink" title="&gt; 没有记录批次信息"></a>&gt; 没有记录批次信息</h3><p><a href="http://master.bioconductor.org/packages/release/workflows/vignettes/rnaseqGene/inst/doc/rnaseqGene.html#removing-hidden-batch-effects" target="_blank" rel="noopener">看这里</a></p><hr><h2 id="3-R中的ber包"><a href="#3-R中的ber包" class="headerlink" title="3.R中的ber包"></a>3.R中的ber包</h2><p>ber的全称就是batch effects removal，使用<code>&gt; install.packages(&quot;ber&quot;)</code>安装ber包，查看<a href="https://cran.r-project.org/web/packages/ber/ber.pdf" target="_blank" rel="noopener">用户手册</a>。</p><p>这个包里有6个函数，它们的作用就是校正<strong>微阵列标准数据</strong>中的批次效应。标准数据指的是：输入矩阵每一行代表独立的样本，每一列代表基因；批次信息作为已知的<strong>分类变量</strong>；期望变量可以大大提高批次效应校正的效率。</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>berr(Y, b, covariates = NULL)</code></td><td>using a two-stage regression approach <a href="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.pdf" target="_blank" rel="noopener">(M. Giordan. February 2013)</a></td></tr><tr><td><code>ber_bg(Y, b, covariates = NULL,partial=TRUE,nSim=150)</code></td><td>using a two-stage regression approach and bagging <a href="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/01.pdf" target="_blank" rel="noopener">(M. Giordan. February 2013)</a></td></tr><tr><td><code>combat_p(Y, b, covariates = NULL, prior.plots=T)</code></td><td>using a parametric empirical Bayes approach <a href="https://www.ncbi.nlm.nih.gov/pubmed/16632515" target="_blank" rel="noopener">(n Johnson et al. 2007)</a></td></tr><tr><td><code>combat_np(Y, b, covariates = NULL)</code></td><td>using a non-parametric empirical Bayes approach <a href="https://www.ncbi.nlm.nih.gov/pubmed/16632515" target="_blank" rel="noopener">(n Johnson et al. 2007)</a></td></tr><tr><td><code>mean_centering(Y, b)</code></td><td>using the means of the batches</td></tr><tr><td><code>standardization(Y, b)</code></td><td>using the means and the standard deviations of the batches</td></tr></tbody></table><p>上表中的：</p><ul><li><code>Y</code>是输入矩阵（样本数 $n{\times}g$ 探针数）</li><li><code>b</code>是 $n$ 维<strong>分类1向量</strong>，每个分量对应着每个样本的批次信息</li><li><code>covariates</code>是一个 $n$ 行的<code>data.frame</code>实例</li></ul><p>上面的6个函数都需要指定<code>b</code>，所以它们都是用来处理<strong>批次信息被记录</strong>的情形的，对于启发性的校正貌似没提出解决方案。</p><hr><h2 id="4-R中的RUVSeq包"><a href="#4-R中的RUVSeq包" class="headerlink" title="4.R中的RUVSeq包"></a>4.R中的RUVSeq包</h2><p>RUVSeq means <em>Remove Unwanted Variation from RNA-Seq Data</em>, which shows us how to conduct a differential expression (DE) analysis that controls for “unwanted variation”, e.g., batch, library preparation, and other nuisance effects, using the between-sample normalization methods proposed in <a href="https://www.nature.com/articles/nbt.2931" target="_blank" rel="noopener">Risso et al. (2014)</a>.</p><p>RUV算法基本原理参考<a href="/Bioinformatics/20190322-afa7.html">这里</a>，原文在<a href="https://www.nature.com/articles/nbt.2931" target="_blank" rel="noopener">这里</a>。</p><hr><h2 id="5-R中的BatchQC包"><a href="#5-R中的BatchQC包" class="headerlink" title="5. R中的BatchQC包"></a>5. R中的BatchQC包</h2><p><a href="https://bioconductor.org/packages/release/bioc/html/BatchQC.html" target="_blank" rel="noopener">BatchQC工具</a></p><hr><h1 id="四、FAQ"><a href="#四、FAQ" class="headerlink" title="四、FAQ"></a>四、FAQ</h1><ol><li><strong>标准化（normalization）可以消除批次效应吗？</strong> 只能缓解，不能消除。</li></ol><hr><h1 id="五、其它资料"><a href="#五、其它资料" class="headerlink" title="五、其它资料"></a>五、其它资料</h1><p>Stanford大学MOOC公开课讲义：<a href="http://genomicsclass.github.io/book/" target="_blank" rel="noopener">PH525x series - Biomedical Data Science</a></p><p><a href="https://bioinformatics.mdanderson.org/public-software/tcga-batch-effects/" target="_blank" rel="noopener">TCGA Batch Effects Viewer</a></p><p>From BioMedSearch: <a href="http://www.biomedsearch.com/nih/Removing-batch-effects-in-analysis/21386892.html" target="_blank" rel="noopener">Removing batch effects in analysis of expression microarray data: an evaluation of six batch adjustment methods.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、定义&quot;&gt;&lt;a href=&quot;#一、定义&quot; class=&quot;headerlink&quot; title=&quot;一、定义&quot;&gt;&lt;/a&gt;&lt;span id=&quot;def&quot;&gt;一、定义&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;下面是大佬给出来的关于&lt;strong&gt;批次效应&lt;/strong&gt;(batch effect)的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments, or if two different lots of reagents, chips or instruments were used. &lt;a href=&quot;https://www.nature.com/articles/nrg2825&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Leek et. al (2010)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="bioinfo" scheme="http://chenyin.top/categories/bioinfo/"/>
    
    
  </entry>
  
  <entry>
    <title>model.matrix(...)</title>
    <link href="http://chenyin.top/R/20190319-1548.html"/>
    <id>http://chenyin.top/R/20190319-1548.html</id>
    <published>2019-03-19T03:34:18.000Z</published>
    <updated>2019-04-14T16:02:20.221Z</updated>
    
    <content type="html"><![CDATA[<p>R中的模型矩阵函数。<a id="more"></a></p><h1 id="分类变量"><a href="#分类变量" class="headerlink" title="分类变量"></a>分类变量</h1><p><strong>分类变量</strong>（Factors）：R中用来存储分类数据的类别信息。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; f = factor(c(<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'a'</span>,<span class="string">'c'</span>))</span><br><span class="line"><span class="comment"># 检查变量是否是分类变量（因子）</span></span><br><span class="line">&gt; class(f)</span><br><span class="line">[<span class="number">1</span>] <span class="string">"factor"</span></span><br><span class="line"><span class="comment"># 查看分类变量中有哪些类别</span></span><br><span class="line">&gt; levels(f)</span><br><span class="line">[<span class="number">1</span>] <span class="string">"a"</span> <span class="string">"b"</span> <span class="string">"c"</span></span><br><span class="line"><span class="comment"># 查看分类变量中有几类</span></span><br><span class="line">&gt; nlevels(f)</span><br><span class="line">[<span class="number">1</span>] <span class="number">3</span></span><br></pre></td></tr></table></figure></p><h1 id="哑变量"><a href="#哑变量" class="headerlink" title="哑变量"></a>哑变量</h1><p><strong>虚拟变量</strong>/<strong>哑变量</strong>（dummy variable）：量化非数值类型的变量，通常取0/1。例如，衡量一个人的性别：男 -&gt; 1，女 -&gt; 0。</p><h1 id="解释变量"><a href="#解释变量" class="headerlink" title="解释变量"></a>解释变量</h1><p><strong>解释变量</strong>（explanatory variable）：单纯从数理角度来看，解释变量等同于控制变量/自变量，与之相对的是<strong>被解释变量</strong>（反应变量/因变量）。<a href="http://blog.sciencenet.cn/blog-334577-426759.html" target="_blank" rel="noopener">REF</a></p><h1 id="设计矩阵"><a href="#设计矩阵" class="headerlink" title="设计矩阵"></a>设计矩阵</h1><p><strong>设计矩阵</strong>（design matrix）：又叫<strong>模型矩阵</strong>（model matrix）或者<strong>回归矩阵</strong>（regressor matrix）。由解释变量值组成的矩阵：一行代表一个独立的观测对象（样本），一列代表对应的变量（特征值、元数据），通常记为$X$。简单理解，就是我们所说的<strong>输入矩阵</strong>，可以是元数据的，也可以是数据的。</p><h1 id="model-matrix-…"><a href="#model-matrix-…" class="headerlink" title="model.matrix(…)"></a>model.matrix(…)</h1><p>定义：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># S3 method for default </span></span><br><span class="line">model.matrix(object, data = environment(object), contrasts.arg = <span class="literal">NULL</span>, xlev = <span class="literal">NULL</span>, …)</span><br><span class="line"><span class="comment"># 函数依据 object 创建设计矩阵，矩阵的创建必须借助于数据集 data</span></span><br><span class="line"><span class="comment"># data 必须能提供与 object 相同名字的变量！</span></span><br></pre></td></tr></table></figure></p><p>以膀胱癌去批次效应为例，元数据形式如下<br><img src="http://barwe-blog.oss-cn-shenzhen.aliyuncs.com/img/19/03/22.png" alt="model.matrix.pheno"></p><p>下面是部分列处理后的结果：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt; model &lt;- model.matrix(~batch, data = pheno)</span><br><span class="line">                  (Intercept)batch</span><br><span class="line">GSM71019.CEL           <span class="number">1</span>     <span class="number">3</span></span><br><span class="line">GSM71020.CEL           <span class="number">1</span>     <span class="number">2</span></span><br><span class="line"><span class="comment"># pheno$batch 是数值型变量，相当于提取列</span></span><br><span class="line"><span class="comment"># 此时新的变量名仍然是 batch</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">&gt;model &lt;-  model.matrix(~cancer, data = pheno)</span><br><span class="line">             (Intercept) cancerCancer cancerNormal</span><br><span class="line">GSM71019.CEL           <span class="number">1</span>            <span class="number">0</span>            <span class="number">1</span></span><br><span class="line">GSM71020.CEL           <span class="number">1</span>            <span class="number">0</span>            <span class="number">1</span></span><br><span class="line"><span class="comment"># pheno$cancer 被处理成分类变量，每一类将单独作为列（哑变量），取值为0/1</span></span><br><span class="line"><span class="comment"># 此时新的变量名为 cancerCancer 和 cancerNormal</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">&gt; model &lt;- model.matrix(~cancer==<span class="string">"Cancer"</span>, data = pheno)</span><br><span class="line">             (Intercept) cancer == <span class="string">"Cancer"</span><span class="literal">TRUE</span></span><br><span class="line">GSM71019.CEL           <span class="number">1</span>                            <span class="number">0</span></span><br><span class="line">GSM71020.CEL           <span class="number">1</span>                            <span class="number">0</span></span><br><span class="line"><span class="comment"># cancer=="Cancer" 是一个 logical 类型</span></span><br><span class="line"><span class="comment"># 这种写法极不优雅！我们应该先定好名字</span></span><br><span class="line">&gt; pheno$hasCancer &lt;- pheno$cancer == <span class="string">"Cancer"</span></span><br><span class="line">&gt; model &lt;- model.matrix(~hasCancer, data=pheno)</span><br><span class="line">             (Intercept) hasCancer</span><br><span class="line">GSM71019.CEL           <span class="number">1</span>         <span class="number">0</span></span><br><span class="line">GSM71020.CEL           <span class="number">1</span>         <span class="number">0</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;R中的模型矩阵函数。
    
    </summary>
    
      <category term="R" scheme="http://chenyin.top/categories/R/"/>
    
    
  </entry>
  
</feed>
